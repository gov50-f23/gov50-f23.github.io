[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Prof. Matt Blackwell\n   CGIS Knafel 305\n   mblackwell@gov.harvard.edu\n   matt_blackwell\n   Schedule an appointment\n\n\n\n\n\nAhmet Akbiyik\nJames Jolin\nJerry Min\nLaura Royden\nJulio Solis Arce\nAndy Wang\n\n\n\n\n\n\n\n   Tue/Thu\n   September 3rd-December 20th, 2023\n   12:00–1:15 PM\n   Emerson 105\n   Slack\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\nIn this course, you will learn the basics of data science as applied to the social sciences. This involves two broad skill sets: (1) learning the computing and programming tools to both manage and analyze data; and (2) understanding the conceptual foundations of why we might manage or analyze data in one way versus another. This course will address both of these topics.\nSpecifically, at the end of the course you should be able to:\n\nSummarize and visualize data\nWrangle messy data into tidy forms\nEvaluate claims about causality\nBe able to use linear regression to analyze data\nUnderstand uncertainty in data analysis and how to quantify it\nUse professional tools for data analysis such as R, RStudio, git, and GitHub\n\nWe will also attempt to inspire a passion for data analysis and create a community among the students to deepen their learning.\n\nExpectations\nIn this course, you will be expected to\n\ncomplete eight problem sets,\ncomplete eight weekly tutorials,\ntake two take-home, open-book exams, and\nwrite one final data analysis project.\n\n\n\nPrerequisites\nNo prerequisites will be assumed. If you are unfamiliar with downloading and installing software programs on your Mac or PC, you may want to allocate additional time to make sure those aspects of the course go smoothly. In particular, we have developed a Problem Set 0 to guide you through installing R, RStudio, and git to give you a sense of the tools we will be using.\n\n\nCredit\nThis course satisfies the Methods requirement for the Government department and the Quantitative Reasoning with Data requirement in the Harvard College Curriculum. It also counts toward completion of the Government department Data Science track.\n\n\nSpeaker Series\nOn various Fridays throughout the semester, we will host the Gov 50 Speaker Series, where guest speakers working in data science in various industries will discuss their career paths and answer student questions."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Syllabus",
    "section": "Course structure",
    "text": "Course structure\n\nFlow of the Course\nThe course will follow a basic flow each week, with small differences if a tutorial or problem set is due or not.\n\nMonday: Complete reading/watching course material, complete tutorial (if due).\nTuesday: Lecture meets.\nWednesday: Submit problem set by 11:55 PM (if due).\nThursday: Lecture meets; problem sets and exams posted; sections meet.\nFriday: Sections meet.\nSunday: Submit exams by 11:55 PM (if due)\n\n\n\nTutorials\nWe will assign short weekly tutorials to assess your knowledge of the material covered in the reading/video materials that week. While you are expected to complete them on time, they will be graded based on completion not on how correct the answers are. With some exceptions, they will usually be due Mondays at 11:59pm ET.\n\n\nLectures\nWe will meet twice a week for lectures where I will combine presenting material and doing live coding demonstrations. Ideally, you should bring your laptop to class and be ready to code along with me! We will have a Slack channel dedicated to answer computing questions by TFs during lecture. Lectures will be recorded and usually posted to Canvas within 24 hours of the lecture time. Keeping up with these lectures is vital to your success in the class.\n\n\nSections\nSections will be small-group settings to practice the tools and techniques that we cover in class. These meetings are essential to gaining the skills you need for the problem sets, exams, and final project. You are expected to attend sections. We will not assign you a particular section, so you are free to attend one that works for your schedule and attend different sections throughout the semester if a conflict arises.\nSections will fall into two types based on prior exposure to the course material: more comfortable and less comfortable. While Gov 50 is designed to be accessible to students without prior exposure to data science and computing, we recognize that students with different backgrounds in the material may require different pedagogical approaches.\n\n\nStudy Halls\nStudy halls are a combination of office hours and drop-in tutoring sessions. Course assistants will hold a table usually at one of the house dining halls or common rooms and help students with assignments and course material. Study halls work best if you come as a group and work on the assignments on your own while you are there and ask for help from the CAs when you get stuck.\nYou can find a list of the Study Hall times on the Study Hall Schedule page.\n\n\nProblem Sets\nOnly reading about data science is about as instructive as reading a lot about hammers or watching someone else wield a hammer. You need to get your hands on a hammer or two. Thus, in this course, you will have mostly-weekly problem sets that will give you an opportunity to apply the statistical techniques you are learning. They will usually be focused on data analysis in general and will often involve a real dataset.\nWe encourage students to rely on peer working groups as they work on these homeworks, but each student will submit their own work individually. We will facilitate the formation of homework peer groups.\n\n\n\n\n\n\nGrace policy\n\n\n\nWhen calculating the final homework portion of the overall grade, we will drop the lowest score and use the remaining scores. Thus, if you have an emergency that forces you to miss one homework, your grade will not be severely affected.\n\n\n\n\nExams\nThere will be two take-home exams during the course. This exam will be similar to a homework in format and in the sense that it will be open book and open internet, but you will not be allowed to collaborate with other students or be able to communicate with any humans about the exam. You will be given several days to complete the exam. We will provide more information about the exam as it approaches.\n\n\n\nExam\nRelease Date\nDue Date\n\n\n\n\nExam 1\nThu, Oct 19th 5:00pm ET\nSun, Oct 22nd 11:59pm ET\n\n\nExam 2\nThu, Dec 7th 5:00pm ET\nSun, Dec 10th 11:59pm ET\n\n\n\n\n\nDiscussion\nWe will be using the Ed platform for discussions on course material. There is a users guide to help orient yourself to the platform. We will enroll you into the platform toward the start of the semester.\n\n\nGrading\nYou (the student) and I (the instructor) should care the most about what you learn, not what numerical/letter summary of that learning you get at the end of the semester. So I would love to not have grades at all, but unfortunately we humans are very good at procrastinating on our good intentions when there is no incentive not to. Thus, we have grades to help solve this commitment problem and to encourage you to put effort into learning the course material.\nHere are how each portion of course contributes to the overall grade:\n\n\n\nCategory\nPercent of Final Grade\n\n\n\n\nR Tutorials\n10%\n\n\nProblem Sets\n40%\n\n\nExams\n30%\n\n\nFinal Project\n20%\n\n\n\nWe will use Gradescope for submission of the various assignments throughout the semester. Once enrollment is finished, Gradescope will automatically connect through Canvas.\nBump-up policy: We reserve the right to “bump up” the grades of students who have made valuable contributions to the course in the lecture, sections, study halls, or discussion/Slack. This also applies to students who show tremendous progress and growth over the semester."
  },
  {
    "objectID": "syllabus.html#final-project",
    "href": "syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\nThe final project is a data analysis project about whatever data excites you. That could be public opinion in U.S. elections, United Nations voting patterns, patterns of racial discrimination in police behavior, the distribution of salaries in basketball, or the economics of the Marvel Cinematic Universe. No matter the topic, you will formula a key research question, find data on that question, answer the question using the tools of the course, and present those results for public consumption.\nThe goal is for this to be a professional project that you can use to showcase the skills that you have gained to potential employers. Your final submission will be a publicly available article/website that contains: (1) a brief introduction to the research question and data collected; (2) a visualization of the data in question that speaks to your research question; (3) a presentation (as a table or graph) of a regression model assessing your question along with a plain-English interpretation; (4) a brief (one-paragraph) section that describes limitations of your analysis and threats to inference, and states how your analysis could be improved (e.g., improved data that would be useful to collect). Finally, there must be a link to the GitHub repo that contains the source code to load, clean, and analyze the data and produce the article.\nThe data collection and cleaning must be meaningful—it’s not sufficient to simply use a pre-cleaned data from an R package. Self-collected data is allowed and even encouraged, though beware that it can only be used for the final project itself only unless you go through human subjects approval from the University.\n\n\n\nMilestone\nDue Date\n\n\n\n\nCreating a GitHub Repository\nFri, Sep 29th 11:59pm ET\n\n\nData and Proposal\nFri, Oct 13th 11:59pm ET\n\n\nFirst Visualization\nFri, Nov 3rd 11:59pm ET\n\n\nFirst Analysis\nFri, Nov 17th 11:59pm ET\n\n\nFinal Report\nWed, Dec 13th 11:59pm ET"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nLate Policy\nFor problem sets, late submissions will be penalized 10% if submitted 0-24 hours late and penalized 20% if submitted 24-48 hours late. We will not accept any submissions after 48 hours. This penalty will be waived the first time a submission is late.\n\n\nRegrading Policy\nIf you feel there has been an error in the grading of one your assignment, you may request (in writing) a regrade of the assignment. Please send your request in an email to Professor Blackwell and the Head TF. A member of the teaching staff will regrade the entire assignment, not just the part you are disputing. Therefore, your regrade might increase or decrease the overall grade on the assignment.\n\n\nOffice Hours and Availability\nOffice hours for the teaching staff are listed on the Office Hours Schedule page.\nIf you have a general question, you should post it to the Ed discussion board (where you can make your question anonymous if you would like). This is almost always the fastest way to get an answer. However, you can also email me directly at mblackwell@gov.harvard.edu if you have a question that is about a personal situation.\n\n\nAcademic Honesty\nThe work that you do in both the problem sets should be your own work. You may seek help from others so long as this does not result in someone else completing your work for you. When asking for help, you may show others your code to help diagnose a bug or highlight a potential issue, but you should not view their (working) code. You should cite any discussions you have with other students in your problem set and note if they helped you with your code. You should never copy and paste code from another student or elsewhere (e.g., websites, former students).\nI also strongly suggest that you make a solo effort at all the problems before consulting others. The exams will be very difficult if you have no experience working on your own. There is no collaboration allowed on the exams."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\n\nBooks\nWe will use the following books in this class:\n\nImai, Kosuke and Nora Webb Willaims. 2022. Quantitative Social Science: An Introduction with Tidyverse, 2022. Princeton University Press.\nIsmay, Chester and Albert Y. Kim. 2022. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse.\nMine Cetinkaya-Rundel and Johanna Hardin. 2021. Introduction to Modern Statistics. OpenIntro.\n\nThe following books are optional, but may be helpful to build you understanding of the material:\n\nFreedman, David, Pisani, Robert, and Purves, Roger. 2007. Statistics. W.W. Norton & Company. 4th edition.\nGonick, Larry, and Woollcott Smith. 1993. The Cartoon Guide to Statistics. HarperPerinnial.\n\n\n\nComputing\nWe’ll use R in this class to conduct data analysis. R is free, open source, and available on all major platforms (including Solaris, so no excuses). RStudio (also free) is a graphical interface to R that is widely used to work with the R language. You can find a virtually endless set of resources for R and RStudio on the internet. For beginners, there are several web-based tutorials. In these, you will be able to learn the basic syntax of R. We’ll post more R resources on the course website. We will also use git and Github to manage our projects.\nYou can get setup with all of these tools by completing Problem Set 0."
  },
  {
    "objectID": "syllabus.html#mental-health",
    "href": "syllabus.html#mental-health",
    "title": "Syllabus",
    "section": "Mental Health",
    "text": "Mental Health\nCollege is a stressful time in one’s life and mixing it with a global pandemic, remote learning, and dislocation makes this one of the most fraught semester any of us have probably faced. We acknowledge that nothing is quite normal and that there may be times when you feel overwhelmed by this course or by life more generally. Please feel free to reach out to any of the course staff if you want to talk about any issues you are having with the course or anything else. We will always try to help and we are committed to being extra accommodating this semester on course policy issues. Please just get in touch.\nOf course, there are other resources at Harvard if you need them. A few are listed below:\n\nCounseling and Mental Health Services\nRoom 13"
  },
  {
    "objectID": "syllabus.html#accessibility",
    "href": "syllabus.html#accessibility",
    "title": "Syllabus",
    "section": "Accessibility",
    "text": "Accessibility\nHarvard University values inclusive excellence and providing equal educational opportunities for all students. Our goal is to remove barriers for disabled students related to inaccessible elements of instruction or design in this course. If reasonable accommodations are necessary to provide access, please contact the Disability Access Office (DAO). Accommodations do not alter fundamental requirements of the course and are not retroactive. Students should request accommodations as early as possible, since they may take time to implement. Students should notify DAO at any time during the semester if adjustments to their communicated accommodation plan are needed."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is the schedule for the semester. You can find the materials for each course meeting under the “Content” links for that week. You should generally:\n\nwatch the lecture videos (if any) and complete the readings by Monday;\ncomplete the tutorials by Monday evening at 11:59pm; and\nsubmit the problem set or exam by Wednesday at 11:59pm.\n\nHere’s a guide to the schedule:\n\nMaterials (): This page contains the readings, slides, and recorded lectures (if any) for the topic. Read/watch these first.\nTutorial (): A link to the tutorial for that week.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed.\n\nThe readings refer to following texts:\n\nQSS: Quantitative Social Science: An Introduction in tidyverse by Kosuke Imai and Nora Webb Williams\nMD: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim\nIMS: Introduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading\n\n\nMaterials\n\n\nTutorial\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\nSeptember 5\n\n\nIntroduction to the course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 7\n\n\nR, Rstudio, and data visualization\n\n\nMD Ch 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nSeptember 11\n\n\nTutorial 1  (submit by 23:59:00)\n\n\nMD Ch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 12\n\n\nData visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 13\n\n\nTutorial 2  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 14\n\n\nData wrangling\n\n\nMD Ch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nSeptember 18\n\n\nTutorial 3  (submit by 23:59:00)\n\n\nQSS 2.1-2.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 19\n\n\nData wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 20\n\n\nProblem Set 1  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 21\n\n\nCausal inference\n\n\nQSS Ch 2.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nSeptember 26\n\n\nCausal inference and randomized experiments\n\n\nQSS 2.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 27\n\n\nProblem Set 2  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 28\n\n\nCausal inference and observational studies\n\n\nQSS 2.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 29\n\n\nFinal Project Milestone 1  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\nOctober 2\n\n\nTutorial 4  (submit by 23:59:00)\n\n\nQSS Ch 2.6-3.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 3\n\n\nSummarizing data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 4\n\n\nProblem Set 3  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 5\n\n\nSurvey sampling\n\n\nQSS 3.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\nOctober 9\n\n\nTutorial 5  (submit by 23:59:00)\n\n\nQSS Ch 3.5-3.6 or MD Ch 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 10\n\n\nSummarizing relationships in our data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 11\n\n\nProblem Set 4  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 12\n\n\nImporting and joining data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 13\n\n\nFinal Project Milestone 2  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\nOctober 16\n\n\nTutorial 6  (submit by 23:59:00)\n\n\nQSS 4.1 (except 4.1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 17\n\n\nMidterm review & prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 18\n\n\nRegression\n\n\nMD Ch 5 or QSS 4.2.1-4.2.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 19–October 22\n\n\nExam 1  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\nOctober 24\n\n\nRegression and model fit\n\n\nMD Ch 6.1-6.2 or QSS 4.2.6-4.3.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 26\n\n\nMultiple regression and interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\nOctober 30\n\n\nTutorial 7  (submit by 23:59:00)\n\n\nMD Ch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 31\n\n\nSampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 1\n\n\nProblem Set 5  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 2\n\n\nSampling distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 3\n\n\nFinal Project Milestone 3  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\nNovember 6\n\n\nTutorial 8  (submit by 23:59:00)\n\n\nMD Ch 8/IMS Ch 12\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 7\n\n\nThe bootstrap and confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 8\n\n\nProblem Set 6  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 9\n\n\nThe bootstrap and confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\nNovember 14\n\n\nHypothesis testing\n\n\nMD Ch 9/IMS Ch 11\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 15\n\n\nProblem Set 7  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 16\n\n\nHypothesis testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 17\n\n\nFinal Project Milestone 4  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\nNovember 22\n\n\nHypothesis testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\nNovember 28\n\n\nMathematical models of uncertainty\n\n\nIMS Ch 13\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 29\n\n\nProblem Set 8  (submit by 23:59:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 30\n\n\nMathematical models of uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\n\nDecember 5\n\n\nUncertainty in regression\n\n\nQSS Ch 7.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 7–December 10\n\n\nExam 2  (submit by 23:59:00)"
  },
  {
    "objectID": "resources/speaker-series.html",
    "href": "resources/speaker-series.html",
    "title": "Gov 50 Speaker Series",
    "section": "",
    "text": "More details to come…"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Getting Software Set Up\nR Tutorials Instructions (for running locally in RStudio)\nProblem Sets and Exams Instructions\nGov 50 R Cheatsheet\nGov 50 Troubleshooting Guide"
  },
  {
    "objectID": "resources/index.html#gov-50-resources",
    "href": "resources/index.html#gov-50-resources",
    "title": "Resources",
    "section": "",
    "text": "Getting Software Set Up\nR Tutorials Instructions (for running locally in RStudio)\nProblem Sets and Exams Instructions\nGov 50 R Cheatsheet\nGov 50 Troubleshooting Guide"
  },
  {
    "objectID": "resources/index.html#r-tutorials",
    "href": "resources/index.html#r-tutorials",
    "title": "Resources",
    "section": "R Tutorials",
    "text": "R Tutorials\n\nHands On Programming with R\nR for Data Science"
  },
  {
    "objectID": "resources/index.html#r-markdown",
    "href": "resources/index.html#r-markdown",
    "title": "Resources",
    "section": "R Markdown",
    "text": "R Markdown\n\nR Markdown Tutorial\nR Markdown: The Definitive Guide"
  },
  {
    "objectID": "resources/index.html#git-and-github-resources",
    "href": "resources/index.html#git-and-github-resources",
    "title": "Resources",
    "section": "Git and GitHub Resources",
    "text": "Git and GitHub Resources\n\nHappy Git with R"
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before attempting the tutorials.\n\nCourse Introduction\nIntroduction to R and RStudio\nData Visualization\nData Wrangling\nData Wrangling and Barplots\nCausality\nRandomized Experiments\nObservational Studies\nSummarizing Data\nSurvey Sampling\nBivariate Relationships and Functions\nJoining and Tidying Data\nMidterm Review and Prediction\nRegression\nModel Fit\nMultiple Regression and Categorical Predictors\nSampling\nSampling Distributions"
  },
  {
    "objectID": "materials/13_models.html",
    "href": "materials/13_models.html",
    "title": "More Mathematical Models",
    "section": "",
    "text": "IMS Ch 15, 16."
  },
  {
    "objectID": "materials/13_models.html#readings",
    "href": "materials/13_models.html#readings",
    "title": "More Mathematical Models",
    "section": "",
    "text": "IMS Ch 15, 16."
  },
  {
    "objectID": "materials/13_models.html#slides-and-code",
    "href": "materials/13_models.html#slides-and-code",
    "title": "More Mathematical Models",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/22) lecture: Math models for hypothesis testing and difference in means\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/11_hyp_tests.html",
    "href": "materials/11_hyp_tests.html",
    "title": "Hypothesis Tests",
    "section": "",
    "text": "MD Ch 9, IMS Ch 11 for another take."
  },
  {
    "objectID": "materials/11_hyp_tests.html#readings",
    "href": "materials/11_hyp_tests.html#readings",
    "title": "Hypothesis Tests",
    "section": "",
    "text": "MD Ch 9, IMS Ch 11 for another take."
  },
  {
    "objectID": "materials/11_hyp_tests.html#data",
    "href": "materials/11_hyp_tests.html#data",
    "title": "Hypothesis Tests",
    "section": "Data",
    "text": "Data\n\nReinstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nCSV files for the data from this week if you cannot install the package:\n\ntea.csv"
  },
  {
    "objectID": "materials/11_hyp_tests.html#slides-and-code",
    "href": "materials/11_hyp_tests.html#slides-and-code",
    "title": "Hypothesis Tests",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/8) lecture: Hypothesis tests\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (11/11) lecture: More hypothesis tests\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/09_sampling.html",
    "href": "materials/09_sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "MD Ch 7"
  },
  {
    "objectID": "materials/09_sampling.html#readings",
    "href": "materials/09_sampling.html#readings",
    "title": "Sampling",
    "section": "",
    "text": "MD Ch 7"
  },
  {
    "objectID": "materials/09_sampling.html#data",
    "href": "materials/09_sampling.html#data",
    "title": "Sampling",
    "section": "Data",
    "text": "Data\n\nInstall the infer package to follow along:\n\n\ninstall.packages(\"infer\")\n\n\nReinstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nCSV files for the data from this week if you cannot install the package:\n\ngov50_class_years.csv"
  },
  {
    "objectID": "materials/09_sampling.html#slides-and-code",
    "href": "materials/09_sampling.html#slides-and-code",
    "title": "Sampling",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (10/31) lecture: Sampling\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (11/2) lecture: Sampling distributions\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/07_prediction.html",
    "href": "materials/07_prediction.html",
    "title": "Prediction: Elections and Regression",
    "section": "",
    "text": "For prediction, see QSS 4.1 (you can skip 4.1.1)\nFor regression, see either MD Ch 5 or QSS 4.2 (you can skip 4.2.5)."
  },
  {
    "objectID": "materials/07_prediction.html#readings",
    "href": "materials/07_prediction.html#readings",
    "title": "Prediction: Elections and Regression",
    "section": "",
    "text": "For prediction, see QSS 4.1 (you can skip 4.1.1)\nFor regression, see either MD Ch 5 or QSS 4.2 (you can skip 4.2.5)."
  },
  {
    "objectID": "materials/07_prediction.html#data",
    "href": "materials/07_prediction.html#data",
    "title": "Prediction: Elections and Regression",
    "section": "Data",
    "text": "Data\n\nFor Thursday, you may want to install the broom package:\n\n\ninstall.packages(\"broom\")"
  },
  {
    "objectID": "materials/07_prediction.html#slides-and-code",
    "href": "materials/07_prediction.html#slides-and-code",
    "title": "Prediction: Elections and Regression",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (10/17) lecture: Midterm review and prediction\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nTuesday (10/19) lecture: Regression\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/05_measurement.html",
    "href": "materials/05_measurement.html",
    "title": "Measuring and Summarizing Data",
    "section": "",
    "text": "Chapter 3 of QSS (though 3.6)"
  },
  {
    "objectID": "materials/05_measurement.html#readings",
    "href": "materials/05_measurement.html#readings",
    "title": "Measuring and Summarizing Data",
    "section": "",
    "text": "Chapter 3 of QSS (though 3.6)"
  },
  {
    "objectID": "materials/05_measurement.html#slides-and-code",
    "href": "materials/05_measurement.html#slides-and-code",
    "title": "Measuring and Summarizing Data",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (10/3) lecture: Summarizing data\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture9.Rmd\n\nThursday (10/5) lecture: Survey Sampling\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/03_data_wrangling.html",
    "href": "materials/03_data_wrangling.html",
    "title": "Data Transformation and Causality",
    "section": "",
    "text": "Chapter 3 of Modern Dive\nChapter 2.1-2.2 of QSS"
  },
  {
    "objectID": "materials/03_data_wrangling.html#readings",
    "href": "materials/03_data_wrangling.html#readings",
    "title": "Data Transformation and Causality",
    "section": "",
    "text": "Chapter 3 of Modern Dive\nChapter 2.1-2.2 of QSS"
  },
  {
    "objectID": "materials/03_data_wrangling.html#data",
    "href": "materials/03_data_wrangling.html#data",
    "title": "Data Transformation and Causality",
    "section": "Data",
    "text": "Data\n\nInstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")"
  },
  {
    "objectID": "materials/03_data_wrangling.html#slides-and-code",
    "href": "materials/03_data_wrangling.html#slides-and-code",
    "title": "Data Transformation and Causality",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (9/19) lecture: Data Wrangling and Barplots\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture5.Rmd\n\nThursday (9/21) lecture: Causality\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture6.Rmd"
  },
  {
    "objectID": "materials/01_intro.html",
    "href": "materials/01_intro.html",
    "title": "Introduction to Gov 50",
    "section": "",
    "text": "The syllabus and schedule pages.\nReview and try to start Problem Set 0"
  },
  {
    "objectID": "materials/01_intro.html#readings",
    "href": "materials/01_intro.html#readings",
    "title": "Introduction to Gov 50",
    "section": "",
    "text": "The syllabus and schedule pages.\nReview and try to start Problem Set 0"
  },
  {
    "objectID": "materials/01_intro.html#code",
    "href": "materials/01_intro.html#code",
    "title": "Introduction to Gov 50",
    "section": "Code",
    "text": "Code\nTo follow along with the visualization material on Thursday, run the following code at the R Console in RStudio:\n\ninstall.packages(\"gapminder\", repos = \"http://cran.rstudio.com\")"
  },
  {
    "objectID": "materials/01_intro.html#slides",
    "href": "materials/01_intro.html#slides",
    "title": "Introduction to Gov 50",
    "section": "Slides",
    "text": "Slides\n\nTuesday (9/5) lecture:\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (9/7) lecture:\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture2.Rmd"
  },
  {
    "objectID": "assignments/tutorials.html",
    "href": "assignments/tutorials.html",
    "title": "Gov 50 R Tutorials Instructions",
    "section": "",
    "text": "Once you have completed the tutorial you will see a screen that prompts you to enter your name (as it appears on Gradescope) and produce a submission report. Clicking “Download” should place a report into the folder of your choosing. Find that PDF and upload it to Gradescope (see the link at the top of every page).\n\nNote that we do not grade these reports on correctness, just that you attempted the tutorial. That said, going slowly and making sure that you understand each question and its answer is a good practice to ensure you are mastering the course material."
  },
  {
    "objectID": "assignments/tutorials.html#submitting-tutorial-reports-to-gradescope",
    "href": "assignments/tutorials.html#submitting-tutorial-reports-to-gradescope",
    "title": "Gov 50 R Tutorials Instructions",
    "section": "",
    "text": "Once you have completed the tutorial you will see a screen that prompts you to enter your name (as it appears on Gradescope) and produce a submission report. Clicking “Download” should place a report into the folder of your choosing. Find that PDF and upload it to Gradescope (see the link at the top of every page).\n\nNote that we do not grade these reports on correctness, just that you attempted the tutorial. That said, going slowly and making sure that you understand each question and its answer is a good practice to ensure you are mastering the course material."
  },
  {
    "objectID": "assignments/tutorials.html#installing-the-gov-50-tutorials-package",
    "href": "assignments/tutorials.html#installing-the-gov-50-tutorials-package",
    "title": "Gov 50 R Tutorials Instructions",
    "section": "Installing the Gov 50 tutorials package",
    "text": "Installing the Gov 50 tutorials package\nOur tutorials will be hosted on an external website so that you should not have to install any packages on your own computer. However, occasionally the server will become slow or unresponsive when there are many students trying to complete the tutorial at once. When this happens, you can install the tutorials locally to your RStudio. To install the package that contains the tutorials for the class run the following lines of code, each entered separately. Note that you may be asked to update packages when you enter this code. You can select 1 for “All” to perform any updates.\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"mattblackwell/gov50tutor\")\n\nAfter the package has been installed, you should be able to find all of the tutorials for the course in the “Tutorials” tab in the top-right pane of RStudio. Scroll down to find a Gov 50 tutorial and click “Start Tutorial” to launch a tutorial:\n\nIf you do not see any QSS tutorials after scrolling to the bottom, try to restart RStudio, and check again. If you still don’t see any tutorials, confirm that you have installed the package by running library(gov50tutor). If you get an error about the package not being installed, try the above installation procedure again and note any error messages you receive before reaching out to the teaching staff for assistance."
  },
  {
    "objectID": "assignments/tutorials.html#faqs",
    "href": "assignments/tutorials.html#faqs",
    "title": "Gov 50 R Tutorials Instructions",
    "section": "FAQs",
    "text": "FAQs\n\nWhat if I receive an error about pdflatex and the submission report doesn’t download?\nTo generate the reports, you need something called LaTeX. You should have installed this with Problem Set 0, but the following command will install it:\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX\n\n\n\nWhat if I don’t see a Tutorial tab in RStudio?\nMake sure that you have RStudio version 1.3 or higher installed on your computer. On a Mac, you can check the version by going to the top left of the menu bar (next to the Apple logo) and clicking on “RStudio” then “About RStudio”. On a Windows PC, you can find the same item under File &gt; About.\n\n\nWhat if my submission report misses some of my attempts?\nThe submission report isn’t perfect and will sometimes say you didn’t attempt a question when you actually did. We do not grade strictly on this and are mostly looking for a confirmation that you made a good-faith effort to complete the tutorial."
  },
  {
    "objectID": "assignments/problem-sets.html",
    "href": "assignments/problem-sets.html",
    "title": "Problem Sets and Exams",
    "section": "",
    "text": "For both problem sets and exams, you will end up producing two types of documents:"
  },
  {
    "objectID": "assignments/problem-sets.html#problem-set-workflow",
    "href": "assignments/problem-sets.html#problem-set-workflow",
    "title": "Problem Sets and Exams",
    "section": "Problem set workflow",
    "text": "Problem set workflow\nThe text of the problem sets will be posted here on the course website. We will distribute templates and data for the problem sets via Github. To get started, follow these steps:\n\nGo to the Ed Board and find the pinned thread with the GitHub Classroom links for each assignment.\nClick on the Github Classroom link for the assignment in question. You’ll be presented with a page that asks you to accept the assignment. Click “Accept this assignment”.\nGo to this new problem set repository on GitHub. Click on the green “Code” button toward the top right of the page. When you click that, a popup will appear and you can copy the URL (you can click the little clipboard icon to copy this automatically).\n\n\n\nNow, switch to RStudio. Go to the menu bar and hit “File &gt; New Project”. You can then choose what type of project to start. Since we’re importing from Github, we’ll use the “Version Control” option (it’s the bottom of the list). In the next menu, choose “Git”. Now, you can paste the URL in the “Repository URL” box. Choose a set of local directories to place this project and hit “Create Project”. Because this is a private repository, you will be prompted to enter your GitHub username and password. Go ahead and do this.\nAs you make changes to your file, make sure to commit those changes. Every commit is a point that you can always go back to if your code stops working for some reason. So doing this often will make sure that you have access to that great feature of git. At an absolute minimum, you should do this every time you complete a question, though you may want to do it more often than that. Committing often means you also don’t have to constantly create new versions of the same file because you’re afraid of making some change. Also, make sure to use informative commit messages so you know what’s changing. Important: be sure to answer the questions in the Rmd file provided by the template. This is what the autograder will look for when you submit.\nAt some point, you’ll want to push your changes back to GitHub so the teaching staff can see your work (if you need help) or so that you can submit to Gradescope."
  },
  {
    "objectID": "assignments/problem-sets.html#submitting-your-assignments",
    "href": "assignments/problem-sets.html#submitting-your-assignments",
    "title": "Problem Sets and Exams",
    "section": "Submitting your assignments",
    "text": "Submitting your assignments\n\nStep 1: Submit the GitHub repository to Gradescope\nOn Gradescope, you’ll find two entries for each assignment. One will be labelled “Autograder” and the other “PDF”. Click on the Autograder entry and select the GitHub submission option. The first time you do this step, you will first need to connect your GitHub account to your Gradescope account.\n\nOnce you have connected your GitHub account, you can find the repository for the relevant problem set and the “main” branch and hit “Submit”.\n\n\n\nStep 2: Check autograder output\nWhen you submit the repository, Gradescope will run a series of tests on your code to see if it runs and to check that you have completed the coding tasks asked of you in the assignment. You’ll see a list of test results and an overall score.\n\nCheck the messages on any questions you got wrong for hints about how to fix them. You may then resubmit the GitHub repository again once you have fixed the code and pushed your changes to GitHub. You should repeat this process as many times as needed before you get a perfect score.\n\n\nStep 3: Upload PDF\nOnce you are satisfied with your autograding, then you should produce your final PDF report by hitting the “Knit” button at the top of the RStudio editor. This should produce a PDF output that you can then upload to Gradescope. In Gradescope, there will be an assignment labelled “Problem Set X: PDF”. There, you will be prompted to upload your final PDF report. You’ll be asked to indicate on which page each answer is. Label the PDF with where those answers are (being sure to mark two different pages if an answer continues on two pages)."
  },
  {
    "objectID": "assignments/problem-sets.html#checklist",
    "href": "assignments/problem-sets.html#checklist",
    "title": "Problem Sets and Exams",
    "section": "Checklist",
    "text": "Checklist\n\nAccept GitHub Classroom invite.\nUse RStudio to “Clone with HTTPS” this repo to your computer.\nMake progress editing the .Rmd file.\n\nAs you do, commit and push to GitHub regularly.\nYou can check your answers with the autograder at any point.\nMake sure to name your code chunks.\nAdd the names of anyone you discuss the problem set with on your write-up.\n\nPush final version to GitHub.\nSubmit the GitHub repository to the Gradescope autograder.\nUpload final .pdf file to Gradescope."
  },
  {
    "objectID": "assignments/problem-sets.html#writing-good-code",
    "href": "assignments/problem-sets.html#writing-good-code",
    "title": "Problem Sets and Exams",
    "section": "Writing good code",
    "text": "Writing good code\nYou’ll be writing code to do the analyses in this class. Code, like any language, provides many different ways of saying the same thing. One good practice of coding is to have what’s called good coding style. This refers to how you format the code that you so that it is (a) easy for you and others to read, and (b) less prone to making mistakes. Here are some general guidelines for writing R code and Rmd file.\n\nWhen writing R code and unless we tell you otherwise, follow the tidyverse style guide. For this class, the relevant parts of this document are the first couple of chapters. If you are ever uncertain about how to name something or how to write some code, see this document and it will likely help you quite a bit.\nMake sure the code chunks in the Rmd file have blank lines above and below them. If you don’t have this, it can sometimes cause problems with compilation.\nTry to keep your lines of code shorter than 80 characters since this makes reading code much easier. Usually this means writing some function arguments on a different line. See the style guide (section 2.5) for more on this.\nInclude comments in your code and format them nicely as in section 3.4 of the style guide. These comments should explain why you wrote the code you wrote and any notes you had about how you came to this solution. This might include, say, other approaches you tried but didn’t work or approaches you might want to try if you ever revisit this. Look at our code and examples to get a sense of how to use comments. In RStudio, you can nicely format a comment by hitting Control-Shift-\\."
  },
  {
    "objectID": "assignments/final-project.html",
    "href": "assignments/final-project.html",
    "title": "Final Project Information",
    "section": "",
    "text": "The final project is a data analysis project about whatever data excites you. That could be public opinion in U.S. elections, United Nations voting patterns, patterns of racial discrimination in police behavior, the distribution of salaries in basketball, or the economics of the Marvel Cinematic Universe. No matter the topic, you will formulate a key research question, find data on that question, answer the question using the tools of the course, and present those results for public consumption.\nHere is a list of milestones that we will have to keep you on track:"
  },
  {
    "objectID": "assignments/final-project.html#milestone-1-create-public-github-repository-due-929",
    "href": "assignments/final-project.html#milestone-1-create-public-github-repository-due-929",
    "title": "Final Project Information",
    "section": "Milestone 1: Create public GitHub repository (due 9/29)",
    "text": "Milestone 1: Create public GitHub repository (due 9/29)\nFollow the instructions on how to create a repository for your final project. You should change the metadata on your article and write a few sentences in the main text about what you might be interested in writing about. You will submit a link to the public article (not the github repository) to Canvas (not Gradescope)."
  },
  {
    "objectID": "assignments/final-project.html#milestone-2-finding-data-and-writing-a-proposal-due-1013",
    "href": "assignments/final-project.html#milestone-2-finding-data-and-writing-a-proposal-due-1013",
    "title": "Final Project Information",
    "section": "Milestone 2: Finding data and writing a proposal (due 10/13)",
    "text": "Milestone 2: Finding data and writing a proposal (due 10/13)\n\nFinding a data source\nThe biggest part of the final project in Gov 50 is finding a data source. Here are some other resources and repositories with data sets:\n\nList of links to political science datasets\nAnother list of political science datasets\nDataIsPlural database of datasets\nHarvard Dataverse - Social Science\nData.gov - Data sets released by the US government\nData published by FiveThirtyEight\nOpen datasets on climate\nHarvard OpenData Group Directory\nHarvard Program on Survey Research Data Collections\nRoper Center : Public Opinion in the US\nPew Research Center Data Sets\nKaggle - Platform for data science competitions with tons of data sets\nAfrobarometer - Multicountry survey of African residents\nLatinobarometer - Multicountry survey of Latin American residents\nAmerican National Election Survey\nCooperative Elections Survey\n\nIf you find a data set that you think is interesting, but you have problems loading the data set into R, please contact the course staff. R can load almost anything, so we can likely help you.\nWe also have the following specific datasets available to use (you will need to merge these with other data for the purposes of the final project):\n\nAmerican National Election Survey, 2016: Data Codebook\nCivil Wars: Data Codebook\nPolitical Violence: Data Codebook\nFox News rollout: Data Codebook\nAfrobarometer: Data Codebook\n\n\n\nGeneral advice for choosing data sources\n\nIf you want to analyze the relationship between X and Y, make sure that these two variables are included in the data set. If you want to look at effects for subgroups, make sure there is a variable that you can use for subsetting.\nTry to look for a ‘codebook’ or some other document that explains what the variables mean and how they are coded.\nFor most projects, preparing the data for analysis takes longer than the actual analysis itself. Try to find a data set where you do not need to extensively recode / clean up the data before you run your analyses, this makes the final project easier.\nIn similar vein, if the data set is greater than about 50MB (this is not a hard cutoff), R commands and analyses tend to take longer.\nImportant: Please do not try to commit any file that is larger than 100MB. You will not be able to push these files to GitHub and will require some complicated work to undo the commits. Instead, you can write an R script that subsets the data to units/variables that you need and save that file, which should be considerably smaller. Then you can put that in your repository and commit that.\nData from experiments is usually simple to analyze, since the analysis commonly involves simple comparisons of group means.\n\n\n\nWriting a proposal\nYou should write a one-paragraph note to describe what data set you will use and what your tentative research question is. Your research question should ask how one dependent variable is related to one or more independent variables. That is, your research question should be able to be answered by a regression analysis. In this paragraph, you should do the following:\n\nState your research question.\nFormulate a hypothesis related to the research question. This hypothesis should be rooted in some sort of theory. In other words, you need to present a plausible story why the hypothesis might be true. Often, this is in the form of a behaviorial or institutional explanation. As social scientists, we are not interested in idiosyncratic explanations; we want to understand systematic patterns and relationships!\nDescribe your explanatory variable(s) of interest and how it is measured. Importantly, we need to observe variation in this variable in order to study it!\nDescribe your outcome variable of interest and how it is measured.\nWhat observed pattern in the data would provide support for your hypothesis? More importantly, what observed pattern would disprove your hypothesis?\n\nFor instance, this would be a comprehensive paragraph that address each of these points in detail:\n\nDoes unified government enhance legislative productivity? In this study, I plan to examine the extent to which periods of unified government produce more landmark laws. I hypothesize that legislative productivity increases during periods of unified government in which one party controls both Houses of Congress and the presidency relative to periods of divided government. During periods of unified government, I expect that it is more likely for major bills to pass both Houses and gain the president’s signature. During periods of divided government, it is more difficult to reach a consensus around legislation that can pass each House and gain the president’s approval. My sample is comprised of each of the 79th (1945-1946) through 103rd (1993-1994) Congresses. My unit of analysis is a Congress (e.g., the 88th Congress). The explanatory variable of interest is whether there is unified government (both Houses and the presidency are controlled by the same party) or divided government. The variable is coded =1 for unified government and =0 for divided government. My outcome variable is the count of landmark pieces of legislation passed in a given Congress. For instance, if the variable were coded =11, it would mean that 11 pieces of landmark legislation were signed into law in that Congress. This variable is measured from David Mayhew’s data set on landmark legislation and relies on Mayhew’s expert knowledge to classify legislation as “landmark.” If I observe greater landmark legislative productivity under unified government relative to divided government, this would provide support for my hypothesis. If, on the other hand, I observe less productivity or the same level of productivity under unified government, this would provide evidence against my hypothesis. When I run my regression of the count of landmark legislation on the unified government indicator variable, a positive, significant coefficient would indicate support for my hypothesis.\n\nYour paragraph might be less detailed and may not refer to any academic literature, but it should address the above items.\nNote that you are not fully committing to any specific question or data in this exercise. If you want to change data or questions later, that is fine. This is just a milestone to keep you on track. You can write this proposal on your index.Rmd file in the public article. You will submit a link to the rendered article with the proposal.\nNote that you will eventually remove this proposal with the actual article. If down the road you don’t want this proposal to be part of the public history of the repository, you can always create a new repository for the final report."
  },
  {
    "objectID": "assignments/final-project.html#milestone-3-one-data-visualization-due-113",
    "href": "assignments/final-project.html#milestone-3-one-data-visualization-due-113",
    "title": "Final Project Information",
    "section": "Milestone 3: One data visualization (due 11/3)",
    "text": "Milestone 3: One data visualization (due 11/3)\nThe next milestone will require that your Distill article loads the data you have selected and produces one interesting and polished data visualization. This could either show the distribution of one variable or the relationship between two variables."
  },
  {
    "objectID": "assignments/final-project.html#milestone-4-add-results-from-one-analysis-due-1117",
    "href": "assignments/final-project.html#milestone-4-add-results-from-one-analysis-due-1117",
    "title": "Final Project Information",
    "section": "Milestone 4: Add results from one analysis (due 11/17)",
    "text": "Milestone 4: Add results from one analysis (due 11/17)\nBy this time, your article should contain one visualization and one analysis that attempts to answer your research question. There does not need to be a long discussion, but the results should be presented in either a second visualization or a nicely formatted table."
  },
  {
    "objectID": "assignments/final-project.html#final-step-write-up-final-report-due-1213",
    "href": "assignments/final-project.html#final-step-write-up-final-report-due-1213",
    "title": "Final Project Information",
    "section": "Final step: Write up final report (due 12/13)",
    "text": "Final step: Write up final report (due 12/13)\nThe final report will include the following sections: (1) an introduction where you introduce the research question and hypothesis and briefly describe why it is interesting; (2) a data section that briefly describes the data source, describes how the key dependent and independent variables are measured (e.g., a survey, statistical model, or expert coding), and also produces a plot that summarizes the dependent variable; (3) a results section that contains a scatterplot, barplot, or boxplot of the main relationship of interest and output for the main regression of interest; and (4) a brief (one paragraph) concluding section that summarizes your results, assesses the extent to which you find support for your hypothesis, describes limitations of your analysis and threats to inference, and states how your analysis could be improved (e.g., improved data that would be useful to collect).\nFor the data section, you should note if your research design is cross-sectional (most projects will be of this type) or one of the other designs we discussed (randomized experiment, before-and-after, differences-in-differences). For the results section, you should interpret (in plain English) the main coefficient of interest in your regression. You should also comment on the statistical significance of the estimated coefficient and whether or not you believe the coefficient to represent a causal effect."
  },
  {
    "objectID": "assignments/06_hw_sampling.html",
    "href": "assignments/06_hw_sampling.html",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "",
    "text": "This problem set is due on November 8, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/06_hw_sampling.html#background",
    "href": "assignments/06_hw_sampling.html#background",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Background",
    "text": "Background\nIn this exercise, we will focus on sampling and sampling distributions when we have access to an entire census for a given population. In this case, the data/durham.csv file contains anonymized data on all registered voters in Durham County, NC that were registered to vote on election day of the 2020 presidential election. The variables in this dataset are:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nncid\nunique voter identification number\n\n\nreg_year\ndid person vote (1) or not (0) in 1994?\n\n\nage\nage of registered voter\n\n\ngender\nvoter gender identity (\"F\" = Female, \"M\" = Male, \"U\" = Undeclared)\n\n\nrace\nvoter racial identity (see table below)\n\n\nlatino\nvoter identifies as Hispanic or Latino (\"HL\"), does not (\"NL\"), or undesignated (\"UN\")\n\n\nparty\nparty registration (\"DEM\" = Democrat, \"Rep\" = Republican, \"LIB\" = Libertarian, \"GRE\" = Green, \"UNA\" = Unaffiliated)\n\n\ndrivers_lic\nvoter has drivers license (1) or not (0)?\n\n\n\nNorth Carolina provides the voter history data in a separate file with a row for each voter who actually casts a ballot in a given election. In the data/vote_history.csv file is a row for every voter that cast a ballot in the 2020 general election. The variables in this dataset are:\n\n\n\nName\nDescription\n\n\n\n\nncid\nunique voter identification number\n\n\nvote_method_2020\nmethod of voting in 2020 general election\n\n\n\nFor the purposed of this exercise, we will treat the voter list from durhman.csv as the population of interest. Doing so is an increasingly common approach for polling, where pollsters are now using the voter file as a sampling frame to conduct their polls. We will repeated sample from this population to better understand sampling uncertainty.\nNote: please follow the directions carefully about setting the seed for the sampling based questions.\nIn the Durham data, the racial category codes are:\n\n\n\nRace\nDescription\n\n\n\n\nA\nASIAN\n\n\nB\nBLACK or AFRICAN AMERICAN\n\n\nI\nAMERICAN INDIAN or ALASKA NATIVE\n\n\nM\nTWO or MORE RACES\n\n\nO\nOTHER\n\n\nP\nNATIVE HAWAIIAN or PACIFIC ISLANDER\n\n\nU\nUNDESIGNATED\n\n\nW\nWHITE"
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-1-6-points",
    "href": "assignments/06_hw_sampling.html#question-1-6-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 1 (6 points)",
    "text": "Question 1 (6 points)\nLoad the voter list data (data/durham.csv) into R using read_csv and save the data as durham and use the same function to load in the voter history data (data/vote_history.csv) as vote_history. Use left_join to merge the voter history data into the voter list data so that all rows of the voter list remain. Use the ncid variable to join the data sets and save this merged data as durham.\nAny registered voter who did not turn out in the 2020 general election will not appear in the vote_history data so their vote_method_2020 variable will be missing in the merged data. Create a new variable called vote_gen_2020 that is 1 if the voter turned out and 0 otherwise (the is.na() function may be helpful).\nIn the write-up, state how many units are in the population (that is, how many rows are in the durham data) and the proportion of registered voters that actually voted in the 2020 elections.\nRubric: 1pt for Rmd file compiling (autograder); 3pt for data loaded and merged (autograder), 1pt for creation of the vote_gen_2020 variable (autograder); 1pt for number of rows and turnout proportion reported (PDF)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-2-4-points",
    "href": "assignments/06_hw_sampling.html#question-2-4-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 2 (4 points)",
    "text": "Question 2 (4 points)\nCreate a density histogram of age with a bin width of 1 and save this plot as age_hist (use the aesthetic mapping y = after_stat(density) in to accomplish this). Create a barplot for turnout (vote_gen_2020) with the proportion on the y-axis (use the aesthetic mapping y = after_stat(prop) in geom_barplot to achieve this). For a slightly nicer plot, use the width = 1 argument. Make sure both of these plots are shown in the PDF.\nRubric: 2pts for age_hist (autograder); 2pts for turnout_hist (autograder)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-3-5-points",
    "href": "assignments/06_hw_sampling.html#question-3-5-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 3 (5 points)",
    "text": "Question 3 (5 points)\nUse summarize() to calculate the population mean and standard deviation of age and vote_gen_2020 (that is the mean and SD of these variables in the durham data) and save the resulting tibble as pop_parameters with the tibble output looking like:\n# A tibble: 1 × 4\n  age_pop_mean age_pop_sd turnout_pop_prop turnout_pop_sd\n         &lt;dbl&gt;      &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1         XX.X       XX.X            X.XXX          X.XXX\nMake sure that the column names are the same for the autograder. (Hint: you can summarize multiple variables in the same call to summarize.) Use knitr::kable() to present the values in nicely formatted table with digits = 2 to create nicely rounded numbers and informative column names (they may need to be abbreviated to fit on the page).\nRubric: 4pts for correct pop_parameters tibble (autograder); 1pts for nicely formatted table (PDF)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-4-8-points",
    "href": "assignments/06_hw_sampling.html#question-4-8-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 4 (8 points)",
    "text": "Question 4 (8 points)\nIn the first line of the code chunk for this question use the following code:\n\nlibrary(infer)\nset.seed(02138)\n\nThen use rep_slice_sample() to take 1,000 samples of size 200 from durham and calculate the sample mean of age and the sample mean/proportion of turnout for each of these samples. Save these as variables named age_mean and turnout_prop and save the resulting tibble as samples_n200.\nCreate a density histogram of the age means and with a bin width of 1 and save this as age_mean_hist (use a bin width of 0.5). Create a density histogram of the turnout proportions and save this as turnout_prop_hist (use a bin width of 0.005) (NOTE: this is not a bar plot like in Question 2). Make sure both of these plots are shown in the PDF and that they have informative labels.\nIn the write-up, compare the sampling distribution of the sample mean and sample proportion here to the population distributions from question 2. Are the shapes of the sampling distributions similar or different to the population distributions? If different, how are they different?\nRubric: 2pts for correct sample_n200 output (autograder); 2pts for correct age_mean_hist (autograder); 2pts for correct turnout_prop_hist (autograder); 1pt informative labels (PDF); 1pt comparison to population distributions (PDF)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-5-7-points",
    "href": "assignments/06_hw_sampling.html#question-5-7-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 5 (7 points)",
    "text": "Question 5 (7 points)\nUse the summarize() function on samples_n200 to calculate the average (named ev_age and ev_turnout) and standard deviation (named se_age and se_turnout) of each sample mean/proportion across the repeated samples. Save this tibble as samp_dist_summary and it should look like this:\n# A tibble: 1 × 4\n  ev_age se_age ev_turnout se_turnout\n   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1   X.XX   X.XX      X.XXX      X.XXX\nMake sure that the column names are the same for the autograder. Use knitr::kable() to present the values in nicely formatted table with digits = 2 to create nicely rounded numbers.\nCompare the mean and SD of these sampling distributions to the population means and SDs from the previous question. Are these distributions centered on the same value? Which has more spread, the population distribution of age/turnout or the sampling distributions of their means?\nRubric: 4pts for correct samp_dist_summary tibble (autograder); 1pt for nicely formatted table (PDF); 2pts for discussion (PDF)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-6-5-points",
    "href": "assignments/06_hw_sampling.html#question-6-5-points",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 6 (5 points)",
    "text": "Question 6 (5 points)\nNow suppose we received a bad voter file that only contained voters under the age of 30 at the time of the 2020 election. Set the seed using set.seed(02138) and repeat the replicate sampling from Question 3 to take 1,000 samples of size 200 from durham after removing voters 30 and over. Calculate the sample proportion of turnout for each of these samples. Save this as a variable named turnout_prop and save the resulting tibble as young_samples_n200.\nCreate a density histogram of the turnout proportions with informative labels and add a vertical line at the true population average turnout (of all ages). Save this plot as young_turnout_prop_hist.\nDoes the sampling distribution of turnout in these samples appear biased or unbiased for the population average turnout? Explain why based on the sampling process.\nRubric: 2pts for correct young_samples_n200 tibble (autograder); 1pt for correctly identifying biased/unbiased (PDF); 2pt explaining bias/unbiasedness (PDF)."
  },
  {
    "objectID": "assignments/04_hw_summarizing.html",
    "href": "assignments/04_hw_summarizing.html",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "",
    "text": "This problem set is due on October 11, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#background",
    "href": "assignments/04_hw_summarizing.html#background",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Background",
    "text": "Background\nIn this problem set, we will analyze the relationship between the gender composition among a judge’s children and voting behavior among circuit court judges. In a recent paper, Adam N. Glynn and Maya Sen argue that having a female child causes circuit court judges to make more pro-feminist decisions. The paper can be found at:\n\nGlynn, Adam N., and Maya Sen. (2015). “Identifying Judicial Empathy: Does Having Daughters Cause Judges to Rule for Women’s Issues?.” American Journal of Political Science Vol. 59, No. 1, pp. 37–54.\n\nThe dataset judges.csv contains the following variables about individual judges:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nThe judge’s name\n\n\nnum_kids\nThe number of children each judge has.\n\n\ncircuit\nWhich federal circuit the judge serves in.\n\n\ngirls\nThe number of female children the judge has.\n\n\nprogressive_vote\nThe proportion of the judge’s votes on women’s issues which were decided in a pro-feminist direction.\n\n\nrace\nThe judge’s race (1 = white, 2 = African-American, 3 = Hispanic, 4 = Asian-American).\n\n\nreligion\nThe judge’s religion (1 = Unitarian, 2 = Episcopalian, 3 = Baptist, 4 = Catholic, 5 = Jewish, 7 = Presbyterian, 8 = Protestant, 9 = Congregationalist, 10 = Methodist, 11 = Church of Christ, 16 = Baha’i, 17 = Mormon, 21 = Anglican, 24 = Lutheran, 99 = unknown).\n\n\nrepublican\nTakes a value of 1 if the judge was appointed by a Republican president, 0 otherwise. Used as a proxy for the judge’s party.\n\n\nsons\nThe number of male children the judge has.\n\n\nwoman\nTakes a value of 1 if the judge is a woman, 0 otherwise.\n\n\nyearb\nThe year the judge was born."
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-1-10-points",
    "href": "assignments/04_hw_summarizing.html#question-1-10-points",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 1 (10 points)",
    "text": "Question 1 (10 points)\nLoad the tidyverse package in the setup chunk. In the first chunk for this question use read_csv to load the data/judges.csv file into a data frame called judges. In this exercise, you will create a cross-tab that shows the gender breakdown within each party. In particular, the result should be a table with 2 rows and 3 columns, where the rows represent each gender and the columns correspond to party. In the end, you should have a table that looks like this (with the correct proportion, not these made up ones):\n\n\n\nGender\nDemocrat\nRepublican\n\n\n\n\nMan\n0.5\n0.6\n\n\nWoman\n0.5\n0.4\n\n\n\nNote that the columns here sum to 1. You can create this using the following data wrangling steps:\n\nOverwrite the judges tibble with the same tibble with two newly created variables: one variable called Gender variable to be labelled \"Woman\" and \"Man\" based on the variable woman and another variable called Party to be labelled \"Republican\" and \"Democrat\" based on the republican variable.\nUse the judges tibble to create the cross-tab and save it as gender_party_table. It should have 2 rows and 3 columns (“Gender”, “Republican”, and “Democrat”). Be very careful about the ordering of your grouping and remember that summarize() drops the last group by default when there is more than one group. You should use group_by(), summarize(), mutate(), and pivot_wider() to accomplish this. (Hint: if you are ending up with the wrong number of rows, make sure that the only variables in the tibble when you try to use pivot_wider are the two grouping variables and the proportion variable.)\nPass gender_party_table to knitr::kable() to create a nicely formatted version of this table.\n\nIn your write-up, answer the following questions:\n\nHow many judges are in this data set?\nWhat proportion of the judges are men? (hint: use mean)\nFrom your table, is the gender breakdown different for judges appointed by Democratic vs Republican presidents?\n\nRubric: 2pts for creating Gender and Party (autograder); 5pts for gender_party_table (autograder); 1pt for properly formatted table (PDF); 2pts for answering question (PDF)."
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-2-7-points",
    "href": "assignments/04_hw_summarizing.html#question-2-7-points",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 2 (7 points)",
    "text": "Question 2 (7 points)\nUse group_by and summarize to calculate the mean of progressive_vote in each combination of the Gender and Party variables you created in question 1 and save this tibble as gender_party_means (this tibble should have columns Gender, Party, and progressive_vote). Use this table to recreate the following barplot:\n\nThe autograder will check the aesthetic mappings, the correct placement of the bars, and the labels, but you do not need to match the exact colors (they are \"steeblue1\" and \"indianred1\" if you do want to match, though).\nIn the main text, briefly interpret the results of the analysis by describing which party and which gender tend to be more progressive. Comment on whether there are larger differences across party or across gender in terms of progressive voting. Should we interpret any of these effects causally? Why or why not?\nRubric: 2pts for gender_party_means table (autograder); 3pts for gender_party_plot (autograder); 2pts for interpretation (PDF)"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-3-9-points",
    "href": "assignments/04_hw_summarizing.html#question-3-9-points",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 3 (9 points)",
    "text": "Question 3 (9 points)\nWhat is the difference in the proportion of pro-feminist decisions between judges who have at least one daughter and those who do not have any? To compute this difference, first create a variable called any_girls that is \"Any Girls\" when the judge has at least 1 girl and \"No Girls\" otherwise and save this variable back to the judges tibble. Then, created a new tibble called parents that is filtered to contain judges that have at least one child. Create an object called vote_by_girls that has three columns: the unique values of the any_girls variable, the mean of the progressive_vote variable for each value of any_girls (column called progressive_mean), and the standard deviation of progressive_vote for each value of any_girls (column called progressive_sd). Present the table using knitr::kable().\nIn the main text, describe which group has a higher average progressive vote and which group’s distribution has more spread. Why might we worry about interpreting any difference in means causally, considering number of children as a possible confounder?\nRubric: 1pt for creating any_girls variable (autograder); 1pt for creating parents tibble (autograder); 4pts for creating the vote_by_girls table (autograder); 1pt for description of the table (PDF); 2pts for discussion of causality (PDF)."
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-4-9-points",
    "href": "assignments/04_hw_summarizing.html#question-4-9-points",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 4 (9 points)",
    "text": "Question 4 (9 points)\nGiven that the number of children might be a confounder for the relationship between number of girls and voting, let’s estimate the average treatment effects of having girls using statistical control for the number of children among judges that have one to three children (that is, first filter to judges that only have between 1 and 3 children, inclusive). Your final table should be called ate_nkids and should be a tibble that has two columns: one with the number of children (1, 2, and 3) and the other with the estimated ATEs of any_girls for each of those levels. Print out this table using the knitr::kable() command.\nAre these estimated effects largely similar or largely different than what you found using all of the data? What assumption do you need to make to interpret these effects causally? Do you think it is plausible in this case?\nRubric: 6pts for ate_nkids (autograder); 1pt for nicely formatted table (PDF); 2pts for discussion (PDF)"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-5-extra-credit-5-points",
    "href": "assignments/04_hw_summarizing.html#question-5-extra-credit-5-points",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 5 (EXTRA CREDIT, 5 points)",
    "text": "Question 5 (EXTRA CREDIT, 5 points)\nThis problem is optional. Any points earned on this problem can be applied to lost points on other parts of the problem set. You cannot earn more than the maximum score on the problem set. There will be no autograder for this question.\nLet’s consider the design of this study. The original authors assume that, conditional on the number of children a judge has, the number of daughters is random (as we did in the previous question). If this is true, half of a judge’s children should be female, on average. A deviation from this proportion could indicate that a gender preference among judges due a stopping rule such as “have children until we get one girl,” which would violate the randomization assumption.\nTo check this assumption, group the data by the number of children and calculate the proportion of children in each group that are girls. Create a barplot that these proportions on the y-axis with the number of children on the x-axis. This barplot should have (a) informative labels on each axis, (b) a y-axis range that runs from 0 to 1, and (c) a horizontal line at 0.5 to compare against. Does it appear that there is strong gender preference/selection happening among judges?"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html",
    "href": "assignments/02_hw_data_wrangling.html",
    "title": "Problem Set 2: Data Wrangling",
    "section": "",
    "text": "This problem set is due on September 27, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#background",
    "href": "assignments/02_hw_data_wrangling.html#background",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Background",
    "text": "Background\nPolitical advertising has traditionally been focused on the medium of television, but in recent cycles, online advertising has become much more popular. In this problem set, you will explore a dataset that has information on Facebook ad spending and impressions by candidates in the 2018 election cycle in the United States. The variables in this data are described below.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncand_id\nunique identifier code for candidate\n\n\ncand_name\nfull name of the candidate\n\n\ncand_name_last\nlast name of the candidate\n\n\nparty\nparty affiliation of the candidate (R = Republican, D = Democrat)\n\n\noffice\noffice being sought by candidate\n\n\nstate\nstate in which the candidate is running\n\n\nincumbency\nincumbency status of candidate (incumbent, challenger, or open seat)\n\n\nspend\nestimated total spending on Facebook ads by candidate\n\n\nimpressions\nestimated total impressions of Facebook ads\n\n\nad_tone_attack\nproportion of FB ads that mention candidate’s opponent only\n\n\nad_tone_promote\nproportion of FB ads that mention candidate only\n\n\nad_tone_contrast\nproportion of FB ads that mention candidate and candidate’s opponent"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-1-8-points",
    "href": "assignments/02_hw_data_wrangling.html#question-1-8-points",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 1 (8 points)",
    "text": "Question 1 (8 points)\nLoad the data using the read_csv function and save it as fb_ads (using this will automatically make fb_ads a tibble). In the text, describe how many candidates there are in the dataset.\nUse dplyr functions to create a table with the number of candidates in each type of incumbency status in the data set. Save this table output as incumbency_table (for the autograder). Use the function knitr::kable() on this table to have a nicely formatted table produced in the knitted output.\nRubric: 2pt for loading the data (autograder); 1pt for describing the number of candidates (PDF); 3pts for creating the table (autograder); 2pt for using kable() to nicely format the output (PDF)"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-2-7-points",
    "href": "assignments/02_hw_data_wrangling.html#question-2-7-points",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 2 (7 points)",
    "text": "Question 2 (7 points)\nFilter the data to just US House and US Senate races and use this to create a tibble called party_incumbent_promote that has 6 rows that summarizes the average of ad_tone_promote for each combination of party and incumbency. Call the variable summarizing the promote variable as promote_prop and be sure to remove any missing values when computing the averages.\nUse knitr::kable() to produce a nicely formatted table. In this call, set the digits arguments to 3 and use the col.names argument to pass a nicer set of names. You can use the following as a template:\n\nknitr::kable(my_table, col.names = c(\"Variable 1\", \"Variable 2\", ...))\n\nIn the writeup, describe which type of candidate sponsored the most promoting ads on average.\nRubric: 3pts for creating party_incumbent_promote correctly (autograder); 2pt for a nicely formatted table (PDF); 1pt for changing the column names of the output table (PDF); 1pt for correctly identifying the type of candidate with highest average (PDF)"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-3-7-points",
    "href": "assignments/02_hw_data_wrangling.html#question-3-7-points",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 3 (7 points)",
    "text": "Question 3 (7 points)\nCreate a new variable called impressions_millions that is the total Facebook ad impressions in millions (as opposed to single impressions). Make sure to save the resulting dataset back as fb_ads.\nCreate a histogram of this variable for just the US House races. Save the ggplot output as plot_q3 and also print it to produce a plot in the output. In the text, describe the shape of the histogram and tell the reader if most of the House candidates had more than 10 million ads impressions on Facebook.\nRubric: 2pt for creating the new variable (autograder); 3pts for creating the histogram object (autograder); 2pts for answering the question about the histogram (PDF)"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-4-13-points",
    "href": "assignments/02_hw_data_wrangling.html#question-4-13-points",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 4 (13 points)",
    "text": "Question 4 (13 points)\nLet’s now recreate the following plot that shows the top 15 House candidates in terms of Facebook ad impressions.\n\nYou should save the ggplot output as fb_top_plot. You should also write fb_top_plot on its own line in the chunk to produce the actual plot. The key features of this graph that you should replicate for the autograder are:\n\nThe barplot should have candidate last names on the y-axis and the impressions_millions variable from question 3 on the x-axis.\nThe data feeding into the ggplot call should only have US House candidates and only the candidates with the highest 15 impressions_millions values.\nThe y-axis should be ordered in ascending values of impressions_millions so that the lowest values are at the bottom. You may want to manipulate cand_name_last to achieve this.\nThe fill color of the bar plot should be mapped to the party variable (but not globally!).\n\nYou do not need to exactly match the labels, but you should have informative labels. The color does not need to match, but if you want to change the fill colors, you can use the scale_fill_manual(values = c(R = \"red\", D = \"blue\")) function (where you can change the red and blue to whatever you want).\nRubric: 3pts for correct axes (autograder); 3pts for correct data fed into ggplot (autograder); 3pts for the correct ordering of the y-axis (PDF); 3pts for fill being mapped to party (autograder); 1pt for plot being in knitted output and having informative labels (PDF)"
  },
  {
    "objectID": "assignments/00-problem-set.html",
    "href": "assignments/00-problem-set.html",
    "title": "Getting Started with R, R Studio, Git, and Github",
    "section": "",
    "text": "In this problem set, we’re going to get R, RStudio, and R Markdown set up on your computer. To get started, follow these steps:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows machine, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/00-problem-set.html#installing-r-and-rstudio",
    "href": "assignments/00-problem-set.html#installing-r-and-rstudio",
    "title": "Getting Started with R, R Studio, Git, and Github",
    "section": "",
    "text": "In this problem set, we’re going to get R, RStudio, and R Markdown set up on your computer. To get started, follow these steps:\n\nDownload and install the most recent version of R. There are versions available for the Windows, Mac, and Linux operating systems. On a Windows machine, you will want to install using the R-x.y.z-win.exe file where x.y.z is a version number. On a Mac, you will want to install using the R-x.y.z.pkg file that is notarized and signed.\nWith R installed, download and install RStudio. RStudio is a type of “integrated development environment” or IDE designed for R. It makes working with R considerably easier and is available for most platforms. It is also free.\nInstall the packages we will use throughout the semester. To do this, either type or copy and paste each of the following lines of code into the “Console” in RStudio (lower left panel by default). Make sure you do this separately for each line. If you are asked if you want to install any packages from source, type “no”. Note that the symbols next to my_package are a less than sign &lt; followed by a minus sign - with no space between them. (Don’t be worried if you see some red text here. Those are usually just messages telling you information about the packages you are installing. Unless you see the word Error you should be fine.)\n\n\nmy_packages &lt;- c(\"tidyverse\", \"usethis\", \"devtools\", \"learnr\",\n                 \"tinytex\", \"gitcreds\", \"gapminder\")\ninstall.packages(my_packages, repos = \"http://cran.rstudio.com\")\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nFor some things in the course, we’ll need produce PDFs from R and that requires something called LaTeX. If you’ve never heard of that, it’s completely fine and you should just run the following two lines of R code:\n\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX"
  },
  {
    "objectID": "assignments/00-problem-set.html#installing-and-configuring-git",
    "href": "assignments/00-problem-set.html#installing-and-configuring-git",
    "title": "Getting Started with R, R Studio, Git, and Github",
    "section": "Installing and configuring git",
    "text": "Installing and configuring git\nGit is a version control program that helps organize the process of writing and maintaining code. It allows you to maintain a history of edits to your code without having to resort to a set of files like:\nmy_code.R\nmy_code_v1.R\nmy_code_v2_FINAL.R\nThere’s a lot to git and it will be harder to use in the beginning, but in the long term there are huge benefits of it. First, when you use git, you are much less likely to encounter a devastating data failure. All of your (committed) changes to your project are preserved, even when you make new changes or you revert old changes.\nGit is also a very useful way for people to collaborate. There is a huge community built up around it. And once your projects are publicly available on Github (a website for hosting git repositories), there are a host of ways that folks can collaborate with you.\n\nInstall git\nYou might already have git installed on your computer, especially if you have a Mac. To check, in RStudio, click on the Terminal tab in the bottom-left panel (next to the Console tab). Type git --version at the command prompt. If you get a response with a version number, great you are all set. If you get any kind of error message, you learn how to install git on your machine here.\n\n\nSetup a GitHub account\nNext, you can setup a GitHub account. You can think of GitHub as similar to Dropbox or Google Drive for your git projects (“repositories”) where everything is public by default. Since you might use this account to interact with potential employers in the future, you should probably pick a professional username.\nOnce you have a GitHub account, you can configure your local git program to interact with your account gracefully. Run the following two lines of code in the Terminal replacing \"John Harvard\" with your name and \"john@harvard.edu\" with your email address used to sign up for GitHub.\ngit config --global user.name \"John Harvard\"\ngit config --global user.email \"john@harvard.edu\"\nYou will not see any output when you enter these commands. To check that they worked, enter the following command at the Terminal:\ngit config --list\nYou should see your name and email address listed in the output.\n\n\nSet up RStudio to talk to GitHub\nWe also need to set up RStudio to be able to communicate with GitHub securely. This requires a bit of fidily configuration that we luckily only have to do once. Basically, we need to get a secret code from GitHub and store it in RStudio (kind of like a app-specific password when you’re using two-factor authentication). We can start the process from by entering the following code at the R Console (not the Terminal):\n\nusethis::create_github_token()\n\nThis will open a page on GitHub asking you to create a “Personal Access Token” or PAT (this is the secret code). You’ll have to give the PAT a note that describes what it’s for and choose an expiration date. To minimize problems throughout the semester, you should set it to a date after the semester ends such as 12/31/2023. We recommend keeping the “scope” selections as they are and clicking the “Generate Token” button at the bottom of the page.\n\nYou will then see a new screen with a long sequence of letters. This is your token or secret code. You should treat it like a password and do not share it with anyone. If you use a password manager like 1Password or LastPass, you can put it in a secure note in those programs. Copy this by hitting the button with the two boxes.\n\nOnce you have copied the PAT, call gitcreds::gitcreds_set() from the RStudio console and paste the PAT when prompted. You should see the following:\n&gt; gitcreds::gitcreds_set()\n? Enter password or token: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n-&gt; Adding new credentials...\n-&gt; Removing credentials from cache...\n-&gt; Done.\nOnce this is done, you should be all set for RStudio to communicate with GitHub. If you have any problems with the PAT process or want to know more about it, the Happy Git and GitHub for the useR book has a great chapter about it.\n\n\nCreating your first repo\nOnce you are logged into Github and at its homepage, you can now create a new repo (shorthand for “repository”). These repositories are like folders in Dropbox except a bit more structured. Create one by clicking on the green “New” button in the top-left of the screen next to “Repositories.” It should look like this:\n\nNow a new screen should pop up requesting some information for the new repo. Give this new Repository the name gov50-problem-set0 and set it to be a private repository. You can give it an informative description and check the “Initialize this repository with a README” checkbox. This latter option will add a README to the repository where you can add more information that will display nicely in the repository’s homepage. Your setup should look something like this:\n\nOnce you create your repo, you are ready to connect it to RStudio on your local computer. The easiest way to do this is go to the repo homepage (you’re already there if you’ve just created it) and click on the green “Code” button toward the top right of the page. When you click that, a popup will appear and you can copy the URL (you can click the little clipboard icon to copy this automatically).\n\nNow, switch to RStudio. Go to the menu bar and hit “File &gt; New Project”. You can then choose what type of project to start. Since we’re importing from Github, we’ll use the “Version Control” option (it’s the bottom of the list). In the next menu, choose “Git”. Now, you can paste the URL in the “Repository URL” box. Choose a set of local directories to place this project and hit “Create Project”. And now you’ll have your project ready to go in RStudio."
  },
  {
    "objectID": "assignments/00-problem-set.html#working-with-a-project-in-rstudio",
    "href": "assignments/00-problem-set.html#working-with-a-project-in-rstudio",
    "title": "Getting Started with R, R Studio, Git, and Github",
    "section": "Working with a project in RStudio",
    "text": "Working with a project in RStudio\nYou’ll see in the bottom right window of RStudio you’ll now see the files from your Github project.\n\nLet’s edit the README file by clicking on README.md in that file pane and you’ll be able to edit it in the top-left pane. Change the header from # gov-50-problem-set0 to # Introduction and save the file (⌘+S or Ctrl+S). If you click on the “Git” tab in the top right panel, you will a list of the changes you’ve made to the repo since the last commit (you can think of a commit as a more permanent type of saving work to the git repo).\n\nOne thing you’ll notice here is that git thinks that gov50-problem-set0.Rproj is a file that maybe should go into the repo. But this file is just for our local copy of RStudio and shouldn’t really go into the repo. To prevent git from bothering us about it every time we open something, we can modify the .gitignore file to tell git to ignore certain files. Open .gitignore and add a new line with *.Rproj on it to tell git to ignore any file with the extension .Rproj. Make sure to save the file.\n\nIf you go back to the Git tab in the top right and refresh (little circular arrow in the top right corner), you see that gov50-problem-set0.Rproj is removed from our list. Now we are ready to commit our changes. Click the “Staged” box for .gitignore and README.md and hit the “Commit” button just above the file list. You’ll see a window with the changes that you are about to commit.\n\nYou can click on different files to see what exactly you are changing. Add a short but informative commit message that describes what you are committing and hit “Commit”. Once this completes, you can hit the “Push” button in the top right to push that commit back to Github.\n\nIf you go back to your repo’s homepage on Github and refresh the page, you’ll see the updates to your README file and the new .gitignore.\n\nAnd you’re done! You’ve just created your first repo."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html",
    "href": "assignments/01_hw_data_viz.html",
    "title": "Problem Set 1: Data Visualization",
    "section": "",
    "text": "This problem set is due on September 20, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#background",
    "href": "assignments/01_hw_data_viz.html#background",
    "title": "Problem Set 1: Data Visualization",
    "section": "Background",
    "text": "Background\nIn this problem set, you will get your bearings on how to produce an Rmarkdown report and how to produce data visualizations using ggplot. The data we will use is the Gapminder dataset, which gives some economic and demographic information about countries over time. The variables in this data are described below.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncountry\nname of the country\n\n\ncontinent\nname of the country’s continent\n\n\nyear\nyear of the measurement, ranging from 1952 to 2007 in 5-year increments\n\n\nlifeExp\nlife expectancy at birth, in years\n\n\npop\npopulation\n\n\ngdpPercap\nGDP per capita (US dollars, inflation-adjusted)\n\n\n\n\n\n\n\nNOTE: In the template you’ll get from GitHub Classroom, the default settings for chunks is to only show the output of the chunk, not the code that generated it (echo = FALSE). This is to give your answers a clean look and to highlight the output rather than the source code. The template also has a part that will dump all of the R code at the end so that we can look at it if needed."
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-1-5-points",
    "href": "assignments/01_hw_data_viz.html#question-1-5-points",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 1 (5 points)",
    "text": "Question 1 (5 points)\nMake sure that you load the gapminder and tidyverse packages in the setup chunk (right after the header). For this question, use the glimpse function to show basic information about the gapminder dataset. In the main text (that is, outside of a code chunk), tell us how many rows and columns there are in the data set and which of the variables are factors.\nRubric: 2 write-up points for using the glimpse function; 2 points for reporting the dimension of the data; 1 point for identifying factors."
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-2-10-points",
    "href": "assignments/01_hw_data_viz.html#question-2-10-points",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 2 (10 points)",
    "text": "Question 2 (10 points)\nLet’s investigate how life expectancy varies across the continents. Using ggplot, we want you to recreate the following figure:\n\nThese are boxplots of the distribution of life expectancy in each continent. Please make sure that you include the labels as shown in this figure. For the autograder to be able to process your plot, you should assign your the output of your ggplot call to plot_q2 and then evaluate plot_q2 to ensure that the plot is produced in the output. That is, your code should look like this:\n\nplot_q2 &lt;- ggplot(&lt;arguments&gt;) +\n  geom_&lt;type&gt;(&lt;arguments&gt;) +\n  ...\n\nplot_q2\n\nNote: in the raw Rmd file for this problem set, you’ll notice the above code chunk has the argument eval = FALSE. This tells Rmarkdown not to try to run the code, since it’s not complete. If you copy and paste this chunk below, be sure to remove that argument to ensure the code runs properly.\nRubric: 10 autograder points."
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-3-5-points",
    "href": "assignments/01_hw_data_viz.html#question-3-5-points",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 3 (5 points)",
    "text": "Question 3 (5 points)\nLooking at the previous plot, which continent has the highest median life expectancy? Which part of the boxplot can we determine this from?\nRubric: 5 write up points (2 for identifying the correct continent, 3 for correctly identifying how to find this on the boxplot)"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-4-15-points",
    "href": "assignments/01_hw_data_viz.html#question-4-15-points",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 4 (15 points)",
    "text": "Question 4 (15 points)\nThe previous boxplot groups all the years together into one boxplot, but what if we want to understand how life expectancy is changing over time? Next, we will recreate the following plot:\n\nThe plot shows each country’s life expectancy trajectory over time, broken out by continent with smoothed average lines overlayed for each continent. To get started, we’ll give you a few clues about what we’ve done here:\n\nThe lines for each country use the color \"gray70\".\nThe linewidth of the smoothed line is 1.1 and the method used is the loess smoother. We also have turned off the standard errors.\nMake sure that the facets are all on one row. Look at the facet_wrap documentation if need help with this.\nMake sure that the labels are correctly specified.\nUse the chunk options fig.width = 11 and fig.height = 4 to shrink the font size so the year labels will not overlap.\n\nFinally, assign the output of your ggplot call to plot_q4 and then evaluate plot_q4 (similarly to what you did in Question 2).\nRubric: 15 autograder points."
  },
  {
    "objectID": "assignments/03_hw_causality.html",
    "href": "assignments/03_hw_causality.html",
    "title": "Problem Set 3: Causality",
    "section": "",
    "text": "This problem set is due on October 4, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/03_hw_causality.html#background",
    "href": "assignments/03_hw_causality.html#background",
    "title": "Problem Set 3: Causality",
    "section": "Background",
    "text": "Background\nA professor in the Government department here at Harvard, Ryan Enos, conducted a randomized field experiment assessing the extent to which exposure to demographic change affected the political views of individuals living in suburban communities around Boston, Massachusetts.\n\nThis exercise is based on: Enos, R. D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111(10): 3699–3704.\n\nSubjects in the experiment were individuals riding on the commuter rail line and overwhelmingly white. Every morning, multiple trains pass through various stations in suburban communities that were used for this study. For pairs of trains leaving the same station at roughly the same time, one was randomly assigned to receive the treatment and one was designated as a control. By doing so all the benefits of randomization apply for this dataset.\nThe treatment in this experiment was the presence of two native Spanish-speaking ‘confederates’ (a term used in experiments to indicate that these individuals worked for the researcher, unbeknownst to the subjects) on the platform each morning prior to the train’s arrival. The presence of these confederates, who would appear as Hispanic foreigners to the subjects, was intended to simulate the kind of demographic change anticipated for the United States in coming years. For those individuals in the control group, no such confederates were present on the platform. The treatment was administered for 10 days. Participants were asked questions related to immigration policy both before the experiment started and after the experiment had ended. The names and descriptions of variables in the data set boston.csv are:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nage\nAge of individual at time of experiment\n\n\nmale\nSex of individual, male (1) or female (0)\n\n\nincome\nIncome group in dollars (not exact income)\n\n\nwhite\nIndicator variable for whether individual identifies as white (1) or not (0)\n\n\ncollege\nIndicator variable for whether individual attended college (1) or not (0)\n\n\nusborn\nIndicator variable for whether individual was born in the US (1) or not (0)\n\n\ntreatment\nIndicator variable for whether an individual was treated (1) or not (0)\n\n\nideology\nSelf-placement on ideology spectrum from Very Liberal (1) through Moderate (3) to Very Conservative (5)\n\n\nnumberim.pre\nPolicy opinion on question about increasing the number immigrants allowed in the country from Increased (1) to Decreased (5)\n\n\nnumberim.post\nSame question as above, asked later\n\n\nremain.pre\nPolicy opinion on question about allowing the children of undocumented immigrants to remain in the country from Allow (1) to Not Allow (5)\n\n\nremain.post\nSame question as above, asked later\n\n\nenglish.pre\nPolicy opinion on question about passing a law establishing English as the official language from Not Favor (1) to Favor (5)\n\n\nenglish.post\nSame question as above, asked later"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-1-6-points",
    "href": "assignments/03_hw_causality.html#question-1-6-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 1 (6 points)",
    "text": "Question 1 (6 points)\nThe benefit of randomly assigning individuals to the treatment or control groups is that the two groups should be similar, on average, in terms of their covariates. This is referred to as ‘covariate balance.’\nLoad the tidyverse package in the setup chunk. Read the data \"data/boston.csv\" into R using read_csv and call the resulting tibble trains. Using the group_by and summarize functions, create a tibble called balance_table that shows the sample average of the age and college variables by whether participants were in the treatment or control groups.\nThen use the knitr::kable() function to create a nice-looking table, including some informative column names. Briefly comment on the whether you think these variables appear balanced.\nRubric: 1pt for successful knitting (autograder); 3pts for correct balance_table object (autograder); 1pt for nicely formatted table (PDF); 1pt for brief comments (PDF)"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-2-7-points",
    "href": "assignments/03_hw_causality.html#question-2-7-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 2 (7 points)",
    "text": "Question 2 (7 points)\nIndividuals in the experiment were asked a series of questions both at the beginning and the end of the experiment. One such question was “Do you think the number of immigrants from Mexico who are permitted to come to the United States to live should be increased, left the same, or decreased?”\nThe response to this question prior to the experiment is in the variable numberim.pre. The response to this question after the experiment is in the variable numberim.post. In both cases the variable is coded on a 1 – 5 scale. Responses with values of 1 are inclusionary (‘pro-immigration’) and responses with values of 5 are exclusionary (‘anti-immigration’). Take the following steps:\n\nCreate a new variable in the trains data called change and define it as the change in immigration attitudes between pre- and post-experiment (post minus pre). Be sure to assign the output of your data wrangling call back to trains.\nCalculate the average change in attitudes about immigration in the treated group and save this output as trt_change. This should be a 1 x 1 tibble.\nCalculate the average change in attitudes about immigration in the control group and save this output as ctr_change. This should be a 1 x 1 tibble.\nCompute the average treatment effect from these two objects and store it as ate. This should be a 1 x 1 tibble.\n\nReport these estimates in the main text (that is, outside the chunk) and describe what they mean substantively with respect to the study.\nRubric: 1pt for creating change variable (autograder); 2pts for trt_change object (autograder); 2pts for ctr_change object (autograder); 1pt for ate object (autograder); 1pt for write up about the results (PDF)."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-3-3-points",
    "href": "assignments/03_hw_causality.html#question-3-3-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 3 (3 points)",
    "text": "Question 3 (3 points)\nThinking about the causal effect of the confederate treatment, describe, in words, what the potential outcomes for a particular person are in the analysis for the last problem (remember that potential outcomes are different than the possible values of the outcome!). Substantively, what does the fundamental problem of causal inference refer to in this context? Make sure to refer to what treatment and control means in this experiment rather than just mention the “treatment” and “control” groups.\nRubric: 3pts for answer (PDF); no autograder points"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-4-2-points",
    "href": "assignments/03_hw_causality.html#question-4-2-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 4 (2 points)",
    "text": "Question 4 (2 points)\nIn your data science group, two members have alternative ideas for what the outcome should have been instead of the change in attitudes on immigration between the beginning and end of the experiment. Jimmy Q. Boxplot thinks that you should have used numberim.pre as the outcome and Suzy T. Histogram thinks that you should have used numberim.post. Are either of these two valid and interesting outcomes to explore in this study? Briefly explain why or why not.\nRubric: 2pts for answer (PDF); no autograder points."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-5-7-points",
    "href": "assignments/03_hw_causality.html#question-5-7-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 5 (7 points)",
    "text": "Question 5 (7 points)\nDoes having attended college influence the effect of being exposed to ‘outsiders’ on exclusionary attitudes? Another way to ask the same question is this: is there evidence of a differential impact of treatment, conditional on attending college versus not attending college?\nUse group_by, summarize, pivot_wider, and mutate to create a tibble called ate_college where each row corresponds to a unique value of the college variable (so two rows total) and that has the following four columns:\n\ncollege: the unique value of the college variable for this row, labelled as either “College Grad” or “Non-College Grad”.\nTreated: the mean of the change variable for each unique value of college in the treated group.\nControl: the mean of the change variable for each unique value of college in the control group.\nATE: the difference between the treatment and control means for this value of college.\n\nIn the first step of building ate_college, you should pipe the data to a mutate() call that (a) modifies the treatment to be \"Treated\" when treatment == 1 and \"Control\" when treatment == 0 and (b) modifies the college variable to be \"College Grad\" when college == 1 and \"Non-College Grad\" when college == 0. When you do this, be sure not to overwrite the trains data with these changes, simply pipe this to the next step.\nPass the table to knitr::kable() to produce a nicely formatted table. Report the ATEs in text and comment on whether or not you see evidence of a differential effect of treatment.\nRubric: 5pts for ate_college object (autograder); 1pt for nicely formatted table (PDF); 1pt for brief write-up (PDF)"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-6-10-points",
    "href": "assignments/03_hw_causality.html#question-6-10-points",
    "title": "Problem Set 3: Causality",
    "section": "Question 6 (10 points)",
    "text": "Question 6 (10 points)\nRepeat the same analysis as in the previous question but this time with respect to income. For age, use case_when and logical vectors to create a new variable income_group that has the following values:\n\n\"1. Under $100k\" when income is below $100,000,\n\"2. $100k - $150k\" when income is between $100,000 and $150,000, and\n\"3. Over $150k\" when income is over $150,000.\n\nRemember to save the output to the trains name. Use this variable along with the approach in the previous question to create a tibble called ate_income (replacing college with income_group) that has four rows corresponding to the three unique values of income_group and four columns: income_group, Treated, Control, and ATE (all defined similarly to the previous question).\nUse the ate_income tibble to produce a barplot (using ggplot) of the ATE for each income group and assigning the plot to the name income_plot. Be sure to include informative labels for the x and y axes, along with a title.\nIn the text, answer these question: Do there appear to be systematic relationships between the treatment effects and income? If so, what patterns do you see?\nRubric: 2pts for income_group variable (autograder); 5pts for ate_income object (autograder); 2pts for income_plot ggplot object (autograder); 1pt descriptions of the results (PDF)"
  },
  {
    "objectID": "assignments/05_hw_regression.html",
    "href": "assignments/05_hw_regression.html",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "",
    "text": "This problem set is due on November 1, 2023 at 11:59pm.\nYou can find instructions for obtaining and submitting problem sets here.\nYou can find the GitHub Classroom link to download the template repository on the Ed Board"
  },
  {
    "objectID": "assignments/05_hw_regression.html#background",
    "href": "assignments/05_hw_regression.html#background",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Background",
    "text": "Background\nFor this problem set, we will analyze data from the following article:\nGuess, Andrew M., Nyhan, Brendan & Reifler, Jason Exposure to untrustworthy websites in the 2016 US election. Nature Human Behavior, Vol 4, pp. 472–480 (2020).\nGuess, Nyhan, and Reifler investigate the consumption of political misinformation online using a combination of survey data matched to inidividual-level web traffic data. This data allowed the researchers to assess what characteristics are predictive of a citizen consuming untrustworthy news in the lead up to the 2016 U.S. presidential election. These untrustworthy sources included, for example, occupydemocracts.com on the left and angrypatriotmovement.com on the right. The authors found that 93% of the fact-checked articles on these sites were classified as false.\nWe will use the data from this paper, contained in the data/fake_news.csv file, to explore the use of multiple regression. A description of the variables is listed below:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nage\nAge of respondent\n\n\nfn_perc\nPercent of online news consumption classified as untrustworthy (fake news)\n\n\nslant_decile\nLiberal-conservative slant of news consumption (1 = very liberal, 10 = very conservative)\n\n\nfemale\nGender identity of the respondent (1 = female, 0 = male)\n\n\nblack\nRespondent identifies as Black (1) or not (0)\n\n\nnonwhite\nRespondent identifies as non-white (1) or white (0)\n\n\ntrump_support\nRespondent support Donald Trump (1) or not (0)\n\n\nclinton_support\nRespondent support Hillary Clinton (1) or not (0)\n\n\ncollege\nRespondent has a college degree\n\n\npolinterest\n4-point scale of interest in politics (1 = “hardly any interest”, 4 = “follows politics most of the time”)\n\n\nknowledge\nPolitical knowledge scale (0=least knowledgeable, 8=most knowledgeable)"
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-1-4-points",
    "href": "assignments/05_hw_regression.html#question-1-4-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 1 (4 points)",
    "text": "Question 1 (4 points)\nRead the data from data/fake_news.csv, subset it to respondents that support either Trump or Clinton, and save it as fake_news.\nIn the write up, indicate the number of respondents in the sample and the average percent of online news consumption that is fake news.\nRubric: 1pt for Rmd file compiling (autograder); 2pt for loading data (autograder); 1pt for reporting number of respondents and average (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-2-6-points",
    "href": "assignments/05_hw_regression.html#question-2-6-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 2 (6 points)",
    "text": "Question 2 (6 points)\nCreate a boxplot that compares fake news consumption by which candidate the respondent supports and save the plot as trump_box. Your plot should look like this:\n\nTo create the x-axis variable that is nicely labeled, use mutate() to create a variable that is \"Trump supporter\" when trump_support is 1 and \"Clinton supporter\" otherwise (it does not matter what this variable is called to the autograder, but don’t overwrite your trump_support variable!). Be sure to use informative labels, though they do not have to match the text exactly.\nIn the write-up, report what the plot tells us about which candidate’s supporters consume more untrustworthy news sources.\nRubric: 3pts for boxplot (autograder); 1pt for informative labels on plot (PDF); 2pts for written description of relationship (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-3-6-points",
    "href": "assignments/05_hw_regression.html#question-3-6-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 3 (6 points)",
    "text": "Question 3 (6 points)\nRun a linear regression with fake news consumption as your outcome variable and Trump support as your predictor. Save this regression as fit_1 and report the coefficients using a nicely formatted table with the following code (you may need to install the broom package to have this work):\n\nfit_1 |&gt;\n  broom::tidy() |&gt;\n  select(term, estimate) |&gt;  \n  knitr::kable(digits = 2)\n\nInterpret both of these coefficients. Do not merely comment on the direction of the association (i.e., whether the slope is positive or negative). Explain what the value of the coefficients mean in terms of the units in which each variable is measured.\nRubric: 3pts for correct lm output (autograder); 1pt for coefficient table (PDF); 2pt for interpretation of coefficients (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-4-5-points",
    "href": "assignments/05_hw_regression.html#question-4-5-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 4 (5 points)",
    "text": "Question 4 (5 points)\nYou decide to investigate the results of the previous question a bit more carefully because you know that Trump supporters are probably different than Trump non-supporters in other ways. Create a scatter plot where the x-axis is the age of the respondent and the y-axis is fake news consumption. Color your points according to whether they support Trump or Clinton. That is, make the points Trump supporters one color, and make the points for Clinton supporters a different color. You plot should look like this:\n\nYou should save this plot as candidate_scatter and to create better labels for the color of the points, use mutate() in a similar way to question 2.\nAnswer these questions in the write-up: What is the relationship between age and consumption of fake news? Can you detect a relationship between age and support for Trump?\nRubric: 3pts for scatter plot (autograder); 2pts for written conclusions about the plots (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-5-6-points",
    "href": "assignments/05_hw_regression.html#question-5-6-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 5 (6 points)",
    "text": "Question 5 (6 points)\nRun a linear regression with fake news consumption as your outcome variable and with Trump support and age as your predictors. Save the output of this regression as fit_2 and report the coefficients using a nicely formatted table with the following code (you may need to install the broom package to have this work):\n\nfit_2 |&gt;\n  broom::tidy() |&gt;\n  select(term, estimate) |&gt;  \n  knitr::kable(digits = 2)\n\nIn the main text, interpret the coefficients on the two predictors, ignoring the intercept for now (you will interpret the intercept in the next question). Explain what each coefficient represents in terms of the units of the relevant variables.\nRubric: 3pts for correct lm output (autograder); 1pt for the coefficient table (PDF); 2pts for correct interpretation of the coefficients (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-6-2-points",
    "href": "assignments/05_hw_regression.html#question-6-2-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 6 (2 points)",
    "text": "Question 6 (2 points)\nNow interpret the intercept from the regression model with two predictors. Is this intercept a substantively important or interesting quantity? Why or why not?\nRubric: 1pt for correct interpretation (PDF); 1pt for argument about substantive importance (PDF)."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-7-6-points",
    "href": "assignments/05_hw_regression.html#question-7-6-points",
    "title": "Problem Set 5: Fake News Consumption in the 2016 US Election",
    "section": "Question 7 (6 points)",
    "text": "Question 7 (6 points)\nNow we’ll see how consumption of misinformation varies by the slant of the media diet. Create a new variable called slant_group that takes on the following values:\n\n\"Liberal\" when the slant_decile is less than or equal to 4\n\"Moderate\" when slant_decile is 5 or 6\n\"Conservative\" when slant_decile is greater than or equal to 7\n\nRun a regression of the untrustworthy news consumption percentage on this newly created variable. Save the output of this regression as fit_3 and report the coefficients using a nicely formatted table with the following code (you may need to install the broom package to have this work):\n\nfit_3 |&gt;\n  broom::tidy() |&gt;\n  select(term, estimate) |&gt;  \n  knitr::kable(digits = 2)\n\nIn the main text, substantively interpret each coefficient in the regression, including the intercept. From this regression, indicate which media diet slant group has the highest consumption of untrustworthy news sources.\nRubric: 3pts for correct lm output (autograder); 1pt for the coefficient table (PDF); 2pts for correct interpretation of the coefficients and identifying the highest group (PDF)."
  },
  {
    "objectID": "assignments/distill.html",
    "href": "assignments/distill.html",
    "title": "Creating the Final Project Repository",
    "section": "",
    "text": "First you need to install the {distill} package with the following code at the Console:\n\ninstall.packages(\"distill\")"
  },
  {
    "objectID": "assignments/distill.html#install-the-distill-package",
    "href": "assignments/distill.html#install-the-distill-package",
    "title": "Creating the Final Project Repository",
    "section": "",
    "text": "First you need to install the {distill} package with the following code at the Console:\n\ninstall.packages(\"distill\")"
  },
  {
    "objectID": "assignments/distill.html#create-your-github-repository",
    "href": "assignments/distill.html#create-your-github-repository",
    "title": "Creating the Final Project Repository",
    "section": "Create your GitHub repository",
    "text": "Create your GitHub repository\nThe next thing you will do is create the GitHub repository that will house your final project. You can do this easily by using the Gov 50 Final Project template. After going to that repository, click on the green “Use this template” button:\n\nWhen presented with the option of the repository name, for now you can call it gov50-final-project or something similar. You can change the name of repository later once you have a more solid idea for your final project idea. Make sure that Public option is selected."
  },
  {
    "objectID": "assignments/distill.html#setting-up-github-pages",
    "href": "assignments/distill.html#setting-up-github-pages",
    "title": "Creating the Final Project Repository",
    "section": "Setting up GitHub pages",
    "text": "Setting up GitHub pages\nOnce you have your repository created, it’s time to have GitHub publish the pages. In the repository click on the settings tab and then select the “Pages” entry in the side bar. There you should be able to select a branch to publish the site from. Select the “main” branch and hit “Save”. The setup should look like this:\n\nOnce you do this, GitHub will start to publish your site. After a few minutes, you’ll see a new box on the Pages settings pane that gives you the URL:\n\nClick on “Visit Site” to see your site in progress. It should be at the URL https://{username}.github.io/{repo-name}."
  },
  {
    "objectID": "assignments/distill.html#working-with-your-repo",
    "href": "assignments/distill.html#working-with-your-repo",
    "title": "Creating the Final Project Repository",
    "section": "Working with your repo",
    "text": "Working with your repo\nYou should now create a project in RStudio based on your final project repository. Open the index.Rmd file and update it with your name and some thoughts about your final project ideas. Hit the “Knit” button to preview your site. If you are happy with your edits, stage the files and commit them in the usual way:\n\nOnce committed, push your changes to GitHub. When you go back to your repo, you’ll see a yellow dot, indicating that GitHub is working on publishing your changes:\n\nOnce it is done publishing, you’ll see a green checkmark:\n\nYou can now go to your site and see the changes."
  },
  {
    "objectID": "assignments/distill.html#milestone-1",
    "href": "assignments/distill.html#milestone-1",
    "title": "Creating the Final Project Repository",
    "section": "Milestone 1",
    "text": "Milestone 1\nYou should change the metadata on your article (your name at the least) and write a few sentences in the main text about what you might be interested in writing about. You will submit a link to the public article (not the github repository) to Canvas (not Gradescope)."
  },
  {
    "objectID": "assignments/distill.html#more-information-on-distill-authoring",
    "href": "assignments/distill.html#more-information-on-distill-authoring",
    "title": "Creating the Final Project Repository",
    "section": "More information on distill authoring",
    "text": "More information on distill authoring\nThe basics of distill are very similar to the basics of the Rmd files that you have been working with. You can also check out the distill website for more information on the basics of authoring distill articles."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Having problems: check our Troubleshooting Guide"
  },
  {
    "objectID": "assignments/index.html#tutorials",
    "href": "assignments/index.html#tutorials",
    "title": "Assignments",
    "section": "Tutorials",
    "text": "Tutorials\n\nTutorial Instructions\nTutorial 1: Data Visualization\nTutorial 2: Data Wrangling\nTutorial 3: Causality\nTutorial 4: Summarizing Data\nTutorial 5: Correlation and Tidying Data\nTutorial 6: Loops\nTutorial 7: Regression and Sampling\nTutorial 8: Bootstrap"
  },
  {
    "objectID": "assignments/index.html#problem-sets",
    "href": "assignments/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\n\nSubmission guide for problem sets\nProblem set 0: Setting up R, RStudio, and GitHub\nProblem set 1: Data visualization\nProblem set 2: Data wrangling\nProblem set 3: Causality\nProblem set 4: Summarizing data\nProblem set 5: Regression\nProblem set 6: Sampling"
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\n\nFinal project information\nFinal project milestone 1: Creating a repo"
  },
  {
    "objectID": "assignments/troubleshooting.html",
    "href": "assignments/troubleshooting.html",
    "title": "Troubleshooting Assignments",
    "section": "",
    "text": "If you are missing your Git tab in RStudio, the most likely culprit is that you simply don’t have the RStudio project for the repository open in RStudio. In the upper right-hand corner of RStudio, you can see the current project that is open. If you do not have a project open, you might see this:\n\nTo get to your project, simply click on that Project: (None) button to reveal a list of recent projects, from which you will usually see the one you are working on:"
  },
  {
    "objectID": "assignments/troubleshooting.html#missing-git-tab-in-rstudio",
    "href": "assignments/troubleshooting.html#missing-git-tab-in-rstudio",
    "title": "Troubleshooting Assignments",
    "section": "",
    "text": "If you are missing your Git tab in RStudio, the most likely culprit is that you simply don’t have the RStudio project for the repository open in RStudio. In the upper right-hand corner of RStudio, you can see the current project that is open. If you do not have a project open, you might see this:\n\nTo get to your project, simply click on that Project: (None) button to reveal a list of recent projects, from which you will usually see the one you are working on:"
  },
  {
    "objectID": "assignments/troubleshooting.html#errors-pushing-to-github",
    "href": "assignments/troubleshooting.html#errors-pushing-to-github",
    "title": "Troubleshooting Assignments",
    "section": "Errors pushing to GitHub",
    "text": "Errors pushing to GitHub\nIf you trying to push to GitHub and you get an error saying something like:\n\n/usr/bin/git push origin HEAD:refs/heads/main To https://github.com/gov50-f23/gov-50-hw-2-mattblackwell.git ! [rejected] HEAD -&gt; main (non-fast-forward) error: failed to push some refs to ‘https://github.com/gov50-f23/gov-50-hw-2-mattblackwell.git’ hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: ‘git pull …’) before pushing again. hint: See the ‘Note about fast-forwards’ in ‘git push –help’ for details.\n\nThis can happen if you edit the repository manually on the GitHub website rather than in your local version. Sometimes you can fix this problem by simply hitting the “Pull” button in RStudio:\n\nIf this solves your problem, great! If you get an error message when trying to pull, then you’ll need to resolve the conflicts manually. First start a new session of RStudio by going to the Session menu and hitting “New Session”:\n\nThis will open a new RStudio window. In this new window, we are going to create a new project from the same repository. When creating this new project, be sure to add _new to the end of the project directory name:\n\nNow you should have two RStudio sessions with two different projects: the original local one and a new one directly from GitHub:\n\nAssuming you want to overwrite whatever is on the GitHub website with what is on you local computer, copy the contents of your Rmd file from the old RStudio project to the same Rmd file in the new RStudio project. In the new RStudio project, knit the Rmd file, commit any changes, and then push to GitHub. Thew new RStudio project should be all synced now.\nOnce you are confident that the new project has all of the changes that you want, simply delete the old RStudio project from your computer."
  },
  {
    "objectID": "assignments/troubleshooting.html#files-larger-than-100-mb",
    "href": "assignments/troubleshooting.html#files-larger-than-100-mb",
    "title": "Troubleshooting Assignments",
    "section": "Files larger than 100 MB",
    "text": "Files larger than 100 MB\nIf you get a push error complaining about files greater than 100 MB, you will need to follow similar steps to the pull error steps in the last section. Once you have copied over the contents of your Rmd files, you can then add your data files. For files over 100 MB, you will add them to the the .gitignore file of your new repository. To do this, add the file to your new repository and it will show up in the Git tab. Right-click on the new file in the Git tab and hit the Ignore button:\n\nA dialog box will open and you can hit “Save” which will add or amend a .gitignore file in your repository. You should then stage and commit that .gitignore file and push.\n\nA good practice would be to now write an Rmd file or R script that will load the ignored big data file, subset it to certain rows and columns and then save the file as a csv file using write_csv(). Once you get that csv file to under 100 MB, you can commit that file and use it as the main data file in your main Rmd file."
  },
  {
    "objectID": "assignments/troubleshooting.html#final-project-website-is-not-updating",
    "href": "assignments/troubleshooting.html#final-project-website-is-not-updating",
    "title": "Troubleshooting Assignments",
    "section": "Final project website is not updating",
    "text": "Final project website is not updating\nIf you have committed and pushed a change to your final project repository and confirmed that the website has deployed (by looking at the green check), but there are no changes to the actual project website, there could be a couple of problems. First, check to make sure you have knitted the Rmd file to HTML and then committed and pushed any changes to the HTML file to GitHub. Once way to ensure this is to check the “Knit on Save” checkbox so that RStudio automatically knits when you save.\nSecond, you might check the website in a second browser or incognito window. Sometimes browsers cache versions of the website and don’t update them immediately. Finally, sometimes the deploy process just doesn’t work or times out for unknown reasons (“internet weather”). In that case, try to add another change, knit, commit, and push to get the process to restart."
  },
  {
    "objectID": "assignments/troubleshooting.html#missing-images-and-figures-in-final-project-website",
    "href": "assignments/troubleshooting.html#missing-images-and-figures-in-final-project-website",
    "title": "Troubleshooting Assignments",
    "section": "Missing images and figures in final project website",
    "text": "Missing images and figures in final project website\nIf you are missing the figures on your final project website, but you saw them when you knitted on your computer, you probably need to stage, commit, and push the output of your knitting to GitHub. In particular, you should see a bunch of files in your Git tab:\n\nYou need to stage all of those, commit the changes, and push. Make sure that you do not stage any large data files by accident."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Data Science for the Social Sciences\n        ",
    "section": "",
    "text": "Data Science for the Social Sciences\n        \n        \n            Learning to use data to explore the social, political, and economic world\n        \n        \n            Gov 50 • Fall 2023Harvard University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\nInstructor\n\n   Prof. Matt Blackwell\n   CGIS Knafel 305\n   mblackwell@gov.harvard.edu\n   matt_blackwell\n   Schedule an appointment\n\n\n\nCourse details\n\n   Tue/Thu\n   September 3rd-December 20th, 2023\n   12:00–1:15 PM\n   Emerson 105\n   Slack\n   Study Hall Schedule\n   Office Hours Schedule\n   Gov 50 Google Calendar\n\n\n\nContacting me\nGeneral questions about the course should be posted to either the course Ed Discussion board or the course Slack. Someone on the teaching staff will attempt to respond to these messages within 25 hours, but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry! For other issues (absences, etc), feel free to email me or send me a direct message on Slack."
  },
  {
    "objectID": "materials/02_r_data_viz.html",
    "href": "materials/02_r_data_viz.html",
    "title": "Data Visualization and Data Wrangling",
    "section": "",
    "text": "Tuesday: Chapter 2 of Modern Dive\nThursday: Chapter 3 of Modern Dive\nFinish Problem Set 0"
  },
  {
    "objectID": "materials/02_r_data_viz.html#readings",
    "href": "materials/02_r_data_viz.html#readings",
    "title": "Data Visualization and Data Wrangling",
    "section": "",
    "text": "Tuesday: Chapter 2 of Modern Dive\nThursday: Chapter 3 of Modern Dive\nFinish Problem Set 0"
  },
  {
    "objectID": "materials/02_r_data_viz.html#data",
    "href": "materials/02_r_data_viz.html#data",
    "title": "Data Visualization and Data Wrangling",
    "section": "Data",
    "text": "Data\n\nInstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")"
  },
  {
    "objectID": "materials/02_r_data_viz.html#slides-and-code",
    "href": "materials/02_r_data_viz.html#slides-and-code",
    "title": "Data Visualization and Data Wrangling",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (9/12) lecture:\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture3.Rmd\n\nThursday (9/14) lecture: Data Wrangling\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture4.Rmd"
  },
  {
    "objectID": "materials/04_causality.html",
    "href": "materials/04_causality.html",
    "title": "Causality and Pivoting",
    "section": "",
    "text": "Chapter 2 of QSS"
  },
  {
    "objectID": "materials/04_causality.html#readings",
    "href": "materials/04_causality.html#readings",
    "title": "Causality and Pivoting",
    "section": "",
    "text": "Chapter 2 of QSS"
  },
  {
    "objectID": "materials/04_causality.html#slides-and-code",
    "href": "materials/04_causality.html#slides-and-code",
    "title": "Causality and Pivoting",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (9/26) lecture: Randomized Experiments\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture7.Rmd\n\nTuesday (9/28) lecture: Observational Studies\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture8.Rmd"
  },
  {
    "objectID": "materials/06_bivariate_tidying.html",
    "href": "materials/06_bivariate_tidying.html",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "",
    "text": "For summarizing relationships, see QSS 3.6 and MD 5.1.1.\nFor pivoting and tidy data, see MD Ch 4.\nFor more on joins, see the chapter 20 in R for Data Science."
  },
  {
    "objectID": "materials/06_bivariate_tidying.html#readings",
    "href": "materials/06_bivariate_tidying.html#readings",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "",
    "text": "For summarizing relationships, see QSS 3.6 and MD 5.1.1.\nFor pivoting and tidy data, see MD Ch 4.\nFor more on joins, see the chapter 20 in R for Data Science."
  },
  {
    "objectID": "materials/06_bivariate_tidying.html#data",
    "href": "materials/06_bivariate_tidying.html#data",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "Data",
    "text": "Data\n\nOn Thursday, We will also use data from the nycflights13 package that you can install with:\n\n\ninstall.packages(\"nycflights13\")"
  },
  {
    "objectID": "materials/06_bivariate_tidying.html#slides-and-code",
    "href": "materials/06_bivariate_tidying.html#slides-and-code",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (10/10) lecture: Summarizing relationships and writing our own functions\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture_11.Rmd\n\nThursday (10/12) lecture: Pivoting longer, joining data.\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\nCode: lecture12.Rmd"
  },
  {
    "objectID": "materials/08_regression.html",
    "href": "materials/08_regression.html",
    "title": "Prediction: More Regression",
    "section": "",
    "text": "For model fit, see QSS 4.2.6 or IMS 7.2.5\nFor multiple regression, IMS 8.1-8.3, MD 6.1-6.2, or QSS 4.3.1-4.3.2"
  },
  {
    "objectID": "materials/08_regression.html#readings",
    "href": "materials/08_regression.html#readings",
    "title": "Prediction: More Regression",
    "section": "",
    "text": "For model fit, see QSS 4.2.6 or IMS 7.2.5\nFor multiple regression, IMS 8.1-8.3, MD 6.1-6.2, or QSS 4.3.1-4.3.2"
  },
  {
    "objectID": "materials/08_regression.html#packages",
    "href": "materials/08_regression.html#packages",
    "title": "Prediction: More Regression",
    "section": "Packages",
    "text": "Packages\n\nIn Thursday’s lecture, we’ll use the modelr package which you can install with the following command:\n\n\ninstall.pacakges(\"modelr\")"
  },
  {
    "objectID": "materials/08_regression.html#slides-and-code",
    "href": "materials/08_regression.html#slides-and-code",
    "title": "Prediction: More Regression",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (10/24) lecture: Model fit and multiple regression\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (10/26) lecture: Multiple regression and categorical independent variables\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/10_bootstrap.html",
    "href": "materials/10_bootstrap.html",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "MD Ch 8, IMS Ch 12 for another take."
  },
  {
    "objectID": "materials/10_bootstrap.html#readings",
    "href": "materials/10_bootstrap.html#readings",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "MD Ch 8, IMS Ch 12 for another take."
  },
  {
    "objectID": "materials/10_bootstrap.html#data",
    "href": "materials/10_bootstrap.html#data",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Data",
    "text": "Data\n\nReinstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nCSV files for the data from this week if you cannot install the package:\n\nanes.csv\ntrains.csv"
  },
  {
    "objectID": "materials/10_bootstrap.html#slides-and-code",
    "href": "materials/10_bootstrap.html#slides-and-code",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/1) lecture: The Bootstrap\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (11/3) lecture: More Confidence Intervals\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/12_tests_models.html",
    "href": "materials/12_tests_models.html",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "",
    "text": "IMS Ch 13 for Thursday."
  },
  {
    "objectID": "materials/12_tests_models.html#readings",
    "href": "materials/12_tests_models.html#readings",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "",
    "text": "IMS Ch 13 for Thursday."
  },
  {
    "objectID": "materials/12_tests_models.html#data",
    "href": "materials/12_tests_models.html#data",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "Data",
    "text": "Data\n\nNo new data for Tuesday."
  },
  {
    "objectID": "materials/12_tests_models.html#slides-and-code",
    "href": "materials/12_tests_models.html#slides-and-code",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/15) lecture: Difference in means tests and Power\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (11/17) lecture: Mathematical models for inference\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/14_ols_inference.html",
    "href": "materials/14_ols_inference.html",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "QSS 7.3 and/or IMS Ch 24."
  },
  {
    "objectID": "materials/14_ols_inference.html#readings",
    "href": "materials/14_ols_inference.html#readings",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "QSS 7.3 and/or IMS Ch 24."
  },
  {
    "objectID": "materials/14_ols_inference.html#data",
    "href": "materials/14_ols_inference.html#data",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Data",
    "text": "Data\n\nReinstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nCSV files for the data from this week if you cannot install the package:\n\najr.csv"
  },
  {
    "objectID": "materials/14_ols_inference.html#slides-and-code",
    "href": "materials/14_ols_inference.html#slides-and-code",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/29) lecture: Inference with Linear Regression\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "resources/cheatsheet.html",
    "href": "resources/cheatsheet.html",
    "title": "Gov 50 Cheat Sheet",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gov50data)\nlibrary(gapminder)"
  },
  {
    "objectID": "resources/cheatsheet.html#r-basics-week-1",
    "href": "resources/cheatsheet.html#r-basics-week-1",
    "title": "Gov 50 Cheat Sheet",
    "section": "R Basics (Week 1)",
    "text": "R Basics (Week 1)\n\nCreating a vector\nYou can create a vector using the c function:\n\n## Any R code that begins with the # character is a comment\n## Comments are ignored by R\n\nmy_numbers &lt;- c(4, 8, 15, 16, 23, 42) # Anything after # is also a\n# comment\nmy_numbers\n\n[1]  4  8 15 16 23 42\n\n\n\n\nInstalling and loading a package\nYou can install a package with the install.packages function, passing the name of the package to be installed as a string (that is, in quotes):\n\ninstall.packages(\"ggplot2\")\n\nYou can load a package into the R environment by calling library() with the name of package without quotes. You should only have one package per library call.\n\nlibrary(ggplot2)\n\n\n\nCalling functions from specific packages\nWe can also use the mypackage:: prefix to access package functions without loading:\n\nknitr::kable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1"
  },
  {
    "objectID": "resources/cheatsheet.html#data-visualization-week-2",
    "href": "resources/cheatsheet.html#data-visualization-week-2",
    "title": "Gov 50 Cheat Sheet",
    "section": "Data Visualization (Week 2)",
    "text": "Data Visualization (Week 2)\n\nScatter plot\nYou can produce a scatter plot with using the x and y aesthetics along with the geom_point() function.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point()\n\n\n\n\n\n\nSmoothed curves\nYou can add a smoothed curve that summarizes the relationship between two variables with the geom_smooth() function. By default, it uses a loess smoother to estimated the conditional mean of the y-axis variable as a function of the x-axis variable.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nAdding a regression line\ngeom_smooth can also add a regression line by setting the argument method = \"lm\" and we can turn off the shaded regions around the line with se = FALSE\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nChanging the scale of the axes\nIf we want the scale of the x-axis to be logged to stretch out the data we can use the scale_x_log10():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nAdding informative labels to a plot\nUse the labs() to add informative labels to the plot:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  scale_x_log10() +\n  labs(x = \"Population Density\",\n       y = \"Percent of County Below Poverty Line\",\n       title = \"Poverty and Population Density\",\n       subtitle = \"Among Counties in the Midwest\",\n       source = \"US Census, 2000\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nMapping aesthetics to variables\nIf you would like to map an aesthetic to a variable for all geoms in the plot, you can put it in the aes call in the ggplot() function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty,\n                     color = state,\n                     fill = state)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nMapping aesthetics for a single geom\nYou can also map aesthetics for a specific geom using the mapping argument to that function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(mapping = aes(color = state)) +\n  geom_smooth(color = \"black\") +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nSetting the aesthetics for all observations\nIf you would like to set the color or size or shape of a geom for all data points (that is, not mapped to any variables), be sure to set these outside of aes():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(color = \"purple\") +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nHistograms\n\nggplot(data = midwest,\n       mapping = aes(x = percbelowpoverty)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "resources/cheatsheet.html#data-wrangling-week-2-3",
    "href": "resources/cheatsheet.html#data-wrangling-week-2-3",
    "title": "Gov 50 Cheat Sheet",
    "section": "Data Wrangling (week 2-3)",
    "text": "Data Wrangling (week 2-3)\n\nSubsetting a data frame\nUse the filter() function from the dplyr package to subset a data frame.\n\nlibrary(gov50data)\n\nnews |&gt;\n  filter(weekday == \"Tue\")\n\n# A tibble: 509 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KAEF     ABC         2017-06-13 Tue      0.0242              0.180\n 2 KBVU     FOX         2017-06-13 Tue      0.00894             0.186\n 3 KBZK     CBS         2017-06-13 Tue      0.129               0.306\n 4 KCVU     FOX         2017-06-13 Tue      0.114               0.124\n 5 KECI     NBC         2017-06-13 Tue      0.115               0.283\n 6 KHSL     CBS         2017-06-13 Tue      0.0821              0.274\n 7 KNVN     NBC         2017-06-13 Tue      0.120               0.261\n 8 KPAX     CBS         2017-06-13 Tue      0.0984              0.208\n 9 KRCR     ABC         2017-06-13 Tue      0.0187              0.206\n10 KTMF     ABC         2017-06-13 Tue      0.233               0.101\n# ℹ 499 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nYou can filter based on multiple conditions to subset to the rows that meet all conditions:\n\nnews |&gt;\n  filter(weekday == \"Tue\",\n         affiliation == \"FOX\")\n\n# A tibble: 70 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KBVU     FOX         2017-06-13 Tue      0.00894            0.186 \n 2 KCVU     FOX         2017-06-13 Tue      0.114              0.124 \n 3 WEMT     FOX         2017-06-13 Tue      0.235              0.149 \n 4 WYDO     FOX         2017-06-13 Tue      0.0949             0.182 \n 5 WEMT     FOX         2017-06-20 Tue      0.268              0.134 \n 6 WYDO     FOX         2017-06-20 Tue      0.0590             0.155 \n 7 WEMT     FOX         2017-06-27 Tue      0.0457             0.164 \n 8 WYDO     FOX         2017-06-27 Tue     -0.0588             0.147 \n 9 WEMT     FOX         2017-07-04 Tue      0.0801             0.0986\n10 WYDO     FOX         2017-07-04 Tue      0.0555             0.0910\n# ℹ 60 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nYou can use the | operator to match one of two conditions (“OR” rather than “AND”):\n\nnews |&gt;\n  filter(affiliation == \"FOX\" | affiliation == \"ABC\")\n\n# A tibble: 1,033 × 10\n   callsign affiliation date       weekday  ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 KTMF     ABC         2017-06-07 Wed      0.0842              0.152 \n 2 KTXS     ABC         2017-06-07 Wed     -0.000488            0.0925\n 3 KAEF     ABC         2017-06-08 Thu      0.0426              0.213 \n 4 KBVU     FOX         2017-06-08 Thu     -0.0860              0.169 \n 5 KTMF     ABC         2017-06-08 Thu      0.0433              0.179 \n 6 KTXS     ABC         2017-06-08 Thu      0.0627              0.158 \n 7 WCTI     ABC         2017-06-08 Thu      0.139               0.225 \n 8 KAEF     ABC         2017-06-09 Fri      0.0870              0.153 \n 9 KTXS     ABC         2017-06-09 Fri      0.0879              0.0790\n10 WCTI     ABC         2017-06-09 Fri      0.0667              0.182 \n# ℹ 1,023 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nTo test if a variable is one of several possible values, you can also use the %in% command:\n\nnews |&gt;\n  filter(weekday %in% c(\"Mon\", \"Fri\"))\n\n# A tibble: 1,021 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KAEF     ABC         2017-06-09 Fri       0.0870            0.153 \n 2 KECI     NBC         2017-06-09 Fri       0.115             0.216 \n 3 KPAX     CBS         2017-06-09 Fri       0.0882            0.315 \n 4 KRBC     NBC         2017-06-09 Fri       0.0929            0.152 \n 5 KTAB     CBS         2017-06-09 Fri       0.0588            0.0711\n 6 KTXS     ABC         2017-06-09 Fri       0.0879            0.0790\n 7 WCTI     ABC         2017-06-09 Fri       0.0667            0.182 \n 8 WITN     NBC         2017-06-09 Fri      -0.0683            0.146 \n 9 WNCT     CBS         2017-06-09 Fri      -0.0181            0.126 \n10 WYDO     FOX         2017-06-09 Fri      -0.0330            0.0802\n# ℹ 1,011 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nIf you want to subset to a set of specific row numbers, you can use the slice function:\n\n## subset to the first 5 rows\nnews |&gt;\n  slice(1:5)\n\n# A tibble: 5 × 10\n  callsign affiliation date       weekday ideology national_politics\n  &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n1 KECI     NBC         2017-06-07 Wed       0.0655            0.225 \n2 KPAX     CBS         2017-06-07 Wed       0.0853            0.283 \n3 KRBC     NBC         2017-06-07 Wed       0.0183            0.130 \n4 KTAB     CBS         2017-06-07 Wed       0.0850            0.0901\n5 KTMF     ABC         2017-06-07 Wed       0.0842            0.152 \n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nHere the 1:5 syntax tells R to produce a vector that starts at 1 and ends at 5, incrementing by 1:\n\n1:5\n\n[1] 1 2 3 4 5\n\n\n\n\nFiltering to the largest/smallest values of a variable\nTo subset to the rows that have the largest or smallest values of a given variable, use the slice_max and slice_max functions. For the largest values, use slice_max and use the n argument to specify how many rows you want:\n\nnews |&gt;\n  slice_max(ideology, n = 5)\n\n# A tibble: 5 × 10\n  callsign affiliation date       weekday ideology national_politics\n  &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n1 KAEF     ABC         2017-06-19 Mon        0.778            0.0823\n2 WYDO     FOX         2017-07-19 Wed        0.580            0.126 \n3 KRCR     ABC         2017-10-03 Tue        0.566            0.123 \n4 KAEF     ABC         2017-10-18 Wed        0.496            0.0892\n5 KBVU     FOX         2017-11-16 Thu        0.491            0.159 \n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nTo get lowest values, use slice_min:\n\nnews |&gt;\n  slice_min(ideology, n = 5)\n\n# A tibble: 5 × 10\n  callsign affiliation date       weekday ideology national_politics\n  &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n1 KRBC     NBC         2017-10-19 Thu       -0.674            0.0731\n2 WJHL     CBS         2017-12-08 Fri       -0.673            0.0364\n3 KRBC     NBC         2017-10-18 Wed       -0.586            0.0470\n4 KCVU     FOX         2017-06-22 Thu       -0.414            0.158 \n5 KRBC     NBC         2017-12-11 Mon       -0.365            0.0674\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\n\n\nSorting rows by a variable\nYou can sort the rows of a data set using the arrange() function. By default, this will sort the rows from smallest to largest.\n\nnews |&gt;\n  arrange(ideology)\n\n# A tibble: 2,560 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KRBC     NBC         2017-10-19 Thu       -0.674            0.0731\n 2 WJHL     CBS         2017-12-08 Fri       -0.673            0.0364\n 3 KRBC     NBC         2017-10-18 Wed       -0.586            0.0470\n 4 KCVU     FOX         2017-06-22 Thu       -0.414            0.158 \n 5 KRBC     NBC         2017-12-11 Mon       -0.365            0.0674\n 6 KAEF     ABC         2017-06-21 Wed       -0.315            0.130 \n 7 KTMF     ABC         2017-12-01 Fri       -0.303            0.179 \n 8 KWYB     ABC         2017-12-01 Fri       -0.303            0.160 \n 9 KTVM     NBC         2017-09-01 Fri       -0.302            0.0507\n10 KNVN     NBC         2017-12-08 Fri       -0.299            0.121 \n# ℹ 2,550 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\nIf you would like to sort the rows from largest to smallest (descending order), you can wrap the variable name with desc():\n\nnews |&gt;\n  arrange(desc(ideology))\n\n# A tibble: 2,560 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KAEF     ABC         2017-06-19 Mon        0.778            0.0823\n 2 WYDO     FOX         2017-07-19 Wed        0.580            0.126 \n 3 KRCR     ABC         2017-10-03 Tue        0.566            0.123 \n 4 KAEF     ABC         2017-10-18 Wed        0.496            0.0892\n 5 KBVU     FOX         2017-11-16 Thu        0.491            0.159 \n 6 KTMF     ABC         2017-11-06 Mon        0.455            0.138 \n 7 KAEF     ABC         2017-06-29 Thu        0.447            0.126 \n 8 KPAX     CBS         2017-11-23 Thu        0.437            0.125 \n 9 KTAB     CBS         2017-11-16 Thu        0.427            0.0631\n10 KCVU     FOX         2017-07-06 Thu        0.406            0.154 \n# ℹ 2,550 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\n\n\nSelecting/subsetting the columns\nYou can subset the data to only certain columns using the select() command:\n\nnews |&gt;\n  select(callsign, date, ideology)\n\n# A tibble: 2,560 × 3\n   callsign date        ideology\n   &lt;chr&gt;    &lt;date&gt;         &lt;dbl&gt;\n 1 KECI     2017-06-07  0.0655  \n 2 KPAX     2017-06-07  0.0853  \n 3 KRBC     2017-06-07  0.0183  \n 4 KTAB     2017-06-07  0.0850  \n 5 KTMF     2017-06-07  0.0842  \n 6 KTXS     2017-06-07 -0.000488\n 7 KAEF     2017-06-08  0.0426  \n 8 KBVU     2017-06-08 -0.0860  \n 9 KECI     2017-06-08  0.0902  \n10 KPAX     2017-06-08  0.0668  \n# ℹ 2,550 more rows\n\n\nIf you want to select a range of columns from, say, callsign to ideology, you can use the : operator:\n\nnews |&gt;\n  select(callsign:ideology)\n\n# A tibble: 2,560 × 5\n   callsign affiliation date       weekday  ideology\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;       &lt;dbl&gt;\n 1 KECI     NBC         2017-06-07 Wed      0.0655  \n 2 KPAX     CBS         2017-06-07 Wed      0.0853  \n 3 KRBC     NBC         2017-06-07 Wed      0.0183  \n 4 KTAB     CBS         2017-06-07 Wed      0.0850  \n 5 KTMF     ABC         2017-06-07 Wed      0.0842  \n 6 KTXS     ABC         2017-06-07 Wed     -0.000488\n 7 KAEF     ABC         2017-06-08 Thu      0.0426  \n 8 KBVU     FOX         2017-06-08 Thu     -0.0860  \n 9 KECI     NBC         2017-06-08 Thu      0.0902  \n10 KPAX     CBS         2017-06-08 Thu      0.0668  \n# ℹ 2,550 more rows\n\n\nYou can remove a variable from the data set by using the minus sign - in front of it:\n\nnews |&gt;\n  select(-callsign)\n\n# A tibble: 2,560 × 9\n   affiliation date       weekday  ideology national_politics local_politics\n   &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;       &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1 NBC         2017-06-07 Wed      0.0655              0.225          0.148 \n 2 CBS         2017-06-07 Wed      0.0853              0.283          0.123 \n 3 NBC         2017-06-07 Wed      0.0183              0.130          0.189 \n 4 CBS         2017-06-07 Wed      0.0850              0.0901         0.138 \n 5 ABC         2017-06-07 Wed      0.0842              0.152          0.129 \n 6 ABC         2017-06-07 Wed     -0.000488            0.0925         0.0791\n 7 ABC         2017-06-08 Thu      0.0426              0.213          0.228 \n 8 FOX         2017-06-08 Thu     -0.0860              0.169          0.247 \n 9 NBC         2017-06-08 Thu      0.0902              0.276          0.152 \n10 CBS         2017-06-08 Thu      0.0668              0.305          0.124 \n# ℹ 2,550 more rows\n# ℹ 3 more variables: sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;, month &lt;ord&gt;\n\n\nYou can also drop several variables using the c() function or the (a:b) syntax:\n\nnews |&gt;\n  select(-c(callsign, date, ideology))\n\n# A tibble: 2,560 × 7\n   affiliation weekday national_politics local_politics sinclair2017  post month\n   &lt;chr&gt;       &lt;ord&gt;               &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;\n 1 NBC         Wed                0.225          0.148             1     0 Jun  \n 2 CBS         Wed                0.283          0.123             0     0 Jun  \n 3 NBC         Wed                0.130          0.189             0     0 Jun  \n 4 CBS         Wed                0.0901         0.138             0     0 Jun  \n 5 ABC         Wed                0.152          0.129             0     0 Jun  \n 6 ABC         Wed                0.0925         0.0791            1     0 Jun  \n 7 ABC         Thu                0.213          0.228             1     0 Jun  \n 8 FOX         Thu                0.169          0.247             1     0 Jun  \n 9 NBC         Thu                0.276          0.152             1     0 Jun  \n10 CBS         Thu                0.305          0.124             0     0 Jun  \n# ℹ 2,550 more rows\n\nnews |&gt;\n  select(-(callsign:ideology))\n\n# A tibble: 2,560 × 5\n   national_politics local_politics sinclair2017  post month\n               &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;\n 1            0.225          0.148             1     0 Jun  \n 2            0.283          0.123             0     0 Jun  \n 3            0.130          0.189             0     0 Jun  \n 4            0.0901         0.138             0     0 Jun  \n 5            0.152          0.129             0     0 Jun  \n 6            0.0925         0.0791            1     0 Jun  \n 7            0.213          0.228             1     0 Jun  \n 8            0.169          0.247             1     0 Jun  \n 9            0.276          0.152             1     0 Jun  \n10            0.305          0.124             0     0 Jun  \n# ℹ 2,550 more rows\n\n\nYou can also select columns based on matching patterns in the names with functions like starts_with() or ends_with():\n\nnews |&gt;\n  select(ends_with(\"politics\"))\n\n# A tibble: 2,560 × 2\n   national_politics local_politics\n               &lt;dbl&gt;          &lt;dbl&gt;\n 1            0.225          0.148 \n 2            0.283          0.123 \n 3            0.130          0.189 \n 4            0.0901         0.138 \n 5            0.152          0.129 \n 6            0.0925         0.0791\n 7            0.213          0.228 \n 8            0.169          0.247 \n 9            0.276          0.152 \n10            0.305          0.124 \n# ℹ 2,550 more rows\n\n\nThis code finds all variables with column names that end with the string “politics”. See the help page for select() for more information on different ways to select.\n\n\nRenaming a variable\nYou can rename a variable useing the function rename(new_name = old_name):\n\nnews |&gt;\n  rename(call_sign = callsign)\n\n# A tibble: 2,560 × 10\n   call_sign affiliation date       weekday  ideology national_politics\n   &lt;chr&gt;     &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 KECI      NBC         2017-06-07 Wed      0.0655              0.225 \n 2 KPAX      CBS         2017-06-07 Wed      0.0853              0.283 \n 3 KRBC      NBC         2017-06-07 Wed      0.0183              0.130 \n 4 KTAB      CBS         2017-06-07 Wed      0.0850              0.0901\n 5 KTMF      ABC         2017-06-07 Wed      0.0842              0.152 \n 6 KTXS      ABC         2017-06-07 Wed     -0.000488            0.0925\n 7 KAEF      ABC         2017-06-08 Thu      0.0426              0.213 \n 8 KBVU      FOX         2017-06-08 Thu     -0.0860              0.169 \n 9 KECI      NBC         2017-06-08 Thu      0.0902              0.276 \n10 KPAX      CBS         2017-06-08 Thu      0.0668              0.305 \n# ℹ 2,550 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\n\n\nCreating new variables\nYou can create new variables that are functions of old variables using the mutate() function:\n\nnews |&gt; mutate(\n  national_local_diff = national_politics - local_politics) |&gt;\n  select(callsign, date, national_politics, local_politics,\n         national_local_diff)\n\n# A tibble: 2,560 × 5\n   callsign date       national_politics local_politics national_local_diff\n   &lt;chr&gt;    &lt;date&gt;                 &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n 1 KECI     2017-06-07            0.225          0.148               0.0761\n 2 KPAX     2017-06-07            0.283          0.123               0.160 \n 3 KRBC     2017-06-07            0.130          0.189              -0.0589\n 4 KTAB     2017-06-07            0.0901         0.138              -0.0476\n 5 KTMF     2017-06-07            0.152          0.129               0.0229\n 6 KTXS     2017-06-07            0.0925         0.0791              0.0134\n 7 KAEF     2017-06-08            0.213          0.228              -0.0151\n 8 KBVU     2017-06-08            0.169          0.247              -0.0781\n 9 KECI     2017-06-08            0.276          0.152               0.124 \n10 KPAX     2017-06-08            0.305          0.124               0.180 \n# ℹ 2,550 more rows\n\n\n\n\nCreating new variables based on yes/no conditions\nIf you want to create a new variable that can take on two values based on a logical conditional, you should use the if_else() function inside of mutate(). For instance, if we want to create a more nicely labeled version of the sinclair2017 variable (which is 0/1), we could do:\n\nnews |&gt;\n  mutate(Ownership = if_else(sinclair2017 == 1,\n                             \"Acquired by Sinclair\",\n                             \"Not Acquired\")) |&gt;\n  select(callsign, affiliation, date, sinclair2017, Ownership)\n\n# A tibble: 2,560 × 5\n   callsign affiliation date       sinclair2017 Ownership           \n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt;               \n 1 KECI     NBC         2017-06-07            1 Acquired by Sinclair\n 2 KPAX     CBS         2017-06-07            0 Not Acquired        \n 3 KRBC     NBC         2017-06-07            0 Not Acquired        \n 4 KTAB     CBS         2017-06-07            0 Not Acquired        \n 5 KTMF     ABC         2017-06-07            0 Not Acquired        \n 6 KTXS     ABC         2017-06-07            1 Acquired by Sinclair\n 7 KAEF     ABC         2017-06-08            1 Acquired by Sinclair\n 8 KBVU     FOX         2017-06-08            1 Acquired by Sinclair\n 9 KECI     NBC         2017-06-08            1 Acquired by Sinclair\n10 KPAX     CBS         2017-06-08            0 Not Acquired        \n# ℹ 2,550 more rows\n\n\n\n\nSummarizing a variable\nYou can calculate summaries of variables in the data set using the summarize() function.\n\nnews |&gt;\n  summarize(\n    avg_ideology = mean(ideology),\n    sd_ideology = sd(ideology),\n    median_ideology = median(ideology)\n  )\n\n# A tibble: 1 × 3\n  avg_ideology sd_ideology median_ideology\n         &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1       0.0907      0.0962          0.0929\n\n\n\n\nSummarizing variables by groups of rows\nBy default, summarize() calculates the summaries of variables for all rows in the data frame. You can also calculate these summaries within groups of rows defined by another variable in the data frame using the group_by() function before summarizing.\n\nnews |&gt;\n  group_by(month) |&gt;\n  summarize(\n    avg_ideology = mean(ideology),\n    sd_ideology = sd(ideology),\n    median_ideology = median(ideology)\n  )\n\n# A tibble: 7 × 4\n  month avg_ideology sd_ideology median_ideology\n  &lt;ord&gt;        &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1 Jun         0.0786      0.0979          0.0838\n2 Jul         0.103       0.0888          0.0992\n3 Aug         0.105       0.0811          0.106 \n4 Sep         0.0751      0.0902          0.0768\n5 Oct         0.0862      0.110           0.0853\n6 Nov         0.0972      0.0893          0.102 \n7 Dec         0.0774      0.125           0.0878\n\n\nHere, the summarize() function breaks apart the original data into smaller data frames for each month and applies the summary functions to those, then combines everything into one tibble.\n\n\nSummarizing by multiple variables\nYou can group by multiple variables and summarize() will create groups based on every combination of each variable:\n\nnews |&gt;\n  group_by(sinclair2017, post) |&gt;\n  summarize(\n    avg_ideology = mean(ideology)\n  )\n\n`summarise()` has grouped output by 'sinclair2017'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   sinclair2017 [2]\n  sinclair2017  post avg_ideology\n         &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1            0     0       0.100 \n2            0     1       0.0768\n3            1     0       0.0936\n4            1     1       0.0938\n\n\nYou’ll notice the message that summarize() sends after using to let us know that resulting tibble is grouped by sinclair2017. By default, summarize() drops the last group you provided in group_by (post in this case). This isn’t an error message, it’s just letting us know some helpful information. If you want to avoid this messaging displaying, you need to specify what grouping you want after using the .groups argument:\n\nnews |&gt;\n  group_by(sinclair2017, post) |&gt;\n  summarize(\n    avg_ideology = mean(ideology),\n    .groups = \"drop_last\"\n  )\n\n# A tibble: 4 × 3\n# Groups:   sinclair2017 [2]\n  sinclair2017  post avg_ideology\n         &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1            0     0       0.100 \n2            0     1       0.0768\n3            1     0       0.0936\n4            1     1       0.0938\n\n\n\n\nSummarizing across many variables\nIf you want to apply the same summary to multiple variables, you can use the across(vars, fun) function, where vars is a vector of variable names (specified like with select()) and fun is a summary function to apply to those variables.\n\nnews |&gt;\n  group_by(sinclair2017, post) |&gt;\n  summarize(\n    across(c(ideology, national_politics), mean)\n  )\n\n`summarise()` has grouped output by 'sinclair2017'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   sinclair2017 [2]\n  sinclair2017  post ideology national_politics\n         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n1            0     0   0.100              0.134\n2            0     1   0.0768             0.126\n3            1     0   0.0936             0.137\n4            1     1   0.0938             0.155\n\n\nAs with select(), you can use the : operator to select a range of variables\n\nnews |&gt;\n  group_by(sinclair2017, post) |&gt;\n  summarize(\n    across(ideology:local_politics, mean)\n  )\n\n`summarise()` has grouped output by 'sinclair2017'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 5\n# Groups:   sinclair2017 [2]\n  sinclair2017  post ideology national_politics local_politics\n         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1            0     0   0.100              0.134          0.168\n2            0     1   0.0768             0.126          0.167\n3            1     0   0.0936             0.137          0.157\n4            1     1   0.0938             0.155          0.139\n\n\n\n\nTable of counts of a categorical variable\nThere are two way to produce a table of counts of each category of a variable. The first is to use group_by and summarize along with the summary function n(), which returns the numbers of rows in each grouping (that is, each combination of the grouping variables):\n\nnews |&gt;\n  group_by(affiliation) |&gt;\n  summarize(n = n())\n\n# A tibble: 4 × 2\n  affiliation     n\n  &lt;chr&gt;       &lt;int&gt;\n1 ABC           687\n2 CBS           758\n3 FOX           346\n4 NBC           769\n\n\nA simpler way to acheive the same outcome is to use the count() function, which implements these two steps:\n\nnews |&gt;\n  count(affiliation)\n\n# A tibble: 4 × 2\n  affiliation     n\n  &lt;chr&gt;       &lt;int&gt;\n1 ABC           687\n2 CBS           758\n3 FOX           346\n4 NBC           769\n\n\n\n\nProducing nicely formatted tables with kable()\nYou can take any tibble in R and convert it into a more readable output by passing it to knitr::kable(). In our homework, generally, we will save the tibble as an object and then pass it to this function.\n\nmonth_summary &lt;- news |&gt;\n  group_by(month) |&gt;\n  summarize(\n    avg_ideology = mean(ideology),\n    sd_ideology = sd(ideology)\n  )\n\nknitr::kable(month_summary)\n\n\n\n\nmonth\navg_ideology\nsd_ideology\n\n\n\n\nJun\n0.0785518\n0.0979339\n\n\nJul\n0.1032917\n0.0888328\n\n\nAug\n0.1049908\n0.0811141\n\n\nSep\n0.0751067\n0.0902003\n\n\nOct\n0.0861639\n0.1098108\n\n\nNov\n0.0971796\n0.0892845\n\n\nDec\n0.0773873\n0.1246451\n\n\n\n\n\nYou can add informative column names to the table using the col.names argument.\n\nknitr::kable(\n  month_summary,\n  col.names = c(\"Month\", \"Average Ideology\", \"SD of Ideology\")\n)\n\n\n\n\nMonth\nAverage Ideology\nSD of Ideology\n\n\n\n\nJun\n0.0785518\n0.0979339\n\n\nJul\n0.1032917\n0.0888328\n\n\nAug\n0.1049908\n0.0811141\n\n\nSep\n0.0751067\n0.0902003\n\n\nOct\n0.0861639\n0.1098108\n\n\nNov\n0.0971796\n0.0892845\n\n\nDec\n0.0773873\n0.1246451\n\n\n\n\n\nFinally, we can round the numbers in the table to be a bit nicer using the digits argument. This will tell kable() how many significant digiits to show.\n\nknitr::kable(\n  month_summary,\n  col.names = c(\"Month\", \"Average Ideology\", \"SD of Ideology\"),\n  digits = 3\n)\n\n\n\n\nMonth\nAverage Ideology\nSD of Ideology\n\n\n\n\nJun\n0.079\n0.098\n\n\nJul\n0.103\n0.089\n\n\nAug\n0.105\n0.081\n\n\nSep\n0.075\n0.090\n\n\nOct\n0.086\n0.110\n\n\nNov\n0.097\n0.089\n\n\nDec\n0.077\n0.125\n\n\n\n\n\n\n\nBarplots of counts\nYou can visualize counts of a variable using a barplot:\n\nnews |&gt;\n  ggplot(mapping = aes(x = affiliation)) +\n  geom_bar()\n\n\n\n\n\n\nBarplots of other summaries\nWe can use barplots to visualize other grouped summaries like means, but we need to use the geom_col() geom instead and specify the variable you want to be the height of the bars .\n\nnews |&gt;\n  group_by(affiliation) |&gt;\n  summarize(\n    avg_ideology = mean(ideology)\n  ) |&gt;\n  ggplot(mapping = aes(x = affiliation, y = avg_ideology)) +\n  geom_col()\n\n\n\n\n\n\nReordering/sorting barplot axes\nOften we want to sort the barplot axes to be in the order of the variable of interest so we can quickly compare them. We can use the fct_reorder(group_var, ordering_var) function to do this where the group_var is the grouping variable that is going on the axes and the ordering_var is the variable that we will sort the groups on.\n\nnews |&gt;\n  group_by(affiliation) |&gt;\n  summarize(\n    avg_ideology = mean(ideology)\n  ) |&gt;\n  ggplot(mapping = aes(x = fct_reorder(affiliation, avg_ideology),\n                       y = avg_ideology)) +\n  geom_col()\n\n\n\n\n\n\nColoring barplots by another variable\nYou can color the barplots by a another variable using the fill aesthetic:\n\nnews |&gt;\n  group_by(callsign, affiliation) |&gt;\n  summarize(\n    avg_ideology = mean(ideology)\n  ) |&gt;\n  slice_max(avg_ideology, n = 10) |&gt;\n  ggplot(mapping = aes(y = fct_reorder(callsign, avg_ideology),\n                       x = avg_ideology)) +\n  geom_col(mapping = aes(fill = affiliation))\n\n`summarise()` has grouped output by 'callsign'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nCreating logical vectors\nYou can create logical variables in your tibbles using mutate:\n\nnews |&gt;\n  mutate(\n    right_leaning = ideology &gt; 0,\n    fall = month == \"Sep\" | month == \"Oct\" | month == \"Nov\",\n    .keep = \"used\"\n)\n\n# A tibble: 2,560 × 4\n    ideology month right_leaning fall \n       &lt;dbl&gt; &lt;ord&gt; &lt;lgl&gt;         &lt;lgl&gt;\n 1  0.0655   Jun   TRUE          FALSE\n 2  0.0853   Jun   TRUE          FALSE\n 3  0.0183   Jun   TRUE          FALSE\n 4  0.0850   Jun   TRUE          FALSE\n 5  0.0842   Jun   TRUE          FALSE\n 6 -0.000488 Jun   FALSE         FALSE\n 7  0.0426   Jun   TRUE          FALSE\n 8 -0.0860   Jun   FALSE         FALSE\n 9  0.0902   Jun   TRUE          FALSE\n10  0.0668   Jun   TRUE          FALSE\n# ℹ 2,550 more rows\n\n\nThe .keep = \"used\" argument here tells mutate to only return the variables created and any variables used to create them. We’re using it here for display purposes.\nYou can filter based on these logical variables. In particular, if we want to subset to rows where both right_leaning and fall were TRUE we could do the following filter:\n\nnews |&gt;\n  mutate(\n    right_leaning = ideology &gt; 0,\n    fall = month == \"Sep\" | month == \"Oct\" | month == \"Nov\",\n    .keep = \"used\"\n  ) |&gt;\n  filter(right_leaning & fall)\n\n# A tibble: 1,050 × 4\n   ideology month right_leaning fall \n      &lt;dbl&gt; &lt;ord&gt; &lt;lgl&gt;         &lt;lgl&gt;\n 1   0.121  Sep   TRUE          TRUE \n 2   0.0564 Sep   TRUE          TRUE \n 3   0.0564 Sep   TRUE          TRUE \n 4   0.324  Sep   TRUE          TRUE \n 5   0.0649 Sep   TRUE          TRUE \n 6   0.0613 Sep   TRUE          TRUE \n 7   0.187  Sep   TRUE          TRUE \n 8   0.0297 Sep   TRUE          TRUE \n 9   0.151  Sep   TRUE          TRUE \n10   0.186  Sep   TRUE          TRUE \n# ℹ 1,040 more rows\n\n\n\n\nUsing ! to negate logicals\nAny time you place the exclamation point in front of a logical, it will turn any TRUE into a FALSE and vice versa. For instance, if we wanted left leaning days in the fall, we could used\n\nnews |&gt;\n  mutate(\n    right_leaning = ideology &gt; 0,\n    fall = month == \"Sep\" | month == \"Oct\" | month == \"Nov\",\n    .keep = \"used\"\n  ) |&gt;\n  filter(!right_leaning & fall)\n\n# A tibble: 167 × 4\n   ideology month right_leaning fall \n      &lt;dbl&gt; &lt;ord&gt; &lt;lgl&gt;         &lt;lgl&gt;\n 1 -0.0387  Sep   FALSE         TRUE \n 2 -0.302   Sep   FALSE         TRUE \n 3 -0.00694 Sep   FALSE         TRUE \n 4 -0.0140  Sep   FALSE         TRUE \n 5 -0.0294  Sep   FALSE         TRUE \n 6 -0.0113  Sep   FALSE         TRUE \n 7 -0.105   Sep   FALSE         TRUE \n 8 -0.0286  Sep   FALSE         TRUE \n 9 -0.0462  Sep   FALSE         TRUE \n10 -0.0313  Sep   FALSE         TRUE \n# ℹ 157 more rows\n\n\nOr if we wanted to subset to any combination except right leaning and fall, we could negate the AND statement using parentheses:\n\nnews |&gt;\n  mutate(\n    right_leaning = ideology &gt; 0,\n    fall = month == \"Sep\" | month == \"Oct\" | month == \"Nov\",\n    .keep = \"used\"\n  ) |&gt;\n  filter(!(right_leaning & fall))\n\n# A tibble: 1,510 × 4\n    ideology month right_leaning fall \n       &lt;dbl&gt; &lt;ord&gt; &lt;lgl&gt;         &lt;lgl&gt;\n 1  0.0655   Jun   TRUE          FALSE\n 2  0.0853   Jun   TRUE          FALSE\n 3  0.0183   Jun   TRUE          FALSE\n 4  0.0850   Jun   TRUE          FALSE\n 5  0.0842   Jun   TRUE          FALSE\n 6 -0.000488 Jun   FALSE         FALSE\n 7  0.0426   Jun   TRUE          FALSE\n 8 -0.0860   Jun   FALSE         FALSE\n 9  0.0902   Jun   TRUE          FALSE\n10  0.0668   Jun   TRUE          FALSE\n# ℹ 1,500 more rows\n\n\nThis is often used in combination with %in% to acheive a “not in” logical:\n\nnews |&gt;\n  filter(!(affiliation %in% c(\"FOX\", \"ABC\")))\n\n# A tibble: 1,527 × 10\n   callsign affiliation date       weekday ideology national_politics\n   &lt;chr&gt;    &lt;chr&gt;       &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1 KECI     NBC         2017-06-07 Wed       0.0655            0.225 \n 2 KPAX     CBS         2017-06-07 Wed       0.0853            0.283 \n 3 KRBC     NBC         2017-06-07 Wed       0.0183            0.130 \n 4 KTAB     CBS         2017-06-07 Wed       0.0850            0.0901\n 5 KECI     NBC         2017-06-08 Thu       0.0902            0.276 \n 6 KPAX     CBS         2017-06-08 Thu       0.0668            0.305 \n 7 KRBC     NBC         2017-06-08 Thu       0.108             0.156 \n 8 KTAB     CBS         2017-06-08 Thu      -0.0178            0.0726\n 9 KECI     NBC         2017-06-09 Fri       0.115             0.216 \n10 KPAX     CBS         2017-06-09 Fri       0.0882            0.315 \n# ℹ 1,517 more rows\n# ℹ 4 more variables: local_politics &lt;dbl&gt;, sinclair2017 &lt;dbl&gt;, post &lt;dbl&gt;,\n#   month &lt;ord&gt;\n\n\n\n\nGrouped summaries with any() and all()\nOnce you group a tibble, you can summarize logicals within groups using two commands. any() will return TRUE if a logical is TRUE for any row in a group and FALSE otherwise. all() will return TRUE when the logical inside it is TRUE for all rows in a group and FALSE otherwise.\n\nnews |&gt;\n  group_by(callsign) |&gt;\n  summarize(\n    any_liberal = any(ideology &lt; 0),\n    all_local = all(national_politics &lt; local_politics)\n  )\n\n# A tibble: 22 × 3\n   callsign any_liberal all_local\n   &lt;chr&gt;    &lt;lgl&gt;       &lt;lgl&gt;    \n 1 KAEF     TRUE        FALSE    \n 2 KBVU     TRUE        FALSE    \n 3 KBZK     TRUE        FALSE    \n 4 KCVU     TRUE        FALSE    \n 5 KECI     TRUE        FALSE    \n 6 KHSL     TRUE        FALSE    \n 7 KNVN     TRUE        FALSE    \n 8 KPAX     TRUE        FALSE    \n 9 KRBC     TRUE        FALSE    \n10 KRCR     TRUE        FALSE    \n# ℹ 12 more rows\n\n\n\n\nConverting logicals\nWhen passed to sum() or mean(), TRUE is converted to 1 and FALSE is converted to 0. This means that sum() will return the number of TRUE values in a vector and mean() will return the proportion of TRUE values.\n\nsum(c(TRUE, FALSE, TRUE, FALSE))\n\n[1] 2\n\nmean(c(TRUE, FALSE, TRUE, FALSE))\n\n[1] 0.5\n\n\n\n\nGrouped logical summaries with sums and means\nAfter grouping, you can summarize using either the sum() or mean() function on a logical. In this case, the sum() will give you the number of time the statement is TRUE within a group, and mean() will give you the proportion of rows that are TRUE within a group.\n\nnews |&gt;\n  group_by(callsign) |&gt;\n  summarize(\n    prop_liberal = mean(ideology &lt; 0),\n    num_local_bigger = sum(national_politics &lt; local_politics)\n  )\n\n# A tibble: 22 × 3\n   callsign prop_liberal num_local_bigger\n   &lt;chr&gt;           &lt;dbl&gt;            &lt;int&gt;\n 1 KAEF           0.138               111\n 2 KBVU           0.143                31\n 3 KBZK           0.0526               11\n 4 KCVU           0.185                38\n 5 KECI           0.137                44\n 6 KHSL           0.132               127\n 7 KNVN           0.115               130\n 8 KPAX           0.0833               74\n 9 KRBC           0.196               103\n10 KRCR           0.0992               99\n# ℹ 12 more rows"
  },
  {
    "objectID": "resources/cheatsheet.html#causality-week-4",
    "href": "resources/cheatsheet.html#causality-week-4",
    "title": "Gov 50 Cheat Sheet",
    "section": "Causality (Week 4)",
    "text": "Causality (Week 4)\nFor this week, we’ll use the data from the transphobia randomized experiment\n\ntrans\n\n# A tibble: 565 × 9\n     age female voted_gen_14 voted_gen_12 treat_ind racename         democrat\n   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n 1    29      0            0            1         0 African American        1\n 2    59      1            1            0         1 African American        1\n 3    35      1            1            1         1 African American        1\n 4    63      1            1            1         1 African American        1\n 5    65      0            1            1         1 African American        0\n 6    51      1            1            1         0 Caucasian               0\n 7    26      1            1            1         0 African American        1\n 8    62      1            1            1         1 African American        1\n 9    37      0            1            1         0 Caucasian               0\n10    51      1            1            1         0 Caucasian               0\n# ℹ 555 more rows\n# ℹ 2 more variables: nondiscrim_pre &lt;dbl&gt;, nondiscrim_post &lt;dbl&gt;\n\n\n\nCalculating a difference in means by filtering\nYou can calculate a difference in means by creating separate summaries of the treatment and control arms of an experiment or observational study and then taking the differences between them:\n\ntreat_mean &lt;- trans |&gt;\n  filter(treat_ind == 1) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post))\ntreat_mean\n\n# A tibble: 1 × 1\n  nondiscrim_mean\n            &lt;dbl&gt;\n1           0.687\n\ncontrol_mean &lt;- trans |&gt;\n  filter(treat_ind == 0) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post))\ncontrol_mean\n\n# A tibble: 1 × 1\n  nondiscrim_mean\n            &lt;dbl&gt;\n1           0.648\n\ndiff_in_means &lt;- treat_mean - control_mean\ndiff_in_means\n\n  nondiscrim_mean\n1      0.03896674\n\n\n\n\nPivoting a tibble to make a cross-tab\nWhen grouping by two variables, we sometimes would like the values of one variable to be in the rows and values of the other variable to be in the columns. But group_by() and summarize() by default puts each combination of values for the two variables into rows. We can use the pivot_wider function to move one of the variables (specified with the names_from argument) to the columns. The values_from argument tells R what variable should it use to fil in the data in the resulting table.\nSo if we wanted to get counts of race (rows) by treatment group (columns), we could start with getting the counts:\n\ntrans |&gt;\n  group_by(treat_ind, racename) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'treat_ind'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9 × 3\n# Groups:   treat_ind [2]\n  treat_ind racename             n\n      &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;\n1         0 African American    58\n2         0 Asian                2\n3         0 Caucasian           77\n4         0 Hispanic           150\n5         1 African American    68\n6         1 Asian                4\n7         1 Caucasian           75\n8         1 Hispanic           130\n9         1 Native American      1\n\n\nThen we can pivot the treatment group to the columns:\n\ntrans |&gt;\n  group_by(treat_ind, racename) |&gt;\n  summarize(n = n()) |&gt;\n  pivot_wider(\n    names_from = treat_ind,\n    values_from = n\n  )\n\n`summarise()` has grouped output by 'treat_ind'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n  racename           `0`   `1`\n  &lt;chr&gt;            &lt;int&gt; &lt;int&gt;\n1 African American    58    68\n2 Asian                2     4\n3 Caucasian           77    75\n4 Hispanic           150   130\n5 Native American     NA     1\n\n\n\n\nCalculating difference in means by pivoting\nNow that we know pivoting, we can use this to calculate difference in means instead of the filter approach. We first calculate the means by treatment group, pivot the rows to columns, and finally use mutate() to create the difference between the means.\n\ntrans |&gt;\n  group_by(treat_ind) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post)) |&gt;\n  pivot_wider(\n    names_from = treat_ind,\n    values_from = nondiscrim_mean\n  ) |&gt;\n  mutate(\n    diff_in_means = `1` - `0`\n  ) \n\n# A tibble: 1 × 3\n    `0`   `1` diff_in_means\n  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 0.648 0.687        0.0390\n\n\n\n\nCreating new labels for variables to make nicer tables\nAbove we had to use the backticks to refer to the 1/0 variable names when calculating the differences in means. We can create slightly nicer output by relabeling the treat_ind variable using if_else:\n\ntrans |&gt;\n  mutate(\n    treat_ind = if_else(treat_ind == 1, \"Treated\", \"Control\")\n  ) |&gt;  \n  group_by(treat_ind) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post)) |&gt;\n  pivot_wider(\n    names_from = treat_ind,\n    values_from = nondiscrim_mean\n  ) |&gt;\n  mutate(\n    diff_in_means = Treated - Control\n  ) \n\n# A tibble: 1 × 3\n  Control Treated diff_in_means\n    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1   0.648   0.687        0.0390\n\n\n\n\nCalculating difference in means by two-level grouping variable\nIf we have a two-level grouping variable like democrat, we can create nice labels for that variable and just add it to the group_by call to get the difference in means within levels of this variable:\n\ntrans |&gt;\n  mutate(\n    treat_ind = if_else(treat_ind == 1, \"Treated\", \"Control\"),\n    party = if_else(democrat == 1, \"Democrat\", \"Non-Democrat\")\n  ) |&gt;\n  group_by(treat_ind, party) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post)) |&gt;\n  pivot_wider(\n    names_from = treat_ind,\n    values_from = nondiscrim_mean\n  ) |&gt;  \n  mutate(\n    diff_in_means = Treated - Control\n  )\n\n`summarise()` has grouped output by 'treat_ind'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 4\n  party        Control Treated diff_in_means\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Democrat       0.704   0.754        0.0498\n2 Non-Democrat   0.605   0.628        0.0234\n\n\n\n\nCreating grouping variables with more than 2 levels\nYou can use case_when() to create a variable that can take on more than 2 values based on conditions of a variable in the tibble:\n\ntrans |&gt;\n  mutate(\n    age_group = case_when(\n      age &lt; 25 ~ \"Under 25\",\n      age &gt;=25 & age &lt; 65 ~ \"Bewteen 25 and 65\",\n      age &gt;= 65 ~ \"Older than 65\"\n    )\n  ) |&gt;\n  select(age, age_group)\n\n# A tibble: 565 × 2\n     age age_group        \n   &lt;dbl&gt; &lt;chr&gt;            \n 1    29 Bewteen 25 and 65\n 2    59 Bewteen 25 and 65\n 3    35 Bewteen 25 and 65\n 4    63 Bewteen 25 and 65\n 5    65 Older than 65    \n 6    51 Bewteen 25 and 65\n 7    26 Bewteen 25 and 65\n 8    62 Bewteen 25 and 65\n 9    37 Bewteen 25 and 65\n10    51 Bewteen 25 and 65\n# ℹ 555 more rows\n\n\n\n\nCalculating difference in mean by grouping variable with more than 2 levels\n\ntrans |&gt;\n  mutate(\n    treat_ind = if_else(treat_ind == 1, \"Treated\", \"Control\"),\n    age_group = case_when(\n      age &lt; 25 ~ \"Under 25\",\n      age &gt;=25 & age &lt; 65 ~ \"Bewteen 25 and 65\",\n      age &gt;= 65 ~ \"Older than 65\"\n    )\n  ) |&gt;\n  group_by(treat_ind, age_group) |&gt;\n  summarize(nondiscrim_mean = mean(nondiscrim_post)) |&gt;\n  pivot_wider(\n    names_from = treat_ind,\n    values_from = nondiscrim_mean\n  ) |&gt;  \n  mutate(\n    diff_in_means = Treated - Control\n  )\n\n`summarise()` has grouped output by 'treat_ind'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 4\n  age_group         Control Treated diff_in_means\n  &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 Bewteen 25 and 65   0.694   0.683       -0.0112\n2 Older than 65       0.576   0.614        0.0378\n3 Under 25            0.556   0.829        0.273 \n\n\n\n\nCalculating before and after difference in means\nWhen we’re estimating causal effects using a before and after design, we can usually create a variable that is the change in the outcome over time for each row and then take the mean of that. For example, in the newspapers data we have the vote for Labour outcome before the treatment (1992) and after (1997):\n\nnewspapers |&gt;\n  filter(to_labour == 1) |&gt;  \n  mutate(\n    vote_change = vote_lab_97 - vote_lab_92\n  ) |&gt;  \n  summarize(avg_change = mean(vote_change))\n\n# A tibble: 1 × 1\n  avg_change\n       &lt;dbl&gt;\n1      0.190\n\n\n\n\nCalculating difference-in-differences estimator\nWhen you want to use the difference in differences estimator, you simply need to take the difference between the average changes over time in the treatment group (to_labour == 1) and the control group (to_labour == 0). We can do this by adding a group_by step:\n\nnewspapers |&gt;\n  mutate(\n    vote_change = vote_lab_97 - vote_lab_92,\n    to_labour = if_else(to_labour == 1, \"switched\", \"unswitched\")\n  ) |&gt;\n  group_by(to_labour) |&gt;  \n  summarize(avg_change = mean(vote_change)) |&gt;\n  pivot_wider(\n    names_from = to_labour,\n    values_from = avg_change\n  ) |&gt;\n  mutate(DID = switched - unswitched)\n\n# A tibble: 1 × 3\n  switched unswitched    DID\n     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1    0.190      0.110 0.0796"
  },
  {
    "objectID": "resources/cheatsheet.html#summarizing-data-week-5",
    "href": "resources/cheatsheet.html#summarizing-data-week-5",
    "title": "Gov 50 Cheat Sheet",
    "section": "Summarizing data (Week 5)",
    "text": "Summarizing data (Week 5)\n\nCalculating the mean, median, and standard deviation\nMany of the summary functions have intuitive names:\n\ngapminder |&gt;\n  summarize(\n    gdp_mean = mean(gdpPercap),\n    gdp_median = median(gdpPercap),\n    gdp_sd = sd(gdpPercap)\n  )\n\n# A tibble: 1 × 3\n  gdp_mean gdp_median gdp_sd\n     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1    7215.      3532.  9857.\n\n\nYou can also use these functions outside of a summarize() call by accessing variables with the $ operator:\n\nmean(gapminder$gdpPercap)\n\n[1] 7215.327\n\n\n\n\nAdding a vertical line to a plot\nYou can layer on vertical lines to a ggplot using the geom_vline() function. For horizontal lines, use geom_hline().\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(aes(xintercept = mean(lifeExp)), color = \"indianred\") +\n  geom_vline(aes(xintercept = median(lifeExp)), color = \"dodgerblue\")\n\n\n\n\n\n\nDropping missing rows with missing data\nTo drop any row of a data frame with missing values in any variable, you can use the drop_na() function. Here we use the nrow() function to show how many rows each tibble has.\n\ncces_2020 |&gt;\n  nrow()\n\n[1] 51551\n\n\n\ncces_2020 |&gt;\n  drop_na() |&gt;\n  nrow()\n\n[1] 45651\n\n\nBy default drop_na() drops rows that have any missingness. If you want to drop rows based on being missing only for a particular variable, just pass the variable name to the function:\n\ncces_2020 |&gt;\n  drop_na(turnout_self) |&gt;\n  nrow()\n\n[1] 48462\n\n\n\n\nRemoving missing values in summary functions\nMost summary functions have an argument na.rm (for “NA remove”) that is FALSE by default, meaning it tries to calculate summaries with missing values included:\n\ncces_2020 |&gt;\n  summarize(avg_turnout = mean(turnout_self))\n\n# A tibble: 1 × 1\n  avg_turnout\n        &lt;dbl&gt;\n1          NA\n\n\nTo remove missing values and calculate the summary based on the observed values for that variable, set na.rm = TRUE:\n\ncces_2020 |&gt;\n  summarize(avg_turnout = mean(turnout_self, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_turnout\n        &lt;dbl&gt;\n1       0.942\n\n\n\n\nDetecting missing values\nYou cannot use x == NA for detecting missing values in R because it treats missing values quite literally and just returns NA for all values:\n\nc(5, 6, NA, 0) == NA\n\n[1] NA NA NA NA\n\n\nInstead you can use is.na():\n\nis.na(c(5, 6, NA, 0))\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nYou can negate this to test for non-missing values:\n\n!is.na(c(5, 6, NA, 0))\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\nUsing is.na to summarize missingness\nApplying mean() to is.na() output will tell us the proportion of values in a vector that are missing. We can combine this with group_by and summarize to see what share of each party ID response are missing turnout:\n\ncces_2020 |&gt;\n  group_by(pid3) |&gt;\n  summarize(\n    missing_turnout = mean(is.na(turnout_self))\n  )\n\n# A tibble: 5 × 2\n  pid3        missing_turnout\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 Democrat             0.0280\n2 Republican           0.0403\n3 Independent          0.0718\n4 Other                0.0709\n5 Not sure             0.431 \n\n\n\n\nProportion table for one variable\nTo get a proportion table for a single categorical variable, start with the approach to creating a table of counts of each category. Then just add a mutate() step to get the proportion:\n\ncces_2020 |&gt;\n  group_by(pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(\n    prop = n / sum(n)\n  )\n\n# A tibble: 7 × 3\n  pres_vote                        n     prop\n  &lt;fct&gt;                        &lt;int&gt;    &lt;dbl&gt;\n1 Joe Biden (Democrat)         26188 0.508   \n2 Donald J. Trump (Republican) 17702 0.343   \n3 Other                         1458 0.0283  \n4 I did not vote in this race    100 0.00194 \n5 I did not vote                  13 0.000252\n6 Not sure                       190 0.00369 \n7 &lt;NA&gt;                          5900 0.114   \n\n\n\n\nProportion table with two variables\nWith two variables, we take the same approach as for one variable, but we need to be careful about the grouping:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n))\n\n`summarise()` has grouped output by 'pid3'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 4\n# Groups:   pid3 [5]\n   pid3        pres_vote                        n   prop\n   &lt;fct&gt;       &lt;fct&gt;                        &lt;int&gt;  &lt;dbl&gt;\n 1 Democrat    Joe Biden (Democrat)         17649 0.968 \n 2 Democrat    Donald J. Trump (Republican)   581 0.0319\n 3 Republican  Joe Biden (Democrat)           856 0.0712\n 4 Republican  Donald J. Trump (Republican) 11164 0.929 \n 5 Independent Joe Biden (Democrat)          6601 0.571 \n 6 Independent Donald J. Trump (Republican)  4951 0.429 \n 7 Other       Joe Biden (Democrat)           735 0.487 \n 8 Other       Donald J. Trump (Republican)   774 0.513 \n 9 Not sure    Joe Biden (Democrat)           347 0.599 \n10 Not sure    Donald J. Trump (Republican)   232 0.401 \n\n\nAfter summarizing, the tibble drops the last grouping variable and groups the resulting tibble by the remaining variable(s). In this case, pres_vote is dropped and the tibble is now grouped by pid3. This means that when we call mutate(), the calculations will be done within levels of pid3. So the first row here is telling us the the share of Democrats that voted for Biden is 0.968.\nIf we want to have the proportion be as a share of the pres_vote instead, we can reorder the grouping variables:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pres_vote, pid3) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n))\n\n`summarise()` has grouped output by 'pres_vote'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 4\n# Groups:   pres_vote [2]\n   pres_vote                    pid3            n   prop\n   &lt;fct&gt;                        &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;\n 1 Joe Biden (Democrat)         Democrat    17649 0.674 \n 2 Joe Biden (Democrat)         Republican    856 0.0327\n 3 Joe Biden (Democrat)         Independent  6601 0.252 \n 4 Joe Biden (Democrat)         Other         735 0.0281\n 5 Joe Biden (Democrat)         Not sure      347 0.0133\n 6 Donald J. Trump (Republican) Democrat      581 0.0328\n 7 Donald J. Trump (Republican) Republican  11164 0.631 \n 8 Donald J. Trump (Republican) Independent  4951 0.280 \n 9 Donald J. Trump (Republican) Other         774 0.0437\n10 Donald J. Trump (Republican) Not sure      232 0.0131\n\n\nFinally, if we want the proportions to be out of all rows of the data, we can tell summarize() to drop the grouping:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 10 × 4\n   pid3        pres_vote                        n    prop\n   &lt;fct&gt;       &lt;fct&gt;                        &lt;int&gt;   &lt;dbl&gt;\n 1 Democrat    Joe Biden (Democrat)         17649 0.402  \n 2 Democrat    Donald J. Trump (Republican)   581 0.0132 \n 3 Republican  Joe Biden (Democrat)           856 0.0195 \n 4 Republican  Donald J. Trump (Republican) 11164 0.254  \n 5 Independent Joe Biden (Democrat)          6601 0.150  \n 6 Independent Donald J. Trump (Republican)  4951 0.113  \n 7 Other       Joe Biden (Democrat)           735 0.0167 \n 8 Other       Donald J. Trump (Republican)   774 0.0176 \n 9 Not sure    Joe Biden (Democrat)           347 0.00791\n10 Not sure    Donald J. Trump (Republican)   232 0.00529\n\n\n\n\nCreate cross-tab proportion tables\nWe can pivot our proportion tables to create cross-tabs, but if we leave in the count variable, we get output that we don’t expect:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  pivot_wider(\n    names_from = pres_vote,\n    values_from = prop\n  )\n\n`summarise()` has grouped output by 'pid3'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 4\n# Groups:   pid3 [5]\n   pid3            n `Joe Biden (Democrat)` `Donald J. Trump (Republican)`\n   &lt;fct&gt;       &lt;int&gt;                  &lt;dbl&gt;                          &lt;dbl&gt;\n 1 Democrat    17649                 0.968                         NA     \n 2 Democrat      581                NA                              0.0319\n 3 Republican    856                 0.0712                        NA     \n 4 Republican  11164                NA                              0.929 \n 5 Independent  6601                 0.571                         NA     \n 6 Independent  4951                NA                              0.429 \n 7 Other         735                 0.487                         NA     \n 8 Other         774                NA                              0.513 \n 9 Not sure      347                 0.599                         NA     \n10 Not sure      232                NA                              0.401 \n\n\nTo combat this, we can either drop the count variable:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  select(-n) |&gt;  \n  pivot_wider(\n    names_from = pres_vote,\n    values_from = prop\n  )\n\n`summarise()` has grouped output by 'pid3'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n# Groups:   pid3 [5]\n  pid3        `Joe Biden (Democrat)` `Donald J. Trump (Republican)`\n  &lt;fct&gt;                        &lt;dbl&gt;                          &lt;dbl&gt;\n1 Democrat                    0.968                          0.0319\n2 Republican                  0.0712                         0.929 \n3 Independent                 0.571                          0.429 \n4 Other                       0.487                          0.513 \n5 Not sure                    0.599                          0.401 \n\n\nOr we can also be specific about what variables we want on the rows. We can use the id_cols argument to specify just the variables that will go on the rows:\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  pivot_wider(\n    id_cols = pid3,\n    names_from = pres_vote,\n    values_from = prop\n  )\n\n`summarise()` has grouped output by 'pid3'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n# Groups:   pid3 [5]\n  pid3        `Joe Biden (Democrat)` `Donald J. Trump (Republican)`\n  &lt;fct&gt;                        &lt;dbl&gt;                          &lt;dbl&gt;\n1 Democrat                    0.968                          0.0319\n2 Republican                  0.0712                         0.929 \n3 Independent                 0.571                          0.429 \n4 Other                       0.487                          0.513 \n5 Not sure                    0.599                          0.401 \n\n\n\n\nVisualizing a proportion table\nWe can visualize our proportion table using by mapping the grouping variable to the x axis and the non-grouped variable to the fill aesthetic. We also need to use the position = \"dodge\" argument to the geom_col() function so that the two columns in each group don’t overlap.\n\ncces_2020 |&gt;\n  filter(pres_vote %in% c(\"Joe Biden (Democrat)\",\n                          \"Donald J. Trump (Republican)\")) |&gt;\n  mutate(pres_vote = if_else(pres_vote == \"Joe Biden (Democrat)\",\n                             \"Biden\", \"Trump\")) |&gt;  \n  group_by(pid3, pres_vote) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  \nggplot(aes(x = pid3, y = prop, fill = pres_vote)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(Biden = \"steelblue1\", Trump = \"indianred1\"))\n\n`summarise()` has grouped output by 'pid3'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "resources/cheatsheet.html#bivariate-relationships-and-joins-week-6",
    "href": "resources/cheatsheet.html#bivariate-relationships-and-joins-week-6",
    "title": "Gov 50 Cheat Sheet",
    "section": "Bivariate Relationships and Joins (Week 6)",
    "text": "Bivariate Relationships and Joins (Week 6)\n\nCalculating a correlation\nYou can calcuate a correlation between two variables using the cor function:\n\ncor(news$national_politics, news$local_politics)\n\n[1] -0.05994132\n\n\nYou can also use the pipe and summarize:\n\nnews |&gt;\n  summarize(cor(national_politics, local_politics))\n\n# A tibble: 1 × 1\n  `cor(national_politics, local_politics)`\n                                     &lt;dbl&gt;\n1                                  -0.0599\n\n\nWhen there are missing values in one of the variables, the correlation will be NA:\n\ncor(covid_votes$one_dose_5plus_pct, covid_votes$dem_pct_2020)\n\n[1] NA\n\n\nIn that case, you can use the argument use = \"pairwise\" to remove the observations with missing values:\n\ncor(covid_votes$one_dose_5plus_pct, covid_votes$dem_pct_2020,\n    use = \"pairwise\")\n\n[1] 0.6664387\n\n\n\n\nWriting a function\nYou can create a function with the function() function (oof). The arguments in the function() call will become the arguments of your function. For example, if we want to create a z-score function that takes in a vector and coverts it to a z-score, we can create a function for this:\n\nz_score &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nHere, x is the argument that takes in the vector and it is used inside the function body to perform the task. Let’s call the function:\n\nz_score(x = c(1, 2, 3, 4, 5))\n\n[1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111\n\n\nWhat happens here is that R runs the code inside the function definition, replacing x with the vector we pass to x, which in this case is c(1, 2, 3, 4, 5).\n\n\nPivoting longer\nWe sometimes have data that has data in the column names that we would actually like to be in the data frame itself. When this happens, we need the opposite of pivot_wider which is called pivot_longer. For example, the mortality data has the years in the column names, but we want the rows to be country-years:\n\nmortality\n\n# A tibble: 217 × 52\n   country      country_code indicator `1972` `1973` `1974` `1975` `1976` `1977`\n   &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Aruba        ABW          Mortalit…   NA     NA     NA     NA     NA     NA  \n 2 Afghanistan  AFG          Mortalit…  291    285.   280.   274.   268    262. \n 3 Angola       AGO          Mortalit…   NA     NA     NA     NA     NA     NA  \n 4 Albania      ALB          Mortalit…   NA     NA     NA     NA     NA     NA  \n 5 Andorra      AND          Mortalit…   NA     NA     NA     NA     NA     NA  \n 6 United Arab… ARE          Mortalit…   80.1   72.6   65.7   59.4   53.6   48.3\n 7 Argentina    ARG          Mortalit…   69.7   68.2   66.1   63.3   59.8   55.7\n 8 Armenia      ARM          Mortalit…   NA     NA     NA     NA     87.1   83.6\n 9 American Sa… ASM          Mortalit…   NA     NA     NA     NA     NA     NA  \n10 Antigua and… ATG          Mortalit…   26.9   25.1   23.5   22.1   20.8   19.5\n# ℹ 207 more rows\n# ℹ 43 more variables: `1978` &lt;dbl&gt;, `1979` &lt;dbl&gt;, `1980` &lt;dbl&gt;, `1981` &lt;dbl&gt;,\n#   `1982` &lt;dbl&gt;, `1983` &lt;dbl&gt;, `1984` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1986` &lt;dbl&gt;,\n#   `1987` &lt;dbl&gt;, `1988` &lt;dbl&gt;, `1989` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1991` &lt;dbl&gt;,\n#   `1992` &lt;dbl&gt;, `1993` &lt;dbl&gt;, `1994` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1996` &lt;dbl&gt;,\n#   `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;, `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;,\n#   `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, …\n\n\nWe can use pivot_longer to move this information. To do so, we need to pass the variables we want to pivot to the cols argument, the name of the new variable to hold the colum names into names_to, and the name of the column to hold the pivoted data in values_to.\n\nmortality_long &lt;- mortality |&gt;\n  select(-indicator) |&gt;  \n  pivot_longer(\n    cols = `1972`:`2020`,\n    names_to = \"year\",\n    values_to = \"child_mortality\"\n  ) |&gt;\n  mutate(year = as.integer(year))\nmortality_long\n\n# A tibble: 10,633 × 4\n   country country_code  year child_mortality\n   &lt;chr&gt;   &lt;chr&gt;        &lt;int&gt;           &lt;dbl&gt;\n 1 Aruba   ABW           1972              NA\n 2 Aruba   ABW           1973              NA\n 3 Aruba   ABW           1974              NA\n 4 Aruba   ABW           1975              NA\n 5 Aruba   ABW           1976              NA\n 6 Aruba   ABW           1977              NA\n 7 Aruba   ABW           1978              NA\n 8 Aruba   ABW           1979              NA\n 9 Aruba   ABW           1980              NA\n10 Aruba   ABW           1981              NA\n# ℹ 10,623 more rows\n\n\nHere we mutate the years to be integers because they are characters when initially pivoted.\n\n\nPivoting longer with a column name prefix\nSometimes the column names have a prefix in front of the data that we want, like in the spotify data where the week columns are all weekX where X is the week number. We’d like to extract X:\n\nspotify\n\n# A tibble: 490 × 54\n   `Track Name`     Artist week1 week2 week3 week4 week5 week6 week7 week8 week9\n   &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 The Box          Roddy…     1     1     1     1     1     1     1     1     1\n 2 ROXANNE          Arizo…     2     4     5     4     4     4     6     7     9\n 3 Yummy            Justi…     3     6    17    17    17    24    15    32    NA\n 4 Circles          Post …     4     7     9    10     7    10    11    10    17\n 5 BOP              DaBaby     5     5     7     5    11    12    18    18    32\n 6 Falling          Trevo…     6     8    10     7     6     8    10    11    18\n 7 Dance Monkey     Tones…     7    13    13    12    12    13    17    13    21\n 8 Bandit (with Yo… Juice…     8    11    14    14    15    20    27    26    42\n 9 Futsal Shuffle … Lil U…     9     9    19    21    24    32    40    49    NA\n10 everything i wa… Billi…    10    17    28     9     8    11    14    17    29\n# ℹ 480 more rows\n# ℹ 43 more variables: week10 &lt;dbl&gt;, week11 &lt;dbl&gt;, week12 &lt;dbl&gt;, week13 &lt;dbl&gt;,\n#   week14 &lt;dbl&gt;, week15 &lt;dbl&gt;, week16 &lt;dbl&gt;, week17 &lt;dbl&gt;, week18 &lt;dbl&gt;,\n#   week19 &lt;dbl&gt;, week20 &lt;dbl&gt;, week21 &lt;dbl&gt;, week22 &lt;dbl&gt;, week23 &lt;dbl&gt;,\n#   week24 &lt;dbl&gt;, week25 &lt;dbl&gt;, week26 &lt;dbl&gt;, week27 &lt;dbl&gt;, week28 &lt;dbl&gt;,\n#   week29 &lt;dbl&gt;, week30 &lt;dbl&gt;, week31 &lt;dbl&gt;, week32 &lt;dbl&gt;, week33 &lt;dbl&gt;,\n#   week34 &lt;dbl&gt;, week35 &lt;dbl&gt;, week36 &lt;dbl&gt;, week37 &lt;dbl&gt;, week38 &lt;dbl&gt;, …\n\n\nWe can use the names_prefix arugment to do this:\n\nspotify |&gt;\n  pivot_longer(\n    cols = c(-`Track Name`, -Artist),\n    names_to = \"week_of_year\",\n    values_to = \"rank\",\n    names_prefix = \"week\"\n  ) |&gt;\n  mutate(\n    week_of_year = as.integer(week_of_year)\n  )\n\n# A tibble: 25,480 × 4\n   `Track Name` Artist      week_of_year  rank\n   &lt;chr&gt;        &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;\n 1 The Box      Roddy Ricch            1     1\n 2 The Box      Roddy Ricch            2     1\n 3 The Box      Roddy Ricch            3     1\n 4 The Box      Roddy Ricch            4     1\n 5 The Box      Roddy Ricch            5     1\n 6 The Box      Roddy Ricch            6     1\n 7 The Box      Roddy Ricch            7     1\n 8 The Box      Roddy Ricch            8     1\n 9 The Box      Roddy Ricch            9     1\n10 The Box      Roddy Ricch           10     1\n# ℹ 25,470 more rows\n\n\n\n\nMerging one data frame into another (joins)\nWe can take the variables from one data frame and put them into another data frame using the various join functions. If we think of the primary data frame as A and the donor data frame as B, then if we want to keep the rows of A the same and just import the columns of B, then we can use the left_join\n\ngapminder |&gt;\n  left_join(mortality_long)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 1,704 × 8\n   country continent  year lifeExp    pop gdpPercap country_code child_mortality\n   &lt;chr&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1 Afghan… Asia       1952    28.8 8.43e6      779. &lt;NA&gt;                     NA \n 2 Afghan… Asia       1957    30.3 9.24e6      821. &lt;NA&gt;                     NA \n 3 Afghan… Asia       1962    32.0 1.03e7      853. &lt;NA&gt;                     NA \n 4 Afghan… Asia       1967    34.0 1.15e7      836. &lt;NA&gt;                     NA \n 5 Afghan… Asia       1972    36.1 1.31e7      740. AFG                     291 \n 6 Afghan… Asia       1977    38.4 1.49e7      786. AFG                     262.\n 7 Afghan… Asia       1982    39.9 1.29e7      978. AFG                     231.\n 8 Afghan… Asia       1987    40.8 1.39e7      852. AFG                     198.\n 9 Afghan… Asia       1992    41.7 1.63e7      649. AFG                     166.\n10 Afghan… Asia       1997    41.8 2.22e7      635. AFG                     142.\n# ℹ 1,694 more rows\n\n\nBy default this keeps all rows of A and any rows in A that do not appear in B give missing values for the newly imported columns of B. If we want only the rows that are in both A and B, we can use inner_join\n\ngapminder |&gt;\n  inner_join(mortality_long)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 1,048 × 8\n   country continent  year lifeExp    pop gdpPercap country_code child_mortality\n   &lt;chr&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1 Afghan… Asia       1972    36.1 1.31e7      740. AFG                    291  \n 2 Afghan… Asia       1977    38.4 1.49e7      786. AFG                    262. \n 3 Afghan… Asia       1982    39.9 1.29e7      978. AFG                    231. \n 4 Afghan… Asia       1987    40.8 1.39e7      852. AFG                    198. \n 5 Afghan… Asia       1992    41.7 1.63e7      649. AFG                    166. \n 6 Afghan… Asia       1997    41.8 2.22e7      635. AFG                    142. \n 7 Afghan… Asia       2002    42.1 2.53e7      727. AFG                    121. \n 8 Afghan… Asia       2007    43.8 3.19e7      975. AFG                     99.9\n 9 Albania Europe     1972    67.7 2.26e6     3313. ALB                     NA  \n10 Albania Europe     1977    68.9 2.51e6     3533. ALB                     NA  \n# ℹ 1,038 more rows\n\n\n\n\nSpecifying the key variables for a join\nBy default, the join functions will merge data frames A and B based on the columns that have the same name in both data frames. This sometimes isn’t what we want. For example, using the nycflights13 package, we might want to merge the flights and planes data frames. Both of these data sets have a year variable but they mean different things in the two data frames (the year of the flights vs the year the plane was manufactured):\n\nlibrary(nycflights13)\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nplanes\n\n# A tibble: 3,322 × 9\n   tailnum  year type              manufacturer model engines seats speed engine\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n 1 N10156   2004 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 2 N102UW   1998 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 3 N103US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 4 N104UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 5 N10575   2002 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 6 N105UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 7 N107US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 8 N108UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 9 N109UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n10 N110UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n# ℹ 3,312 more rows\n\n\nIn this case, we want to specify what variable we want to do the merge with. Here, we want to merge based on tailnum:\n\nflights |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  select(year.x, tailnum, origin, dest, carrier, type, engine)\n\n# A tibble: 336,776 × 7\n   year.x tailnum origin dest  carrier type                    engine   \n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                   &lt;chr&gt;    \n 1   2013 N14228  EWR    IAH   UA      Fixed wing multi engine Turbo-fan\n 2   2013 N24211  LGA    IAH   UA      Fixed wing multi engine Turbo-fan\n 3   2013 N619AA  JFK    MIA   AA      Fixed wing multi engine Turbo-fan\n 4   2013 N804JB  JFK    BQN   B6      Fixed wing multi engine Turbo-fan\n 5   2013 N668DN  LGA    ATL   DL      Fixed wing multi engine Turbo-fan\n 6   2013 N39463  EWR    ORD   UA      Fixed wing multi engine Turbo-fan\n 7   2013 N516JB  EWR    FLL   B6      Fixed wing multi engine Turbo-fan\n 8   2013 N829AS  LGA    IAD   EV      Fixed wing multi engine Turbo-fan\n 9   2013 N593JB  JFK    MCO   B6      Fixed wing multi engine Turbo-fan\n10   2013 N3ALAA  LGA    ORD   AA      &lt;NA&gt;                    &lt;NA&gt;     \n# ℹ 336,766 more rows\n\n\nBecause both data sets had a year variable, the join function renames them to year.x in the first data frame and year.y for the second. We could rename them before merging to solve this problem."
  },
  {
    "objectID": "resources/cheatsheet.html#regression-week-8",
    "href": "resources/cheatsheet.html#regression-week-8",
    "title": "Gov 50 Cheat Sheet",
    "section": "Regression (Week 8)",
    "text": "Regression (Week 8)\n\nAdd binned means to a scatterplot\nYou can add binned means of the y-axis variable in a scatterplot using the stat_summary_bin(fun = \"mean\") function with your ggplot call:\n\nggplot(health, aes(x = steps_lag, y = weight)) +\n  geom_point(color = \"steelblue1\", alpha = 0.25) +\n  labs(\n    x = \"Steps on day prior (in 1000s)\",\n    y = \"Weight\",\n    title = \"Weight and Steps\"\n  ) +\n  stat_summary_bin(fun = \"mean\", color = \"indianred1\", size = 3,\n                   geom = \"point\", binwidth = 1)\n\nWarning: Removed 1 rows containing non-finite values (`stat_summary_bin()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nRunning a regression (estimating the line of best fit)\nWe can estimate the line of best fit using the lm function:\n\nfit &lt;- lm(weight ~ steps, data = health)\nfit\n\n\nCall:\nlm(formula = weight ~ steps, data = health)\n\nCoefficients:\n(Intercept)        steps  \n   170.5435      -0.2206  \n\n\n\n\nExtracting the vector of estimated regression coefficients\n\ncoef(fit)\n\n(Intercept)       steps \n170.5434834  -0.2205511 \n\n\n\n\nObtaining unit-level statistics about the regression\nThe broom package has a number of functions that will allow you to extract information about the regression output from lm(). For example, if you want information about each unit such as the predicted/fitted value and the residual, you can use augment:\n\nlibrary(broom)\naugment(fit)\n\n# A tibble: 644 × 8\n   weight steps .fitted .resid    .hat .sigma   .cooksd .std.resid\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1   168  17.5     167.  1.31  0.00368   4.69 0.000146       0.281\n 2   169. 18.4     166.  2.62  0.00463   4.69 0.000730       0.560\n 3   168  19.6     166.  1.78  0.00609   4.69 0.000444       0.381\n 4   167. 10.4     168. -1.05  0.00217   4.69 0.0000547     -0.224\n 5   168. 18.7     166.  1.78  0.00496   4.69 0.000362       0.381\n 6   168.  9.14    169. -0.728 0.00296   4.69 0.0000359     -0.156\n 7   166.  8.69    169. -2.33  0.00331   4.69 0.000412      -0.498\n 8   168. 13.8     167.  0.911 0.00165   4.69 0.0000313      0.195\n 9   169  11.9     168.  1.08  0.00165   4.69 0.0000439      0.231\n10   169. 24.6     165.  4.19  0.0155    4.68 0.00639        0.902\n# ℹ 634 more rows\n\n\n\n\nObtaining overall information about the model fitted\nThe glance function will give you some high-level information about the regression, including the \\(R^2\\):\n\nglance(fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0332        0.0317  4.68      22.1 0.00000323     1 -1907. 3820. 3834.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nYou can also access the \\(R^{2}\\) using the summary() function:\n\nsummary(fit)$r.squared\n\n[1] 0.03322261\n\n\n\n\nRunning a multiple regression\nYou can fit a regression with multiple predictors by putting a + between them on the right-hand side of the formula:\n\nmult.fit &lt;- lm(seat_change ~ approval + rdi_change,\n               data = midterms)\nmult.fit\n\n\nCall:\nlm(formula = seat_change ~ approval + rdi_change, data = midterms)\n\nCoefficients:\n(Intercept)     approval   rdi_change  \n   -117.226        1.526        3.217"
  },
  {
    "objectID": "resources/office-hours.html",
    "href": "resources/office-hours.html",
    "title": "Office House Schedule",
    "section": "",
    "text": "For the latest schedule, including changes to the schedule, please check Ed or the Gov 50 Google calendar.\n\n\n\n\n\nName\n\n\nDay\n\n\nStart Time\n\n\nEnd Time\n\n\nLocation\n\n\n\n\n\n\nAhmet Akbiyik\n\n\nThursday\n\n\n7:00 PM\n\n\n8:00 PM\n\n\nDunster House Dining Hall\n\n\n\n\nAda Cruz\n\n\nFriday\n\n\n10:00 AM\n\n\n12:00 PM\n\n\nEliot Dhall\n\n\n\n\nEthan Miles\n\n\nFriday\n\n\n2:00 PM\n\n\n3:00 PM\n\n\nCGIS K Café\n\n\n\n\nJerry Min\n\n\nFriday\n\n\n5:00:00 PM\n\n\n6:00 PM\n\n\nCGIS K Café\n\n\n\n\nShriya Yarlagadda\n\n\nSunday\n\n\n8:00 PM\n\n\n9:00 PM\n\n\nLowell DHall\n\n\n\n\nAndy Wang\n\n\nMonday\n\n\n3:00 PM\n\n\n4:00 PM\n\n\nFairfax Common Room\n\n\n\n\nKatherine Jackson\n\n\nMonday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nPfoho Dhall - 2nd Floor\n\n\n\n\nJulio Solis\n\n\nTuesday\n\n\n1:15 PM\n\n\n2:15 PM\n\n\nEmerson Hall\n\n\n\n\nJames Jolin\n\n\nTuesday\n\n\n4:00 PM\n\n\n5:00 PM\n\n\nCGIS K Café\n\n\n\n\nPranav Moudgalya\n\n\nTuesday\n\n\n8:00 PM\n\n\n9:00 PM\n\n\nLeverett Dining Hall\n\n\n\n\nChris Shen\n\n\nWednesday\n\n\n9:00 PM\n\n\n10:00 PM\n\n\nQuincy Dining Hall"
  },
  {
    "objectID": "resources/study-halls.html",
    "href": "resources/study-halls.html",
    "title": "Study Hall Schedule",
    "section": "",
    "text": "For the latest schedule, including changes to the schedule, please check Ed or the Gov 50 Google calendar.\n\n\n\n\n\nName\n\n\nday\n\n\nStart Time\n\n\nEnd Time\n\n\nLocation\n\n\n\n\n\n\nKate De Groote\n\n\nFriday\n\n\n1:00 PM\n\n\n3:00 PM\n\n\nLowell Dining Hall\n\n\n\n\nIsa Peña\n\n\nFriday\n\n\n2:00 PM\n\n\n4:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nJulia Poulson\n\n\nFriday\n\n\n4:00 PM\n\n\n6:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nEric Forteza\n\n\nSunday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nGowri Rangu\n\n\nSunday\n\n\n7:30 PM\n\n\n9:30 PM\n\n\nLowell Dining Hall\n\n\n\n\nMitja Bof\n\n\nSunday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nWinthrop Dining Hall\n\n\n\n\nJason Wang\n\n\nSunday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nLeverett Dining Hall\n\n\n\n\nIsa Peña\n\n\nMonday\n\n\n8:00 AM\n\n\n10:00 AM\n\n\nCabot Dining Hall\n\n\n\n\nRyan McCarthy\n\n\nMonday\n\n\n9:00 AM\n\n\n11:00 AM\n\n\nEliot Dining Hall\n\n\n\n\nAlex Heuss\n\n\nMonday\n\n\n4:30 PM\n\n\n6:30 PM\n\n\nCGIS Cafe\n\n\n\n\nFrank T Berrios\n\n\nMonday\n\n\n5:00 PM\n\n\n7:00 PM\n\n\nCabot Dining Hall\n\n\n\n\nChris Mesfin\n\n\nMonday\n\n\n7:00 PM\n\n\n9:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nAzeez Richardson\n\n\nMonday\n\n\n7:00 PM\n\n\n9:00 PM\n\n\nDunster Dining Hall\n\n\n\n\nVivian Nguyen\n\n\nMonday\n\n\n8:00 PM\n\n\n10:00 PM\n\n\nMather Dining Hall\n\n\n\n\nAlina Esanu\n\n\nTuesday\n\n\n9:00 AM\n\n\n11:00 AM\n\n\nQuincy Dining Hall\n\n\n\n\nZachary Mecca\n\n\nTuesday\n\n\n9:30 AM\n\n\n11:30 AM\n\n\nEliot Dining Hall\n\n\n\n\nTracy Jiang\n\n\nTuesday\n\n\n3:00 PM\n\n\n5:00 PM\n\n\nAdam Dining Hall\n\n\n\n\nCameron Snowden\n\n\nTuesday\n\n\n4:00 PM\n\n\n6:00 PM\n\n\nPfoho Dining Hall\n\n\n\n\nGroup Study Hall\n\n\nTuesday\n\n\n6:00 PM\n\n\n11:00 PM\n\n\nCGIS Cafe (First Floor)\n\n\n\n\nGowri Rangu\n\n\nWednesday\n\n\n2:00 PM\n\n\n4:00 PM\n\n\nLowell Dining Hall\n\n\n\n\nGroup Study Hall\n\n\nWednesday\n\n\n6:00 PM\n\n\n11:00 PM\n\n\nCGIS Cafe (First Floor)"
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Course Staff",
    "section": "",
    "text": "Prof. Matt Blackwell\n   CGIS Knafel 305\n   mblackwell@gov.harvard.edu\n   matt_blackwell\n   Schedule an appointment\n\n\n\n\n\n\n\n\n\n\n\n\n   Laura Royden\n   lroyden@g.harvard.edu\n\n\n\n\n\n\n\n\n\n\n\n\n   Ahmet Akbiyik\n   ahmetakbiyik@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ada Cruz ’24\n   acruz@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Katherine Jackson ’25\n   katherinejackson@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   James René Jolin ’24\n   jamesjolin@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ethan Miles\n   ethanmiles@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Jerry Min\n   jiemin@fas.harvard.edu\n\n\n\n\n\n\n\n\n\n   Pranav Moudgalya ’26\n   pmoudgalya@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Christopher Shen ’26\n   christopher_shen@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Julio Solis Arce\n   jsolisarce@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Andy Wang ’23\n   azwang@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Shriya Yarlagadda ’25\n   syarlagadda1@college.harvard.edu\n\n\n\n\n\n\n\n\n\n\n\n\n   Frank T. Berrios ’25\n\n\n\n\n\n\n\n\n\n   Mitja Bof ’26\n\n\n\n\n\n\n\n\n\n   Kate De Groote ’24\n\n\n\n\n\n\n\n\n\n   Alina Esanu ’24\n\n\n\n\n\n\n\n\n\n   Eric Forteza ’24\n\n\n\n\n\n\n\n\n\n   Alex Heuss ’26\n\n\n\n\n\n\n\n\n\n   Tracy Jiang ’24\n\n\n\n\n\n\n\n\n\n   Ryan McCarthy ’24\n\n\n\n\n\n\n\n\n\n   Zachary Mecca ’24\n\n\n\n\n\n\n\n\n\n   Chris Mesfin ’26\n\n\n\n\n\n\n\n\n\n   Vivian Nguyen ’24\n\n\n\n\n\n\n\n\n\n   Isa Peña ’24\n\n\n\n\n\n\n\n\n\n   Julia Poulson ’26\n\n\n\n\n\n\n\n\n\n   Gowri Rangu ’26\n\n\n\n\n\n\n\n\n\n   Azeez Richardson ’25\n\n\n\n\n\n\n\n\n\n   Cameron Snowden ’25\n\n\n\n\n\n\n\n\n\n   Jason Wang ’24"
  },
  {
    "objectID": "staff.html#head-instructor",
    "href": "staff.html#head-instructor",
    "title": "Course Staff",
    "section": "",
    "text": "Prof. Matt Blackwell\n   CGIS Knafel 305\n   mblackwell@gov.harvard.edu\n   matt_blackwell\n   Schedule an appointment"
  },
  {
    "objectID": "staff.html#head-teaching-fellow",
    "href": "staff.html#head-teaching-fellow",
    "title": "Course Staff",
    "section": "",
    "text": "Laura Royden\n   lroyden@g.harvard.edu"
  },
  {
    "objectID": "staff.html#teaching-fellows",
    "href": "staff.html#teaching-fellows",
    "title": "Course Staff",
    "section": "",
    "text": "Ahmet Akbiyik\n   ahmetakbiyik@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ada Cruz ’24\n   acruz@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Katherine Jackson ’25\n   katherinejackson@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   James René Jolin ’24\n   jamesjolin@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Ethan Miles\n   ethanmiles@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Jerry Min\n   jiemin@fas.harvard.edu\n\n\n\n\n\n\n\n\n\n   Pranav Moudgalya ’26\n   pmoudgalya@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Christopher Shen ’26\n   christopher_shen@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Julio Solis Arce\n   jsolisarce@g.harvard.edu\n\n\n\n\n\n\n\n\n\n   Andy Wang ’23\n   azwang@college.harvard.edu\n\n\n\n\n\n\n\n\n\n   Shriya Yarlagadda ’25\n   syarlagadda1@college.harvard.edu"
  },
  {
    "objectID": "staff.html#course-assistants",
    "href": "staff.html#course-assistants",
    "title": "Course Staff",
    "section": "",
    "text": "Frank T. Berrios ’25\n\n\n\n\n\n\n\n\n\n   Mitja Bof ’26\n\n\n\n\n\n\n\n\n\n   Kate De Groote ’24\n\n\n\n\n\n\n\n\n\n   Alina Esanu ’24\n\n\n\n\n\n\n\n\n\n   Eric Forteza ’24\n\n\n\n\n\n\n\n\n\n   Alex Heuss ’26\n\n\n\n\n\n\n\n\n\n   Tracy Jiang ’24\n\n\n\n\n\n\n\n\n\n   Ryan McCarthy ’24\n\n\n\n\n\n\n\n\n\n   Zachary Mecca ’24\n\n\n\n\n\n\n\n\n\n   Chris Mesfin ’26\n\n\n\n\n\n\n\n\n\n   Vivian Nguyen ’24\n\n\n\n\n\n\n\n\n\n   Isa Peña ’24\n\n\n\n\n\n\n\n\n\n   Julia Poulson ’26\n\n\n\n\n\n\n\n\n\n   Gowri Rangu ’26\n\n\n\n\n\n\n\n\n\n   Azeez Richardson ’25\n\n\n\n\n\n\n\n\n\n   Cameron Snowden ’25\n\n\n\n\n\n\n\n\n\n   Jason Wang ’24"
  }
]