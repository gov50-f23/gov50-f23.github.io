---
title: "Gov 50 Cheat Sheet"
slug: cheatsheet
---

```{r}
#| label: packages
#| message: false
library(tidyverse)
library(gov50data)
library(gapminder)
```


## R Basics (Week 1)

### Creating a vector 

You can create a vector using the `c` function:

```{r}
## Any R code that begins with the # character is a comment
## Comments are ignored by R

my_numbers <- c(4, 8, 15, 16, 23, 42) # Anything after # is also a
# comment
my_numbers
```


### Installing and loading a package

You can install a package with the `install.packages` function, passing the name of the package to be installed as a string (that is, in quotes):

```{r}
#| eval = FALSE
install.packages("ggplot2")
```

You can load a package into the R environment by calling `library()` with the name of package without quotes. You should only have one package per library call. 

```{r}
library(ggplot2)
```

### Calling functions from specific packages

We can also use the `mypackage::` prefix to access package functions without loading:

```{r}
knitr::kable(head(mtcars))
```

## Data Visualization (Week 2)

### Scatter plot

You can produce a scatter plot with using the `x` and `y` aesthetics along with the `geom_point()` function. 

```{r}
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point()
```


### Smoothed curves 

You can add a smoothed curve that summarizes the relationship between two variables with the `geom_smooth()` function. By default, it uses a  loess smoother to estimated the conditional mean of the y-axis variable as a function of the x-axis variable. 

```{r}
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point() + geom_smooth()
```


### Adding a regression line 

`geom_smooth` can also add a regression line by setting the argument `method = "lm"` and we can turn off the shaded regions around the line with `se = FALSE`

```{r }
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE)
```

### Changing the scale of the axes

If we want the scale of the x-axis to be logged to stretch out the data we can use the `scale_x_log10()`:

```{r }
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  scale_x_log10()
```

### Adding informative labels to a plot

Use the `labs()` to add informative labels to the plot:

```{r}
#| label: labels
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +
  scale_x_log10() +
  labs(x = "Population Density",
       y = "Percent of County Below Poverty Line",
       title = "Poverty and Population Density",
       subtitle = "Among Counties in the Midwest",
       source = "US Census, 2000")
```

### Mapping aesthetics to variables

If you would like to map an aesthetic to a variable for all geoms in the plot, you can put it in the `aes` call in the `ggplot()` function:

```{r }
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty,
                     color = state,
                     fill = state)) +
  geom_point() +
  geom_smooth() +
  scale_x_log10()
```


### Mapping aesthetics for a single geom

You can also map aesthetics for a specific geom using the `mapping` argument to that function:


```{r}
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point(mapping = aes(color = state)) +
  geom_smooth(color = "black") +
  scale_x_log10()
```

### Setting the aesthetics for all observations

If you would like to set the color or size or shape of a geom for all data points (that is, not mapped to any variables), be sure to set these outside of `aes()`:

```{r }
ggplot(data = midwest,
       mapping = aes(x = popdensity,
                     y = percbelowpoverty)) +
  geom_point(color = "purple") +
  geom_smooth() +
  scale_x_log10()
```

### Histograms

```{r}
#| label: histogram
ggplot(data = midwest,
       mapping = aes(x = percbelowpoverty)) +
  geom_histogram()
```


## Data Wrangling (week 2-3)

### Subsetting a data frame

Use the `filter()` function from the `dplyr` package to subset a data frame.


```{r}
library(gov50data)

news |>
  filter(weekday == "Tue")
```


You can filter based on multiple conditions to subset to the rows that meet all conditions:


```{r}
news |>
  filter(weekday == "Tue",
         affiliation == "FOX")
```


You can use the `|` operator to match one of two conditions ("OR" rather than "AND"):

```{r}
news |>
  filter(affiliation == "FOX" | affiliation == "ABC")
```

To test if a variable is one of several possible values, you can also use the `%in%` command:

```{r}
news |>
  filter(weekday %in% c("Mon", "Fri"))
```

If you want to subset to a set of specific row numbers, you can use the `slice` function:

```{r}
## subset to the first 5 rows
news |>
  slice(1:5)
```


Here the `1:5` syntax tells R to produce a vector that starts at 1 and ends at 5, incrementing by 1:

```{r}
1:5
```

### Filtering to the largest/smallest values of a variable

To subset to the rows that have the largest or smallest values of a given variable, use the `slice_max` and `slice_max` functions. For the largest  values, use `slice_max` and use the `n` argument to specify how many rows you want:

```{r}
news |>
  slice_max(ideology, n = 5)
```

To get lowest values, use `slice_min`:

```{r}
news |>
  slice_min(ideology, n = 5)
```


### Sorting rows by a variable

You can sort the rows of a data set using the `arrange()` function. By default, this will sort the rows from smallest to largest. 

```{r}
news |>
  arrange(ideology)
```

If you would like to sort the rows from largest to smallest (descending order), you can wrap the variable name with `desc()`:

```{r}
news |>
  arrange(desc(ideology))
```


### Selecting/subsetting the columns

You can subset the data to only certain columns using the `select()` command:

```{r}
#| label: "select"
news |>
  select(callsign, date, ideology)
```

If you want to select a range of columns from, say, `callsign` to `ideology`, you can use the `:` operator:

```{r}
#| label: "select-range"
news |>
  select(callsign:ideology)
```

You can remove a variable from the data set by using the minus sign `-` in front of it:

```{r}
#| label: "select-minus"
news |>
  select(-callsign)
```

You can also drop several variables using the `c()` function or the `(a:b)` syntax:

```{r}
news |>
  select(-c(callsign, date, ideology))

news |>
  select(-(callsign:ideology))
```


You can also select columns based on matching patterns in the names with functions like `starts_with()` or `ends_with()`:

```{r}
#| label: "ends_with"

news |>
  select(ends_with("politics"))
```


This code finds all variables with column names that end with the string "politics". See the [help page for `select()`](https://dplyr.tidyverse.org/reference/select.html) for more information on different ways to select. 


### Renaming a variable 

You can rename a variable useing the function `rename(new_name = old_name)`:

```{r}
news |>
  rename(call_sign = callsign)
```


### Creating new variables 

You can create new variables that are functions of old variables using the `mutate()` function:


```{r}
news |> mutate(
  national_local_diff = national_politics - local_politics) |>
  select(callsign, date, national_politics, local_politics,
         national_local_diff)
```


### Creating new variables based on yes/no conditions

If you want to create a new variable that can take on two values based on a logical conditional, you should use the `if_else()` function inside of `mutate()`. For instance, if we want to create a more nicely labeled version of the `sinclair2017` variable (which is 0/1), we could do:

```{r}
news |>
  mutate(Ownership = if_else(sinclair2017 == 1,
                             "Acquired by Sinclair",
                             "Not Acquired")) |>
  select(callsign, affiliation, date, sinclair2017, Ownership)
```


### Summarizing a variable 

You can calculate summaries of variables in the data set using the `summarize()` function.


```{r}
news |>
  summarize(
    avg_ideology = mean(ideology),
    sd_ideology = sd(ideology),
    median_ideology = median(ideology)
  )
```



### Summarizing variables by groups of rows

By default, `summarize()` calculates the summaries of variables for all rows in the data frame. You can also calculate these summaries within groups of rows defined by another variable in the data frame using the `group_by()` function before summarizing. 


```{r}
news |>
  group_by(month) |>
  summarize(
    avg_ideology = mean(ideology),
    sd_ideology = sd(ideology),
    median_ideology = median(ideology)
  )
```

Here, the `summarize()` function breaks apart the original data into smaller data frames for each month and applies the summary functions to those, then combines everything into one tibble. 


### Summarizing by multiple variables

You can group by multiple variables and `summarize()` will create groups based on every combination of each variable: 

```{r}
news |>
  group_by(sinclair2017, post) |>
  summarize(
    avg_ideology = mean(ideology)
  )
```

You'll notice the message that `summarize()` sends after using to let us know that resulting tibble is grouped by `sinclair2017`. By default, `summarize()` drops the last group you provided in `group_by` (`post` in this case).  This isn't an error message, it's just letting us know some helpful information. If you want to avoid this messaging displaying, you need to specify what grouping you want after using the `.groups` argument:


```{r}
news |>
  group_by(sinclair2017, post) |>
  summarize(
    avg_ideology = mean(ideology),
    .groups = "drop_last"
  )
```


### Summarizing across many variables


If you want to apply the same summary to multiple variables, you can use the `across(vars, fun)` function, where `vars` is a vector of variable names (specified like with `select()`) and `fun` is a summary function to apply to those variables.

```{r}
news |>
  group_by(sinclair2017, post) |>
  summarize(
    across(c(ideology, national_politics), mean)
  )
```

As with `select()`, you can use the `:` operator to select a range of variables 

```{r}
news |>
  group_by(sinclair2017, post) |>
  summarize(
    across(ideology:local_politics, mean)
  )
```


### Table of counts of a categorical variable

There are two way to produce a table of counts of each category of a variable. The first is to use `group_by` and `summarize` along with the summary function `n()`, which returns the numbers of rows in each grouping (that is, each combination of the grouping variables):


```{r}
news |>
  group_by(affiliation) |>
  summarize(n = n())
```

A simpler way to acheive the same outcome is to use the `count()` function, which implements these two steps:


```{r}
news |>
  count(affiliation)
```




### Producing nicely formatted tables with `kable()`

You can take any tibble in R and convert it into a more readable output by passing it to `knitr::kable()`. In our homework, generally, we will save the tibble as an object and then pass it to this function. 

```{r}
month_summary <- news |>
  group_by(month) |>
  summarize(
    avg_ideology = mean(ideology),
    sd_ideology = sd(ideology)
  )

knitr::kable(month_summary)
```


You can add informative column names to the table using the `col.names` argument. 

```{r}
knitr::kable(
  month_summary,
  col.names = c("Month", "Average Ideology", "SD of Ideology")
)
```

Finally, we can round the numbers in the table to be a bit nicer using the `digits` argument. This will tell `kable()` how many significant digiits to show. 


```{r}
knitr::kable(
  month_summary,
  col.names = c("Month", "Average Ideology", "SD of Ideology"),
  digits = 3
)
```


### Barplots of counts

You can visualize counts of a variable using a barplot:


```{r}
news |>
  ggplot(mapping = aes(x = affiliation)) +
  geom_bar()
```


### Barplots of other summaries 

We can use barplots to visualize  other grouped summaries like means, but we need to use the `geom_col()` geom instead and specify the variable you want to be the height of the bars .


```{r}
news |>
  group_by(affiliation) |>
  summarize(
    avg_ideology = mean(ideology)
  ) |>
  ggplot(mapping = aes(x = affiliation, y = avg_ideology)) +
  geom_col()
```


### Reordering/sorting barplot axes 

Often we want to sort the barplot axes to be in the order of the variable of interest so we can quickly compare them. We can use the `fct_reorder(group_var, ordering_var)` function to do this where the `group_var` is the grouping variable that is going on the axes and the `ordering_var` is the variable that we will sort the groups on. 

```{r}
news |>
  group_by(affiliation) |>
  summarize(
    avg_ideology = mean(ideology)
  ) |>
  ggplot(mapping = aes(x = fct_reorder(affiliation, avg_ideology),
                       y = avg_ideology)) +
  geom_col()

```


### Coloring barplots by another variable 

You can color the barplots by a another variable using the `fill` aesthetic:

```{r}
news |>
  group_by(callsign, affiliation) |>
  summarize(
    avg_ideology = mean(ideology)
  ) |>
  slice_max(avg_ideology, n = 10) |>
  ggplot(mapping = aes(y = fct_reorder(callsign, avg_ideology),
                       x = avg_ideology)) +
  geom_col(mapping = aes(fill = affiliation))
```


### Creating logical vectors

You can create logical variables in your tibbles using mutate:

```{r}
news |>
  mutate(
    right_leaning = ideology > 0,
    fall = month == "Sep" | month == "Oct" | month == "Nov",
    .keep = "used"
)
```


The `.keep = "used"` argument here tells mutate to only return the variables created and any variables used to create them. We're using it here for display purposes. 

You can filter based on these logical variables. In particular, if we want to subset to rows where both `right_leaning` and `fall` were `TRUE` we could do the following filter:

```{r}
news |>
  mutate(
    right_leaning = ideology > 0,
    fall = month == "Sep" | month == "Oct" | month == "Nov",
    .keep = "used"
  ) |>
  filter(right_leaning & fall)
```

### Using `!` to negate logicals

Any time you place the exclamation point in front of a logical, it will turn any `TRUE` into a `FALSE` and vice versa. For instance, if we wanted left leaning days in the fall, we could used


```{r}
news |>
  mutate(
    right_leaning = ideology > 0,
    fall = month == "Sep" | month == "Oct" | month == "Nov",
    .keep = "used"
  ) |>
  filter(!right_leaning & fall)
```

Or if we wanted to subset to any combination **except** right leaning and fall, we could negate the AND statement using parentheses:


```{r}
news |>
  mutate(
    right_leaning = ideology > 0,
    fall = month == "Sep" | month == "Oct" | month == "Nov",
    .keep = "used"
  ) |>
  filter(!(right_leaning & fall))
```

This is often used in combination with `%in%` to acheive a "not in" logical:

```{r}
news |>
  filter(!(affiliation %in% c("FOX", "ABC")))
```


### Grouped summaries with `any()` and `all()` 

Once you group a tibble, you can summarize logicals within groups using two commands. `any()` will return `TRUE` if a logical is `TRUE` for any row in a group and `FALSE` otherwise. `all()` will return `TRUE` when the logical inside it is `TRUE` for all rows in a group and `FALSE` otherwise. 


```{r}
news |>
  group_by(callsign) |>
  summarize(
    any_liberal = any(ideology < 0),
    all_local = all(national_politics < local_politics)
  )
```


### Converting logicals


When passed to `sum()` or `mean()`, `TRUE` is converted to 1 and `FALSE` is converted to 0. This means that `sum()` will return the number of `TRUE` values in a vector and `mean()` will return the proportion of `TRUE` values. 


```{r}
sum(c(TRUE, FALSE, TRUE, FALSE))

mean(c(TRUE, FALSE, TRUE, FALSE))
```

### Grouped logical summaries with sums and means

After grouping, you can summarize using either the `sum()` or `mean()` function on a logical. In this case, the `sum()` will give you the number of time the statement is `TRUE` within a group, and `mean()` will give you the proportion of rows that are `TRUE` within a group. 


```{r}
news |>
  group_by(callsign) |>
  summarize(
    prop_liberal = mean(ideology < 0),
    num_local_bigger = sum(national_politics < local_politics)
  )
```


## Causality (Week 4)

For this week, we'll use the data from the transphobia randomized experiment 

```{r}
trans
```

### Calculating a difference in means by filtering

You can calculate a difference in means by creating separate summaries of the treatment and control arms of an experiment or observational study and then taking the differences between them:


```{r}
treat_mean <- trans |>
  filter(treat_ind == 1) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post))
treat_mean


control_mean <- trans |>
  filter(treat_ind == 0) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post))
control_mean

diff_in_means <- treat_mean - control_mean
diff_in_means
```


### Pivoting a tibble to make a cross-tab

When grouping by two variables, we sometimes would like the values of one variable to be in the rows and values of the other variable to be in the columns. But `group_by()` and `summarize()` by default puts each combination of values for the two variables into rows. We can use the `pivot_wider` function to move one of the variables (specified with the `names_from` argument) to the columns. The `values_from` argument tells R what variable should it use to fil in the data in the resulting table. 

So if we wanted to get counts of race (rows) by treatment group (columns), we could start with getting the counts:

```{r}
trans |>
  group_by(treat_ind, racename) |>
  summarize(n = n())
```

Then we can pivot the treatment group to the columns:

```{r}
trans |>
  group_by(treat_ind, racename) |>
  summarize(n = n()) |>
  pivot_wider(
    names_from = treat_ind,
    values_from = n
  )
```


### Calculating difference in means by pivoting

Now that we know pivoting, we can use this to calculate difference in means instead of the filter approach. We first calculate the means by treatment group, pivot the rows to columns, and finally use `mutate()` to create the difference between the means.


```{r}
trans |>
  group_by(treat_ind) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post)) |>
  pivot_wider(
    names_from = treat_ind,
    values_from = nondiscrim_mean
  ) |>
  mutate(
    diff_in_means = `1` - `0`
  ) 
```


### Creating new labels for variables to make nicer tables

Above we had to use the backticks to refer to the 1/0 variable names when calculating the differences in means. We can create slightly nicer output by relabeling the `treat_ind` variable using `if_else`:

```{r}
trans |>
  mutate(
    treat_ind = if_else(treat_ind == 1, "Treated", "Control")
  ) |>  
  group_by(treat_ind) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post)) |>
  pivot_wider(
    names_from = treat_ind,
    values_from = nondiscrim_mean
  ) |>
  mutate(
    diff_in_means = Treated - Control
  ) 
```


### Calculating difference in means by two-level grouping variable

If we have a two-level grouping variable like `democrat`, we can create nice labels for that variable and just add it to the `group_by` call to get the difference in means within levels of this variable:

```{r}
trans |>
  mutate(
    treat_ind = if_else(treat_ind == 1, "Treated", "Control"),
    party = if_else(democrat == 1, "Democrat", "Non-Democrat")
  ) |>
  group_by(treat_ind, party) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post)) |>
  pivot_wider(
    names_from = treat_ind,
    values_from = nondiscrim_mean
  ) |>  
  mutate(
    diff_in_means = Treated - Control
  )
```


### Creating grouping variables with more than 2 levels

You can use [`case_when()`](https://dplyr.tidyverse.org/reference/case_when.html) to create a variable that can take on more than 2 values based on conditions of a variable in the tibble:

```{r}
trans |>
  mutate(
    age_group = case_when(
      age < 25 ~ "Under 25",
      age >=25 & age < 65 ~ "Bewteen 25 and 65",
      age >= 65 ~ "Older than 65"
    )
  ) |>
  select(age, age_group)
```


### Calculating difference in mean by grouping variable with more than 2 levels


```{r}
trans |>
  mutate(
    treat_ind = if_else(treat_ind == 1, "Treated", "Control"),
    age_group = case_when(
      age < 25 ~ "Under 25",
      age >=25 & age < 65 ~ "Bewteen 25 and 65",
      age >= 65 ~ "Older than 65"
    )
  ) |>
  group_by(treat_ind, age_group) |>
  summarize(nondiscrim_mean = mean(nondiscrim_post)) |>
  pivot_wider(
    names_from = treat_ind,
    values_from = nondiscrim_mean
  ) |>  
  mutate(
    diff_in_means = Treated - Control
  )
```


### Calculating before and after difference in means


When we're estimating causal effects using a before and after design, we can usually create a variable that is the change in the outcome over time for each row and then take the mean of that. For example, in the newspapers data we have the vote for Labour outcome before the treatment (1992) and after (1997):


```{r}
newspapers |>
  filter(to_labour == 1) |>  
  mutate(
    vote_change = vote_lab_97 - vote_lab_92
  ) |>  
  summarize(avg_change = mean(vote_change))
```


### Calculating difference-in-differences estimator

When you want to use the difference in differences estimator, you simply need to take the difference between the average changes over time in the treatment group (`to_labour == 1`) and the control group (`to_labour == 0`). We can do this by adding a `group_by` step:


```{r}
newspapers |>
  mutate(
    vote_change = vote_lab_97 - vote_lab_92,
    to_labour = if_else(to_labour == 1, "switched", "unswitched")
  ) |>
  group_by(to_labour) |>  
  summarize(avg_change = mean(vote_change)) |>
  pivot_wider(
    names_from = to_labour,
    values_from = avg_change
  ) |>
  mutate(DID = switched - unswitched)
```


## Summarizing data (Week 5)


### Calculating the mean, median, and standard deviation

Many of the summary functions have intuitive names:

```{r}
gapminder |>
  summarize(
    gdp_mean = mean(gdpPercap),
    gdp_median = median(gdpPercap),
    gdp_sd = sd(gdpPercap)
  )
```


You can also use these functions outside of a `summarize()` call by accessing variables with the `$` operator:

```{r}
mean(gapminder$gdpPercap)
```


### Adding a vertical line to a plot

You can layer on vertical lines to a ggplot using the [`geom_vline()`](https://ggplot2.tidyverse.org/reference/geom_abline.html) function. For horizontal lines, use `geom_hline()`. 

```{r}
ggplot(gapminder, aes(x = lifeExp)) +
  geom_histogram(binwidth = 1) +
  geom_vline(aes(xintercept = mean(lifeExp)), color = "indianred") +
  geom_vline(aes(xintercept = median(lifeExp)), color = "dodgerblue")
```


### Dropping missing rows with missing data

To drop any row of a data frame with missing values in any variable, you can use the `drop_na()` function. Here we use the `nrow()` function to show how many rows each tibble has.  

```{r}
cces_2020 |>
  nrow()
```


```{r}
cces_2020 |>
  drop_na() |>
  nrow()
```

By default `drop_na()` drops rows that have any missingness. If you want to drop rows based on being missing only for a particular variable, just pass the variable name to the function:

```{r}
cces_2020 |>
  drop_na(turnout_self) |>
  nrow()
```


### Removing missing values in summary functions

Most summary functions have an argument `na.rm` (for "NA remove") that is `FALSE` by default, meaning it tries to calculate summaries with missing values included:

```{r}
cces_2020 |>
  summarize(avg_turnout = mean(turnout_self))
```

To remove missing values and calculate the summary based on the observed values for that variable, set `na.rm = TRUE`:



```{r}
cces_2020 |>
  summarize(avg_turnout = mean(turnout_self, na.rm = TRUE))
```


### Detecting missing values 

You cannot use `x == NA` for detecting missing values in R because it treats missing values quite literally and just returns `NA` for all values:

```{r}
c(5, 6, NA, 0) == NA
```


Instead you can use `is.na()`:

```{r}
is.na(c(5, 6, NA, 0))
```


You can negate this to test for non-missing values:


```{r}
!is.na(c(5, 6, NA, 0))
```


### Using `is.na` to summarize missingness


Applying `mean()` to  `is.na()` output will tell us the proportion of values in a vector that are missing. We can combine this with `group_by` and `summarize` to see what share of each party ID response are missing turnout:

```{r}
cces_2020 |>
  group_by(pid3) |>
  summarize(
    missing_turnout = mean(is.na(turnout_self))
  )
```


### Proportion table for one variable

To get a proportion table for a single categorical variable, start with the approach to creating a table of counts of each category. Then just add a `mutate()` step to get the proportion:


```{r}
cces_2020 |>
  group_by(pres_vote) |>
  summarize(n = n()) |>
  mutate(
    prop = n / sum(n)
  )
```


### Proportion table with two variables

With two variables, we take the same approach as for one variable, but we need to be careful about the grouping:

```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n))
```

After summarizing, the tibble drops the last grouping variable and groups the resulting tibble by the remaining variable(s). In this case, `pres_vote` is dropped and the tibble is now grouped by `pid3`. This means that when we call `mutate()`, the calculations will be done **within** levels of `pid3`. So the first row here is telling us the the share of Democrats that voted for Biden is 0.968. 


If we want to have the proportion be as a share of the `pres_vote` instead, we can reorder the grouping variables:

```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pres_vote, pid3) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n))
```


Finally, if we want the proportions to be out of all rows of the data, we can tell `summarize()` to drop the grouping:

```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n(), .groups = "drop") |>
  mutate(prop = n / sum(n))
```


### Create cross-tab proportion tables

We can pivot our proportion tables to create cross-tabs, but if we leave in the count variable, we get output that we don't expect:

```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n)) |>
  pivot_wider(
    names_from = pres_vote,
    values_from = prop
  )
```


To combat this, we can either drop the count variable:

```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n)) |>
  select(-n) |>  
  pivot_wider(
    names_from = pres_vote,
    values_from = prop
  )
```


Or we can also be specific about what variables we want on the rows. We can use the `id_cols` argument to specify just the variables that will go on the rows:


```{r}
cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n)) |>
  pivot_wider(
    id_cols = pid3,
    names_from = pres_vote,
    values_from = prop
  )
```


### Visualizing a proportion table

We can visualize our proportion table using by mapping the grouping variable to the `x` axis and the non-grouped variable to the `fill` aesthetic. We also need to use the `position = "dodge"` argument to the `geom_col()` function so that the two columns in each group don't overlap.  

```{r}
#| label: "crosstab-plot"

cces_2020 |>
  filter(pres_vote %in% c("Joe Biden (Democrat)",
                          "Donald J. Trump (Republican)")) |>
  mutate(pres_vote = if_else(pres_vote == "Joe Biden (Democrat)",
                             "Biden", "Trump")) |>  
  group_by(pid3, pres_vote) |>
  summarize(n = n()) |>
  mutate(prop = n / sum(n)) |>
  
ggplot(aes(x = pid3, y = prop, fill = pres_vote)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c(Biden = "steelblue1", Trump = "indianred1"))
```

## Bivariate Relationships and Joins (Week 6)

### Calculating a correlation

You can calcuate a correlation between two variables using the `cor` function:


```{r}
cor(news$national_politics, news$local_politics)
```

You can also use the pipe and summarize:

```{r}
news |>
  summarize(cor(national_politics, local_politics))
```

When there are missing values in one of the variables, the correlation will be `NA`:

```{r}
cor(covid_votes$one_dose_5plus_pct, covid_votes$dem_pct_2020)
```

In that case, you can use the argument `use = "pairwise"` to remove the observations with missing values:

```{r}
cor(covid_votes$one_dose_5plus_pct, covid_votes$dem_pct_2020,
    use = "pairwise")
```


### Writing a function

You can create a function with the `function()` function (oof). The arguments in the `function()` call will become the arguments of your function. For example, if we want to create a z-score function that takes in a vector and coverts it to a z-score, we can create a function for this:

```{r}
z_score <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}
```

Here, `x` is the argument that takes in the vector and it is used inside the function body to perform the task. Let's call the function:

```{r}
z_score(x = c(1, 2, 3, 4, 5))
```

What happens here is that R runs the code inside the function definition, replacing `x` with the vector we pass to `x`, which in this case is `c(1, 2, 3, 4, 5)`. 


### Pivoting longer


We sometimes have data that has data in the column names that we would actually like to be in the data frame itself. When this happens, we need the opposite of `pivot_wider` which is called `pivot_longer`. For example, the `mortality` data has the years in the column names, but we want the rows to be country-years:


```{r}
mortality
```

We can use `pivot_longer` to move this information. To do so, we need to pass the variables we want to pivot to the `cols` argument, the name of the new variable to hold the colum names into `names_to`, and the name of the column to hold the pivoted data in `values_to`. 


```{r}
mortality_long <- mortality |>
  select(-indicator) |>  
  pivot_longer(
    cols = `1972`:`2020`,
    names_to = "year",
    values_to = "child_mortality"
  ) |>
  mutate(year = as.integer(year))
mortality_long
```

Here we mutate the years to be integers because they are characters when initially pivoted. 


### Pivoting longer with a column name prefix

Sometimes the column names have a prefix in front of the data that we want, like in the `spotify` data where the week columns are all `weekX` where `X` is the week number. We'd like to extract `X`:

```{r}
spotify
```



We can use the `names_prefix` arugment to do this:


```{r}
spotify |>
  pivot_longer(
    cols = c(-`Track Name`, -Artist),
    names_to = "week_of_year",
    values_to = "rank",
    names_prefix = "week"
  ) |>
  mutate(
    week_of_year = as.integer(week_of_year)
  )
```


### Merging one data frame into another (joins)

We can take the variables from one data frame and put them into another data frame using the various join functions. If we think of the primary data frame as A and the donor data frame as B, then if we want to keep the rows of A the same and just import the columns of B, then we can use the `left_join`

```{r}
gapminder |>
  left_join(mortality_long)
```

By default this keeps all rows of A and any rows in A that do not appear in B give missing values for the newly imported columns of B. If we want only the rows that are in both A and B, we can use `inner_join`


```{r}
gapminder |>
  inner_join(mortality_long)
```
 
### Specifying the key variables for a join

By default, the join functions will merge data frames A and B based on the columns that have the same name in both data frames. This sometimes isn't what we want. For example, using the `nycflights13` package, we might want to merge the `flights` and `planes` data frames. Both of these data sets have a `year` variable but they mean different things in the two data frames (the year of the flights vs the year the plane was manufactured):

```{r}
library(nycflights13)
flights

planes
```

In this case, we want to specify what variable we want to do the merge with. Here, we want to merge based on `tailnum`:


```{r}
flights |>
  left_join(planes, by = "tailnum") |>
  select(year.x, tailnum, origin, dest, carrier, type, engine)
```

Because both data sets had a `year` variable, the join function renames them to `year.x` in the first data frame and `year.y` for the second. We could rename them before merging to solve this problem. 


## Regression (Week 8)


### Add binned means to a scatterplot

You can add binned means of the y-axis variable in a scatterplot using the `stat_summary_bin(fun = "mean")` function with your ggplot call:



```{r, binned}
ggplot(health, aes(x = steps_lag, y = weight)) +
  geom_point(color = "steelblue1", alpha = 0.25) +
  labs(
    x = "Steps on day prior (in 1000s)",
    y = "Weight",
    title = "Weight and Steps"
  ) +
  stat_summary_bin(fun = "mean", color = "indianred1", size = 3,
                   geom = "point", binwidth = 1)
```



### Running a regression (estimating the line of best fit)

We can estimate the line of best fit using the `lm` function:

```{r}
fit <- lm(weight ~ steps, data = health)
fit
```


### Extracting the vector of estimated regression coefficients

```{r}
coef(fit)
```


### Obtaining unit-level statistics about the regression

The `broom` package has a number of functions that will allow you to extract information about the regression output from `lm()`. For example, if you want information about each unit such as the predicted/fitted value and the residual, you can use `augment`:

```{r}
library(broom)
augment(fit)
```



### Obtaining overall information about the model fitted

The `glance` function will give you some high-level information about the regression, including the $R^2$:


```{r}
glance(fit)
```


You can also access the $R^{2}$ using the `summary()` function:

```{r}
summary(fit)$r.squared
```

### Running a multiple regression

You can fit a regression with multiple predictors by putting a `+` between them on the right-hand side of the formula:

```{r}
mult.fit <- lm(seat_change ~ approval + rdi_change,
               data = midterms)
mult.fit
```


## Sampling (Week 9)


### Selecting a random subset of rows of the data

The `slice_sample` function will return a random subset of rows from your data:

```{r}
class_years |>
  slice_sample(n = 5)
```

 
### Repeated sampling from a data frame

You can take multiple random samples from a data set using the `rep_slice_sample` function from the `infer` package. In the resulting data frame you will get a new column called `replicate` which will indicate which of the repeated samples this row belongs to.  

```{r}
library(infer)
class_years |>
  rep_slice_sample(n = 5, reps = 2)
```


You can use this to get repeated draws of a sample statistic like the sample proportion:


```{r}
samples_n20 <- class_years |>
  rep_slice_sample(n = 20, reps = 100) |>
  group_by(replicate) |>
  summarize(fy_prop = mean(year == "First-Year"))
samples_n20
```


### Converting a column of a tibble into a vector

You can use the `pull` function to access a column of a tibble as a R vector, removing all the tibble infrastructure:


```{r}
class_years |>
  slice_sample(n = 5) |>
  pull(year)
```


### Barplot with proportions rather than counts

You can use the `y = after_stat(prop)` aesthetic to get a barplot with proportions instead of count to show us the chance/probability of selecting a first-year student:

```{r}
#| label: "disc_dist"
class_years |>
  mutate(first_year = as.numeric(year == "First-Year")) |>
  ggplot(aes(x = first_year)) +
  geom_bar(mapping = aes(y = after_stat(prop)), width = 0.1)
```


### Density histograms

We can use the `y = after_stat(density)` to create a **density histogram** instead of a count histogram so that the area of the histogram boxes are equal to the chance of randomly selecting a unit in that bin: 

```{r}
#| label: "cont"
midwest |>
  ggplot(aes(x = percollege)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1)
```

## Bootstrap (Week 10)


### Bootstrap distributions

You can generate bootstrap samples by using `rep_slice_sample(prop = 1, replace = TRUE)`. From there, if you group by the replicate column, you can do summaries within each bootstrap sample.  

```{r}
#| label: "boot_dist"
bootstrap_dist <- anes |>
  rep_slice_sample(prop = 1, reps = 1000, replace = TRUE) |>
  group_by(replicate) |>
  summarize(sci_therm_mean = mean(sci_therm))
bootstrap_dist
```

You can also generate bootstraps using the `specify/generate` workflow:


```{r}
boot_dist_infer <- anes |>
  specify(response = sci_therm) |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "mean")
boot_dist_infer
```

You can also pass these functions to `visualize()` to plot the distributions (though you could always use ggplot as well):


```{r}
visualize(boot_dist_infer)
```


### Calculating bootstrap confidence intervals

You can use the `get_confidence_intervals` or `get_ci` (they are the same function) with `type = "percentile"` to get bootstrap confidence intervals. Use the `level` argument to set the confidence level 

```{r}
boot_dist_infer |>
  get_confidence_interval(level = 0.95, type = "percentile")
```



### Bootstrapping a difference in means

To get the bootstrapped distribution of a difference in means, we can use `rep_slice_sample` and our usual dplyr workflow: 

```{r}
dim_boots <- trains |>
  rep_slice_sample(prop = 1, replace = TRUE, reps = 1000) |>
  group_by(replicate, treatment) |>
  summarize(post_mean = mean(numberim.post)) |>
  pivot_wider(names_from = treatment, values_from = post_mean) |>
  mutate(ATE = `1` - `0`)
dim_boots
```

Note that if you use this approach, you'll need to select the column that you want before passing it to `get_ci`:

```{r}
dim_boots |>
  select(replicate, ATE) |>
  get_ci(level = 0.95)
```

We can also use the `specify` workflow from `infer`, but we need to be careful about setting the order of the groups for the difference in means in the `calculate` step:

```{r}
dim_boots_infer <- trains |>
  mutate(treatment = if_else(treatment == 1, "Treated", "Control")) |>  
  specify(numberim.post ~ treatment) |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "diff in means", order = c("Treated", "Control"))
dim_boots_infer
```


### Bootstrapping the difference between estimated ATEs


When you want to estimate the interaction between two variables or the difference in ATEs between two groups, we need to get both the treatment and grouping variables into the columns using `pivot_wider`. Passing both variables to the `names_from` argument will create columns based on each combination of the variables with a `_` to separate the column names:

```{r}
#| label: "boot_int"
trains |>
  mutate(
    treatment = if_else(treatment == 1, "Treated", "Control"),
    college = if_else(college == 1, "College", "Noncollege") 
  ) |>
  group_by(treatment, college) |>
  summarize(post_mean = mean(numberim.post)) |>
  pivot_wider(
    names_from = c(treatment, college),
    values_from = post_mean
  ) 
```

Thus, to calculate the difference in ATEs we can:


```{r}
trains |>
  mutate(
    treatment = if_else(treatment == 1, "Treated", "Control"),
    college = if_else(college == 1, "College", "Noncollege") 
  ) |>
  group_by(treatment, college) |>
  summarize(post_mean = mean(numberim.post)) |>
  pivot_wider(
    names_from = c(treatment, college),
    values_from = post_mean
  ) |>
  mutate(
    ATE_c = Treated_College - Control_College,
    ATE_nc = Treated_Noncollege - Control_Noncollege,
    interaction = ATE_c - ATE_nc
  )
```


We can now add the `rep_slice_sample` step to produce a bootstrap distribution of this interaction: 

```{r}
int_boots <- trains |>
  mutate(
    treatment = if_else(treatment == 1, "Treated", "Control"),
    college = if_else(college == 1, "College", "Noncollege") 
  ) |>
  rep_slice_sample(prop = 1, replace = TRUE, reps = 1000) |>  
  group_by(replicate, treatment, college) |>
  summarize(post_mean = mean(numberim.post)) |>
  pivot_wider(
    names_from = c(treatment, college),
    values_from = post_mean
  ) |>
  mutate(
    ATE_c = Treated_College - Control_College,
    ATE_nc = Treated_Noncollege - Control_Noncollege,
    interaction = ATE_c - ATE_nc
  ) |>
  select(replicate, ATE_c, ATE_nc, interaction)
int_boots
```

## Hypothesis testing (Week 11)

### Generating the null distribution of a sample mean


```{r}

null_dist <- gss |>
  specify(response = hours) |>
  hypothesize(null = "point", mu = 40) |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "mean")
null_dist
```

### Calculating the p-value of a hypothesis test

If we want to calculate the p-value of the test, we need to first calculate the observed value of the sample mean and then pass that to the `get_p_value` function:

```{r}
obs_mean <- gss |>
  specify(response = hours) |>
  calculate(stat = "mean")

get_p_value(null_dist, obs_mean, direction = "both")
```

We can also visualize the p-value:

```{r}
visualize(null_dist) +
  shade_p_value(obs_mean, direction = "both")
```

### Null distribution for a difference in means

For a diffrence in means, we can use a similar approach, but need to turn our treatment/grouping variable into a charactor/factor variable. Then we need to make sure we sepcify the order the difference in mean should be taken across groups. First we calculate the effect and then get the null distribution using the `permute` approach. 

```{r}
trains <- trains |>
  mutate(treated = if_else(treatment == 1, "Treated", "Untreated"))

ate <- trains |>
  specify(numberim.post ~ treated) |>
  calculate(stat = "diff in means",
            order = c("Treated", "Untreated"))
ate


null_dist <- trains |>
  specify(numberim.post ~ treated) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000,
           type = "permute") |>
  calculate(stat = "diff in means", order = c("Treated", "Untreated"))
null_dist
```


Getting the p-value is the same:


```{r}
get_p_value(null_dist, obs_stat = ate, direction = "both")
```
